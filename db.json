{"meta":{"version":1,"warehouse":"4.0.0"},"models":{"Asset":[{"_id":"themes/next_8.8/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/images/logo.svg","path":"images/logo.svg","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/images/logo-algolia-nebula-blue-full.svg","path":"images/logo-algolia-nebula-blue-full.svg","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/images/ye_16.ico","path":"images/ye_16.ico","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/images/ye_32.ico","path":"images/ye_32.ico","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/css/noscript.styl","path":"css/noscript.styl","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/bookmark.js","path":"js/bookmark.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/comments.js","path":"js/comments.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/config.js","path":"js/config.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/comments-buttons.js","path":"js/comments-buttons.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/motion.js","path":"js/motion.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/next-boot.js","path":"js/next-boot.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/pjax.js","path":"js/pjax.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/schedule.js","path":"js/schedule.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/utils.js","path":"js/utils.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/schemes/muse.js","path":"js/schemes/muse.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/fancybox.js","path":"js/third-party/fancybox.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/pace.js","path":"js/third-party/pace.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/rating.js","path":"js/third-party/rating.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/quicklink.js","path":"js/third-party/quicklink.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/analytics/baidu-analytics.js","path":"js/third-party/analytics/baidu-analytics.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/analytics/google-analytics.js","path":"js/third-party/analytics/google-analytics.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/analytics/growingio.js","path":"js/third-party/analytics/growingio.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/comments/disqus.js","path":"js/third-party/comments/disqus.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/comments/disqusjs.js","path":"js/third-party/comments/disqusjs.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/comments/gitalk.js","path":"js/third-party/comments/gitalk.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/comments/changyan.js","path":"js/third-party/comments/changyan.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/comments/isso.js","path":"js/third-party/comments/isso.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/comments/livere.js","path":"js/third-party/comments/livere.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/comments/utterances.js","path":"js/third-party/comments/utterances.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/chat/gitter.js","path":"js/third-party/chat/gitter.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/chat/chatra.js","path":"js/third-party/chat/chatra.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/chat/tidio.js","path":"js/third-party/chat/tidio.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/math/katex.js","path":"js/third-party/math/katex.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/math/mathjax.js","path":"js/third-party/math/mathjax.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/search/local-search.js","path":"js/third-party/search/local-search.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/statistics/firestore.js","path":"js/third-party/statistics/firestore.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/search/algolia-search.js","path":"js/third-party/search/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/statistics/lean-analytics.js","path":"js/third-party/statistics/lean-analytics.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/tags/mermaid.js","path":"js/third-party/tags/mermaid.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/tags/pdf.js","path":"js/third-party/tags/pdf.js","modified":1,"renderable":1}],"Cache":[{"_id":"source/_posts/Algorithm.md","hash":"d3c8265f341b1f479f940fd01f845a33c26cbc93","modified":1639730782799},{"_id":"source/_posts/An-Introduction-to-Git.md","hash":"5bffb32f7ea47f70f18c5367a9654afc2457ea19","modified":1639730786683},{"_id":"source/_posts/Construct-Your-Blog-with-Hexo-and-Github.md","hash":"72bd982e57011f68292f3f2d8d055bac89a37693","modified":1639712271057},{"_id":"source/_posts/Experiments.md","hash":"e4af00974e4c7a0b54c84c02ce10e45bcfdd8501","modified":1640312776249},{"_id":"source/_posts/First-Step-to-RL.md","hash":"15fb3d81dc2d8640f1dd7e9415be2f6c4133e2c7","modified":1639622354208},{"_id":"source/_posts/Transformer-and-BERT.md","hash":"9961d853a801477fb0ebc1738940c6ae4e7f4127","modified":1639792992773},{"_id":"source/_posts/Personal-Thought.md","hash":"16a7b4ab87f88a826c506fff8c68e8bd123df18b","modified":1640154693872},{"_id":"source/tags/index.md","hash":"3207ebf9794561395cf0c54633880ab070040ade","modified":1638946119840},{"_id":"source/_posts/Tips-in-Papers.md","hash":"8d1d1b6a08ee2b8e44f7682cf01b63292caf9967","modified":1640161796008},{"_id":"source/about/index.md","hash":"05b5e469a2cd6f7de545752294be494eab99936d","modified":1639728236458},{"_id":"source/categories/index.md","hash":"408ea9b07f3b1a1339731fe7b364d88bc5644aff","modified":1637754696896},{"_id":"source/_posts/First-Step-to-RL/q_learning.py","hash":"3a68ec1153d26f2c18c9d34ee5deb5e495b8dca9","modified":1638761670515},{"_id":"source/_posts/First-Step-to-RL/policy_network.py","hash":"b3e8f06360cad0084a0656edaed4539f1b01e327","modified":1638761670499},{"_id":"source/_posts/Tips-in-Papers/ADCM.jpg","hash":"ee5b75f3a063e07624c5f03111f98902d885cd8e","modified":1639703970059},{"_id":"source/_posts/Tips-in-Papers/Saccader_eq1.jpg","hash":"2ea2a74f960377a6a9d0bc98ecfadd8ba09cd316","modified":1639834017991},{"_id":"source/_posts/Tips-in-Papers/Saccader_eq2.jpg","hash":"cf5832e89b29e39275d2fa4a2e39592e97f9a39e","modified":1639834068705},{"_id":"source/_posts/Tips-in-Papers/Saccader_eq3.jpg","hash":"8fa845242bb6f737736d91485ca237e7132e19b5","modified":1639833635328},{"_id":"source/_posts/Tips-in-Papers/Saccader_gl.jpg","hash":"fb8050f8b71a7c667cfa85672b505b3340d2231b","modified":1640144317644},{"_id":"source/_posts/Tips-in-Papers/Saccader_Over.jpg","hash":"deaccd942ffca9e163a9f2c9bdfb69251d1004cb","modified":1639817165825},{"_id":"source/_posts/Tips-in-Papers/Saccader_Cell.jpg","hash":"b17e79bf86bb3422527bf51d200ceceeb53ce936","modified":1639817097223},{"_id":"source/_posts/Tips-in-Papers/Tnet_over.jpg","hash":"ed04ca4049c6f909b0bf51ea57ffa9a2b445e1c8","modified":1640160184675},{"_id":"source/_posts/An-Introduction-to-Git/git.jpg","hash":"db9ed8bb86df7e73d5be3bcae4cc8656e4a7a0ed","modified":1639191445656},{"_id":"themes/next_8.8/.eslintrc.json","hash":"611e15c3fcb41dc68fa8532ee595a1262a1b5a8a","modified":1638944857842},{"_id":"themes/next_8.8/.gitattributes","hash":"aeeca2f1e987d83232d7870d1435a4e3ed66b648","modified":1638944857843},{"_id":"themes/next_8.8/_vendors.yml","hash":"ba72c575e627697a050614411706cb20206d4b71","modified":1638944857855},{"_id":"themes/next_8.8/crowdin.yml","hash":"4a53f5985e545c635cb56b2a57ed290cb8cf8942","modified":1638944857855},{"_id":"themes/next_8.8/package.json","hash":"e527d094273cf3be4766630bbfe6cc8cf1eeb529","modified":1638944857899},{"_id":"themes/next_8.8/renovate.json","hash":"767b077c7b615e20af3cf865813cd64674a9bea6","modified":1638944857899},{"_id":"themes/next_8.8/LICENSE.md","hash":"8cfb03967dd4cbaf3b825271ffce0039aa3fc22a","modified":1638944857853},{"_id":"themes/next_8.8/README.md","hash":"43fe29330352545446a532e6630866251129882a","modified":1638944857854},{"_id":"themes/next_8.8/.editorconfig","hash":"731c650ddad6eb0fc7c3d4a91cad1698fe7ad311","modified":1638944857842},{"_id":"themes/next_8.8/.gitignore","hash":"087b7677078303acb2acb47432165950e4d29b43","modified":1638944857853},{"_id":"themes/next_8.8/_config.yml","hash":"4ab5a431bed77cf248e5a6f2905e1bbedf1d7d3c","modified":1639637912097},{"_id":"themes/next_8.8/.github/issue_label_bot.yaml","hash":"533fbe6b2f87d7e7ec6949063bb7ea7eb4fbe52d","modified":1638944857847},{"_id":"themes/next_8.8/.stylintrc","hash":"6259e2a0b65d46865ab89564b88fc67638668295","modified":1638944857853},{"_id":"themes/next_8.8/.github/CODE_OF_CONDUCT.md","hash":"593ae64e72d43c020a697eac65b1f9c3483ff097","modified":1638944857844},{"_id":"themes/next_8.8/.github/config.yml","hash":"0956bf71b6f36632b63b14d26580458041a5abd2","modified":1638944857847},{"_id":"themes/next_8.8/.github/CONTRIBUTING.md","hash":"2fdca1040427cabfe27cae6754ec5e027ec7092e","modified":1638944857844},{"_id":"themes/next_8.8/.github/label-commenter-config.yml","hash":"a1aa85a2fc66ff0c52c65bd97b0fa282e297a73f","modified":1638944857848},{"_id":"themes/next_8.8/.github/PULL_REQUEST_TEMPLATE.md","hash":"a103e2d875f7434191859e5b42075cfa9a4cbcb3","modified":1638944857846},{"_id":"themes/next_8.8/.githooks/install.js","hash":"305c2a269818466eed9e381b866c6cd1ad7f8afd","modified":1638944857843},{"_id":"themes/next_8.8/.githooks/pre-commit","hash":"b69b9d0b51e27d5d4c87c3242f5067c2cda26e44","modified":1638944857844},{"_id":"themes/next_8.8/.github/labeler.yml","hash":"ff76a903609932a867082b8ccced906e9910533a","modified":1638944857848},{"_id":"themes/next_8.8/.github/release-drafter.yml","hash":"de38f816e3023e0a5c1fd1f3c2b626f78bc35246","modified":1638944857848},{"_id":"themes/next_8.8/docs/AUTHORS.md","hash":"579014d47f45b27fd1618b9709f0efe9585c7449","modified":1638944857856},{"_id":"themes/next_8.8/docs/LICENSE.txt","hash":"d1cd5a8e83d3bbdb50f902d2b487813da95ddfd3","modified":1638944857857},{"_id":"themes/next_8.8/languages/README.md","hash":"b1c96465b3bc139bf5ba6200974b66581d8ff85a","modified":1638944857859},{"_id":"themes/next_8.8/docs/AGPL3.md","hash":"f463f95b169d64983f59fa6f3e4b6760290a0e6b","modified":1638944857856},{"_id":"themes/next_8.8/languages/default.yml","hash":"ea5e6aee4cb14510793ac4593a3bddffe23e530c","modified":1638944857860},{"_id":"themes/next_8.8/languages/ar.yml","hash":"cc7e3e2855348563d746f15c4752b9c63fcdd91a","modified":1638944857859},{"_id":"themes/next_8.8/languages/de.yml","hash":"83023c4246b93a2f89f342afe29a7b9e1185f74f","modified":1638944857859},{"_id":"themes/next_8.8/languages/en.yml","hash":"66445143decfbb5eb7031eb370698e31d5222a7a","modified":1638944857860},{"_id":"themes/next_8.8/languages/fa.yml","hash":"e09fad889ab3ae87874093e1acd51edc9297d869","modified":1638944857861},{"_id":"themes/next_8.8/languages/es.yml","hash":"07955d78028cea2a590c63fdc2c01ca3ee05a727","modified":1638944857860},{"_id":"themes/next_8.8/languages/id.yml","hash":"d7c337ca72efb0bd02ade8b5560c559384ad84dd","modified":1638944857861},{"_id":"themes/next_8.8/languages/fr.yml","hash":"328c255c82e9b561e20a9f51a4d84abc63d1b90a","modified":1638944857861},{"_id":"themes/next_8.8/languages/ja.yml","hash":"57a35b21aca04ce8bca64fb5933f35626c462ea3","modified":1638944857862},{"_id":"themes/next_8.8/languages/ko.yml","hash":"d6e2add7488065ec4f7d21cfcf7f0eaa877a84f4","modified":1638944857862},{"_id":"themes/next_8.8/languages/it.yml","hash":"c038ff0cadbe405750d980bcacfd3900acf96905","modified":1638944857862},{"_id":"themes/next_8.8/languages/nl.yml","hash":"e47858bd1e0d0622c15366ae6c0513d996f589e3","modified":1638944857863},{"_id":"themes/next_8.8/languages/pt.yml","hash":"ff93459250c33d3c7ba06c30164cc4208edf9b33","modified":1638944857863},{"_id":"themes/next_8.8/languages/pt-BR.yml","hash":"305025e932832328b7e2a8a584638a23c462e68f","modified":1638944857863},{"_id":"themes/next_8.8/languages/si.yml","hash":"c15ed758dbad890e856f4fc281208d7b78cc1a59","modified":1638944857864},{"_id":"themes/next_8.8/languages/ru.yml","hash":"7d13108f4a70ff6a162508a49678e4a477fa7b56","modified":1638944857863},{"_id":"themes/next_8.8/languages/zh-CN.yml","hash":"f8379d15038e22ef7039d91272cb4f36842dbbe1","modified":1638944857865},{"_id":"themes/next_8.8/languages/tr.yml","hash":"d3262d2221b0583a52e5d20a3cd1380f5dc49378","modified":1638944857864},{"_id":"themes/next_8.8/languages/uk.yml","hash":"f32871f67c63d26bc4e3e15df9b01f5a41236a50","modified":1638944857864},{"_id":"themes/next_8.8/languages/zh-HK.yml","hash":"c1ee97ceb56da76ecdc7b69fa975f28c8574441b","modified":1638944857865},{"_id":"themes/next_8.8/languages/vi.yml","hash":"e452ea8c48993262a3e8fce9d92072cafabfc734","modified":1638944857865},{"_id":"themes/next_8.8/layout/post.njk","hash":"707a50e50b90df5fbeaf8407d12895d04163a290","modified":1638944857898},{"_id":"themes/next_8.8/layout/_layout.njk","hash":"2842f3e9fdde5bbd14cac89629221e68d80c8ea1","modified":1638944857866},{"_id":"themes/next_8.8/languages/zh-TW.yml","hash":"70c45076ad722b777956048fcc430eac37844c11","modified":1638944857865},{"_id":"themes/next_8.8/layout/page.njk","hash":"fddfdee95f5da86eab8a85d6eb1901996d2153cf","modified":1638944857898},{"_id":"themes/next_8.8/layout/archive.njk","hash":"aa491dba8f746e626c273a920effedf7d0b32170","modified":1638944857897},{"_id":"themes/next_8.8/layout/category.njk","hash":"82f541452cae76a94ee15cb8d8a888f44260a0fd","modified":1638944857897},{"_id":"themes/next_8.8/test/index.js","hash":"983a505399796b9d9e174ba46d89abbdde38f8ee","modified":1638944857965},{"_id":"themes/next_8.8/layout/index.njk","hash":"fa52c3049871e879980cb6abccdea3792ca4ce70","modified":1638944857898},{"_id":"themes/next_8.8/layout/tag.njk","hash":"b6c017d30d08ddd30d66e9c6f3a71aa65d214eac","modified":1638944857899},{"_id":"themes/next_8.8/.github/workflows/label-commenter.yml","hash":"7dec949b13131783e726facb2f4acde0945db1b8","modified":1638944857849},{"_id":"themes/next_8.8/.github/workflows/lock.yml","hash":"58eca481fd71088a8ae1dbc04645bcfc03460b87","modified":1638944857849},{"_id":"themes/next_8.8/.github/workflows/release-drafter.yml","hash":"359b74890a47d784e35a5cc3c7885d5cdf302e82","modified":1638944857852},{"_id":"themes/next_8.8/.github/workflows/labeler.yml","hash":"46d0b29dc561fe571d91fd06a7c8ef606b984c72","modified":1638944857849},{"_id":"themes/next_8.8/.github/workflows/stale.yml","hash":"32e7dfb55ecf8af66aebfed471be09ef2eb10e18","modified":1638944857852},{"_id":"themes/next_8.8/.github/ISSUE_TEMPLATE/config.yml","hash":"daeedc5da2ee74ac31cf71846b766ca6499e9fc6","modified":1638944857845},{"_id":"themes/next_8.8/.github/workflows/linter.yml","hash":"b57d876c90d1645a52bbba8a52d47ad0b0c96140","modified":1638944857849},{"_id":"themes/next_8.8/.github/workflows/tester.yml","hash":"645bb69d0b6cc062c47fabb1ccb2297ccbcfa7f5","modified":1638944857852},{"_id":"themes/next_8.8/.github/ISSUE_TEMPLATE/other.md","hash":"618d07b49f4774cd79613d4001984a19d954a6ad","modified":1638944857846},{"_id":"themes/next_8.8/.github/ISSUE_TEMPLATE/bug-report.md","hash":"032194e7975564176f2109aa8b7c020fa6d5e6b1","modified":1638944857845},{"_id":"themes/next_8.8/.github/ISSUE_TEMPLATE/feature-request.md","hash":"4a7885fe2c8b25be02ab57c345cd862aeeeeacaf","modified":1638944857846},{"_id":"themes/next_8.8/docs/zh-CN/CONTRIBUTING.md","hash":"a09ceb82b45dd8b7da76c227f3d0bb7eebe7d5d1","modified":1638944857858},{"_id":"themes/next_8.8/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"7befb4325b107dd668d9eae3d7e86a34910ce3f2","modified":1638944857858},{"_id":"themes/next_8.8/docs/zh-CN/README.md","hash":"354b0b0a24cbe97cccf2ec8bd97eb7d624fa0dea","modified":1638944857858},{"_id":"themes/next_8.8/layout/_macro/post-collapse.njk","hash":"d9d8e6d7a6a8c80009dd5334cc17fd3e4977a008","modified":1638944857867},{"_id":"themes/next_8.8/layout/_scripts/index.njk","hash":"4eb65641b47ea9b23ed2ddfd69b18f21d7d8f214","modified":1638944857877},{"_id":"themes/next_8.8/layout/_macro/post.njk","hash":"367cafd3acc1c6a045d8a72de0479aabbf4a3559","modified":1638944857867},{"_id":"themes/next_8.8/layout/_scripts/vendors.njk","hash":"0a1470440f11362df2b1cd6b6228e273d9f999d6","modified":1638944857877},{"_id":"themes/next_8.8/docs/ru/README.md","hash":"e1d6bf38cf34972ca2ee5331a727787fe14082a3","modified":1638944857857},{"_id":"themes/next_8.8/layout/_macro/sidebar.njk","hash":"f3c1fc4b3333cb09a40b6b3b9042e5ab277fe885","modified":1639645479653},{"_id":"themes/next_8.8/layout/_partials/comments.njk","hash":"d6b7bb7764e3b471ed6b4e5715f6cbe2dd453f59","modified":1638944857868},{"_id":"themes/next_8.8/layout/_macro/sidebar.njk_backup","hash":"eec74e135d01948361020140c3798769e1e7363b","modified":1639644621600},{"_id":"themes/next_8.8/layout/_partials/pagination.njk","hash":"2de77d533c91532a8a4052000244d0c1693370df","modified":1638944857873},{"_id":"themes/next_8.8/layout/_third-party/fancybox.njk","hash":"53ad3c31762b74e5d29787b37d5e494cc4fded9b","modified":1638944857888},{"_id":"themes/next_8.8/layout/_third-party/index.njk","hash":"33a4a3275474bd3bb2e8d1b0ea01b42dda9ea608","modified":1638944857888},{"_id":"themes/next_8.8/layout/_partials/widgets.njk","hash":"967594ee64805e27b7ff9d957e23ab3f5c948600","modified":1638944857877},{"_id":"themes/next_8.8/layout/_partials/footer.njk","hash":"0347cb6077a969136aac26ebdc205a7817010ee7","modified":1639127865723},{"_id":"themes/next_8.8/layout/_third-party/rating.njk","hash":"d0444179fec512760ab1d4f76928d795b971c884","modified":1638944857891},{"_id":"themes/next_8.8/scripts/events/index.js","hash":"8bca7ae3cebb3857866d718a562c5d8820fcfbe5","modified":1638944857900},{"_id":"themes/next_8.8/scripts/filters/default-injects.js","hash":"0c9a1fe9906672724dbf274154a37bac1915ca2c","modified":1638944857905},{"_id":"themes/next_8.8/layout/_partials/languages.njk","hash":"537026fc120adeef9148c98ebf074207e3810538","modified":1638944857871},{"_id":"themes/next_8.8/scripts/filters/minify.js","hash":"9789307212d729c8cb65e3541348938a1965ff6f","modified":1638944857906},{"_id":"themes/next_8.8/layout/_third-party/quicklink.njk","hash":"73bc15a9c3c5c239ab90efa19a1e721f41f3cb93","modified":1638944857890},{"_id":"themes/next_8.8/scripts/filters/locals.js","hash":"8499b9c8c6cdae8aa7e4f5ec5b4b76037969db76","modified":1638944857905},{"_id":"themes/next_8.8/layout/_third-party/pace.njk","hash":"13b2a77b4858a127f458ea092b6f713b052befac","modified":1638944857890},{"_id":"themes/next_8.8/scripts/filters/number.js","hash":"63735cb9d02921e25b2606490340a70db89abbec","modified":1638945314670},{"_id":"themes/next_8.8/scripts/filters/post.js","hash":"5a132b7f9280a40b3d5fb40928c8cbbe071fe6f6","modified":1638944857906},{"_id":"themes/next_8.8/scripts/helpers/font.js","hash":"0a6fa582a0890ecaf5f03f758a730936e48aeca1","modified":1638944857907},{"_id":"themes/next_8.8/scripts/helpers/engine.js","hash":"18cc82558e7a9f3b6086c41ce9de0c46e807a66c","modified":1638944857906},{"_id":"themes/next_8.8/scripts/helpers/next-config.js","hash":"e73f43f1bcb46965e317285d6831e129a40ea59b","modified":1638944857907},{"_id":"themes/next_8.8/scripts/helpers/next-url.js","hash":"98fc68cf3fcd6253bbb94068ab1d86578a4ef9ea","modified":1638944857908},{"_id":"themes/next_8.8/scripts/helpers/next-vendors.js","hash":"52acbc74c1ead8a77cd3bbcba4e033053683f7d0","modified":1638944857908},{"_id":"themes/next_8.8/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1638944857949},{"_id":"themes/next_8.8/source/images/logo.svg","hash":"2cb74fd3ea2635e015eabc58a8d488aed6cf6417","modified":1638944857950},{"_id":"themes/next_8.8/source/images/avatar.gif","hash":"2dbc3e2f2d624b2ca1afe6edc2ca17307f1950c8","modified":1638944857949},{"_id":"themes/next_8.8/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1638944857948},{"_id":"themes/next_8.8/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1638944857949},{"_id":"themes/next_8.8/source/images/logo-algolia-nebula-blue-full.svg","hash":"b85e274207b1392782476a0430feac98db1e7da0","modified":1638944857950},{"_id":"themes/next_8.8/source/css/noscript.styl","hash":"7dc97674c232f6ca71e48b95e3f66472cd8e9c05","modified":1638944857948},{"_id":"themes/next_8.8/source/images/ye_16.ico","hash":"b8fb01b5361da89831d232a831a1532e9822bd72","modified":1638945436254},{"_id":"themes/next_8.8/source/css/_colors.styl","hash":"a88430865c99f47ce1d8240f8895819b8b7b0c06","modified":1638944857912},{"_id":"themes/next_8.8/source/images/ye_32.ico","hash":"375c99cd785d93dd989c36604ffbd10ada71322a","modified":1638945436261},{"_id":"themes/next_8.8/scripts/tags/button.js","hash":"86c71c73a63744efbbbb367612871fede0d69529","modified":1638944857909},{"_id":"themes/next_8.8/scripts/tags/caniuse.js","hash":"8e912c715702addaf0cefe63e580e45b97ae8c3f","modified":1638944857909},{"_id":"themes/next_8.8/scripts/tags/group-pictures.js","hash":"1c609312a71d47f838226346aad5c2e1c35f15dd","modified":1638944857909},{"_id":"themes/next_8.8/scripts/tags/center-quote.js","hash":"b4d12e6fe29089be0f43bafc9eea736602cd16bf","modified":1638944857909},{"_id":"themes/next_8.8/scripts/tags/index.js","hash":"255dd1090e8319b557eeca43571f0e4f8aab013b","modified":1638944857910},{"_id":"themes/next_8.8/source/css/main.styl","hash":"38b8a12681a3a04bed02aa1659054912ed6def11","modified":1638944857948},{"_id":"themes/next_8.8/scripts/tags/label.js","hash":"c18b0e619a779ed40be7f014db92af18f45fbd5c","modified":1638944857910},{"_id":"themes/next_8.8/scripts/tags/link-grid.js","hash":"3f358bb78c5c6fdf45de287f3ead553e3a6a93c2","modified":1638944857910},{"_id":"themes/next_8.8/scripts/tags/note.js","hash":"a12fd53e421400836a3722ae69130969558d6ac0","modified":1638944857911},{"_id":"themes/next_8.8/scripts/tags/mermaid.js","hash":"b3844e168b51a99d495ca05562ffac47677f5728","modified":1638944857911},{"_id":"themes/next_8.8/scripts/tags/pdf.js","hash":"317ba4611020cc840854386dde098dbbe452777e","modified":1638944857911},{"_id":"themes/next_8.8/scripts/tags/tabs.js","hash":"e0ed5fe1bc9d2957952a1aacdf3252d6ef3f9743","modified":1638944857912},{"_id":"themes/next_8.8/scripts/tags/video.js","hash":"f6ad3f52779f0636251238d3cbdc5b6f91cc5aba","modified":1638944857912},{"_id":"themes/next_8.8/source/js/comments.js","hash":"0b4daf0ce610760bd52e95d423f61f3e1c72442a","modified":1638944857951},{"_id":"themes/next_8.8/source/css/_mixins.styl","hash":"2ca820b221fb7458e6ef4fbcff826e1d1cf4b473","modified":1638944857938},{"_id":"themes/next_8.8/source/js/bookmark.js","hash":"1457291a7244b7786ec35b949d97183e4fbd181d","modified":1638944857950},{"_id":"themes/next_8.8/source/js/config.js","hash":"211a9ab35205ccfa6b7c74394bade84da0d00af7","modified":1638944857951},{"_id":"themes/next_8.8/source/js/motion.js","hash":"20b979ebe3671cb415e6e7171485d65cc347086e","modified":1638944857952},{"_id":"themes/next_8.8/source/js/comments-buttons.js","hash":"81ea6cbcdf0357094753d7523919c1eafa38e79f","modified":1638944857951},{"_id":"themes/next_8.8/source/js/next-boot.js","hash":"b0bdb542a809932182cfbb8772328115142a0b77","modified":1638944857952},{"_id":"themes/next_8.8/source/js/pjax.js","hash":"85293c253e0f43540572c4e4615c712325a732e2","modified":1638944857952},{"_id":"themes/next_8.8/source/js/schedule.js","hash":"6dade4388aa6579576a35758075134f573985d57","modified":1638944857953},{"_id":"themes/next_8.8/test/helpers/index.js","hash":"2fb58dca3df2fe53116ee2b1232fa26ebe7b2ce5","modified":1638944857964},{"_id":"themes/next_8.8/test/helpers/next-url.js","hash":"08e84781f1cd54e5634b86877ad9cefae4a78e95","modified":1638944857964},{"_id":"themes/next_8.8/source/js/utils.js","hash":"c13fa66aae52f59f88881738c00ebdcaf0209496","modified":1638944857963},{"_id":"themes/next_8.8/test/helpers/font.js","hash":"6f5076bd3f2724e47b46ca69028393a9b6275cd1","modified":1638944857964},{"_id":"themes/next_8.8/test/tags/center-quote.js","hash":"2ac4b5a358681691a17e736de06fce0b640a7023","modified":1638944857966},{"_id":"themes/next_8.8/test/tags/caniuse.js","hash":"2852be850d9103c25114253a45e6c62e32517de4","modified":1638944857965},{"_id":"themes/next_8.8/test/tags/index.js","hash":"5cad001936a694bf32d59751cc2b68a66199f976","modified":1638944857966},{"_id":"themes/next_8.8/test/tags/group-pictures.js","hash":"8f66d3c6f03fb11d85aa2ab05c9b3c9aa2b4e994","modified":1638944857966},{"_id":"themes/next_8.8/test/tags/button.js","hash":"a50ca44eaec3d91c2958e3157d624cd3e68828c7","modified":1638944857965},{"_id":"themes/next_8.8/test/tags/label.js","hash":"6cad7d84c42511459a89cda3971e8ea5cdee0125","modified":1638944857967},{"_id":"themes/next_8.8/test/tags/mermaid.js","hash":"f718a3d0e303d842e2ca5a3b162539a49e45a520","modified":1638944857967},{"_id":"themes/next_8.8/test/tags/pdf.js","hash":"2d114596a8a180b2f3cd2a9c6528a328961f12d4","modified":1638944857968},{"_id":"themes/next_8.8/test/tags/link-grid.js","hash":"41730266306c02362258384cd73659223928361f","modified":1638944857967},{"_id":"themes/next_8.8/test/tags/video.js","hash":"88db9a3a26cd35525c43c0339fcd1c5965ec9518","modified":1638944857969},{"_id":"themes/next_8.8/test/tags/note.js","hash":"161a81ce749e239d2403681372d48ecc1b51d7b9","modified":1638944857968},{"_id":"themes/next_8.8/layout/_partials/head/head-unique.njk","hash":"bd87e3a877ebab4508fc2b48b41c96b45c4dd970","modified":1638944857869},{"_id":"themes/next_8.8/test/tags/tabs.js","hash":"b19d2592347eae5d6a7a97ca7e8cec03e8f25b51","modified":1638944857968},{"_id":"themes/next_8.8/test/validate/index.js","hash":"560862194991c5963da5a411629d8e6c71d20ee2","modified":1638944857969},{"_id":"themes/next_8.8/layout/_partials/header/index.njk","hash":"1b2ae17f3c394ce310fe2d9ed5f4d07d8cc74ae7","modified":1638944857870},{"_id":"themes/next_8.8/layout/_partials/head/head.njk","hash":"abcc550cb14374fb7452d6edee63967ad9583d1c","modified":1638944857869},{"_id":"themes/next_8.8/layout/_partials/header/brand.njk","hash":"8e08c19e1bd92f3179907b0ff3743d6e2371d7ae","modified":1638944857869},{"_id":"themes/next_8.8/layout/_partials/header/menu-item.njk","hash":"f066390762faf6684a523e2eb943420023aac2b1","modified":1638944857870},{"_id":"themes/next_8.8/layout/_partials/header/menu.njk","hash":"67372599fe025ebe442b73151e5bb56415758356","modified":1638944857870},{"_id":"themes/next_8.8/layout/_partials/page/categories.njk","hash":"b352346dd2cb42f7eeaec5e39d9a2a353b029775","modified":1638944857872},{"_id":"themes/next_8.8/layout/_partials/page/tags.njk","hash":"752df7d12360a077c51a25609916a3ecc1763bb3","modified":1638944857872},{"_id":"themes/next_8.8/layout/_partials/page/schedule.njk","hash":"130e776575d634201d4f8ef3d78dc12624f19fde","modified":1638944857872},{"_id":"themes/next_8.8/layout/_partials/page/page-header.njk","hash":"92553feb26f30f7fc9147bc4ef122908a9da06be","modified":1638944857872},{"_id":"themes/next_8.8/layout/_partials/page/breadcrumb.njk","hash":"9c136edd2248e2d50c1f6110b75e2b75c299bbd7","modified":1638944857871},{"_id":"themes/next_8.8/layout/_partials/header/sub-menu.njk","hash":"940cad08a67e6c361214045096bd3cdffdf44fcf","modified":1638944857870},{"_id":"themes/next_8.8/layout/_partials/post/post-footer.njk","hash":"e3502059bcc443ce932946a9891fcbe8b2bb362d","modified":1638944857874},{"_id":"themes/next_8.8/layout/_partials/post/post-copyright.njk","hash":"0ebc0142abebbeef4278e32abb543c7d7fa75d88","modified":1638944857873},{"_id":"themes/next_8.8/layout/_partials/post/post-reward.njk","hash":"58b3f657a47bae406e5fcf19cd5e42680785ac71","modified":1638944857875},{"_id":"themes/next_8.8/layout/_partials/post/post-followme.njk","hash":"ebf83083856f8bd81ad47ffb985d44e338b4e6bb","modified":1638944857873},{"_id":"themes/next_8.8/layout/_partials/search/algolia-search.njk","hash":"93fbb449fbd599cb4315d7eb0daeb239811b233f","modified":1638944857875},{"_id":"themes/next_8.8/layout/_partials/search/index.njk","hash":"9766852e72c1809d8c1eea71ac6116b4cc0886d2","modified":1638944857876},{"_id":"themes/next_8.8/layout/_partials/post/post-related.njk","hash":"80d3dac42740d2aef677e25165e31c05eb048887","modified":1638944857875},{"_id":"themes/next_8.8/layout/_partials/post/post-meta.njk","hash":"9a9c4fb7e7c4fe4b7d474bdfdb4ed2b0a5423df2","modified":1638944857874},{"_id":"themes/next_8.8/layout/_third-party/analytics/cloudflare.njk","hash":"c7cea42f6db2137c11ca1d83e43fcb7ad7ccfb89","modified":1638944857881},{"_id":"themes/next_8.8/layout/_third-party/analytics/baidu-analytics.njk","hash":"3e80332f88b101141be69f2a07f54ed8c053eabb","modified":1638944857880},{"_id":"themes/next_8.8/layout/_third-party/analytics/growingio.njk","hash":"9ff9ec05c2037beea229a6bb698f9e3546973220","modified":1638944857882},{"_id":"themes/next_8.8/layout/_third-party/analytics/google-analytics.njk","hash":"52ad137450f7b3d6a330e16b3ed1c6174290f0eb","modified":1638944857881},{"_id":"themes/next_8.8/layout/_partials/search/localsearch.njk","hash":"f73d25a8ccfdd5d4ca2953dc434ff8ce36034c57","modified":1638944857876},{"_id":"themes/next_8.8/layout/_third-party/chat/chatra.njk","hash":"09d2c9487d75894d45a823e3237ae9f90fd6ee01","modified":1638944857882},{"_id":"themes/next_8.8/layout/_partials/sidebar/site-overview.njk","hash":"c5c38b4fb137cc799a6ec31f391d1efc12234c8c","modified":1638944857876},{"_id":"themes/next_8.8/layout/_third-party/analytics/index.njk","hash":"465fcffd4216f8ca0ea2613fe9cf7308f71b9da5","modified":1638944857882},{"_id":"themes/next_8.8/layout/_third-party/chat/gitter.njk","hash":"375a86f0b19e130cfa7707007e3a53d9ae7c9b64","modified":1638944857883},{"_id":"themes/next_8.8/layout/_third-party/comments/disqus.njk","hash":"b0828dd1b1fd66ecd612d9e886a08e7579e9a4f7","modified":1638944857884},{"_id":"themes/next_8.8/layout/_third-party/comments/changyan.njk","hash":"5f7967bd946060f4102263a552ddfbae9975e7ea","modified":1638944857884},{"_id":"themes/next_8.8/layout/_third-party/chat/tidio.njk","hash":"3fbc72427c1211e5dcfd269af1a74852a7ba5c1a","modified":1638944857883},{"_id":"themes/next_8.8/layout/_third-party/comments/disqusjs.njk","hash":"c5086b4c35f730f82c99c4a8317f2f153ebde869","modified":1638944857884},{"_id":"themes/next_8.8/layout/_third-party/comments/isso.njk","hash":"38badcc7624a13961381c2465478056b9602aee5","modified":1638944857885},{"_id":"themes/next_8.8/layout/_third-party/comments/livere.njk","hash":"b8e0d5de584cece5e05b03db5b86145aa1e422b4","modified":1638944857885},{"_id":"themes/next_8.8/layout/_third-party/comments/gitalk.njk","hash":"6fd4df5c21cfe530dbb0c012bc0b202f2c362b9c","modified":1638944857884},{"_id":"themes/next_8.8/layout/_third-party/comments/utterances.njk","hash":"a7921be7328e1509d33b435175f5333a9aada66f","modified":1638944857888},{"_id":"themes/next_8.8/layout/_third-party/math/katex.njk","hash":"a84db8bc8804335f95609a221ac1746433dcdc89","modified":1638944857889},{"_id":"themes/next_8.8/layout/_third-party/search/algolia-search.njk","hash":"67f67a77f27103177b9940446f43610229536d82","modified":1638944857891},{"_id":"themes/next_8.8/scripts/events/lib/config.js","hash":"a912944cae0d864458d365867b8a9c89f348e68a","modified":1638944857900},{"_id":"themes/next_8.8/layout/_third-party/math/index.njk","hash":"1856c4b035c5b8e64300a11af0461b519dfc4cf4","modified":1638944857889},{"_id":"themes/next_8.8/layout/_third-party/search/localsearch.njk","hash":"210c32b654adae3d8076c4417d370b42af258cea","modified":1638944857891},{"_id":"themes/next_8.8/layout/_third-party/math/mathjax.njk","hash":"a62aa1ed4e35b8d0451d83f341bf0a97538bc9a4","modified":1638944857890},{"_id":"themes/next_8.8/scripts/events/lib/highlight.js","hash":"00cec6980cafd417def885f496371856cd524a25","modified":1638944857901},{"_id":"themes/next_8.8/scripts/events/lib/injects.js","hash":"1f1ea7b579a49f17574c31d78d663c54896133eb","modified":1638944857901},{"_id":"themes/next_8.8/scripts/events/lib/vendors.js","hash":"2f7057a8d3fce08aa7e2a17d7b7a1f03ac3d8ed6","modified":1638944857902},{"_id":"themes/next_8.8/scripts/events/lib/utils.js","hash":"8508e96a5f883a5a57d8c1b8b5ea438fa29aafd3","modified":1638944857901},{"_id":"themes/next_8.8/layout/_third-party/statistics/busuanzi-counter.njk","hash":"d97790e4b442a1e3ded7d7b4f84b8ee6cdb6e8ea","modified":1638944857892},{"_id":"themes/next_8.8/layout/_third-party/statistics/index.njk","hash":"866ffa15a3250678eb8a90aa6f609fa965db90fd","modified":1638944857895},{"_id":"themes/next_8.8/layout/_third-party/statistics/firestore.njk","hash":"af5336e8bbdc4638435971da115bb7443d374ade","modified":1638944857892},{"_id":"themes/next_8.8/scripts/filters/comment/changyan.js","hash":"cfff8331fdaa2ede4ab08c58cfc6d98c7d2374d9","modified":1638944857902},{"_id":"themes/next_8.8/layout/_third-party/statistics/lean-analytics.njk","hash":"8703d1855bb8d251c9b7c2940b7e3be525e53000","modified":1638944857895},{"_id":"themes/next_8.8/layout/_third-party/tags/mermaid.njk","hash":"dd8f963acd5a3685be46fd5319c06df0308d99b2","modified":1638944857896},{"_id":"themes/next_8.8/layout/_third-party/tags/pdf.njk","hash":"0386c708975cc5faea4f782611c5d2c6b8ac2850","modified":1638944857897},{"_id":"themes/next_8.8/scripts/filters/comment/common.js","hash":"713056d33dbcd8e9748205c5680b456c21174f4e","modified":1638944857903},{"_id":"themes/next_8.8/scripts/filters/comment/default-config.js","hash":"1cb58aa6b88f7461c3c3f9605273686adcc30979","modified":1638944857903},{"_id":"themes/next_8.8/scripts/filters/comment/disqusjs.js","hash":"70eb507ef7f1a4fc3ca71a3814cc57afe7f3f60c","modified":1638944857903},{"_id":"themes/next_8.8/scripts/filters/comment/gitalk.js","hash":"96e58efba0dc76af409cc7d2db225f0fe4526ea8","modified":1638944857904},{"_id":"themes/next_8.8/scripts/filters/comment/disqus.js","hash":"3283bdd6e5ac7d10376df8ddd5faaec5dc1bd667","modified":1638944857903},{"_id":"themes/next_8.8/scripts/filters/comment/livere.js","hash":"bb8ebb541c40362c0cbbd8e83d3b777302bb6c40","modified":1638944857904},{"_id":"themes/next_8.8/scripts/filters/comment/utterances.js","hash":"a50718c081685fd35ff8ea9ca13682c284399ed8","modified":1638944857905},{"_id":"themes/next_8.8/scripts/filters/comment/isso.js","hash":"c22cbccd7d514947e084eeac6a3af1aa41ec857a","modified":1638944857904},{"_id":"themes/next_8.8/source/css/_variables/Muse.styl","hash":"d3a8f6e71c86926d0c2a247a31d7446d829736d5","modified":1638944857946},{"_id":"themes/next_8.8/source/css/_variables/Mist.styl","hash":"ee5024be8e39605f0c6d71db038e15e0693d0f41","modified":1638944857946},{"_id":"themes/next_8.8/source/css/_variables/Gemini.styl","hash":"c4537fa2de33d98baff2c87a73801770414e0b69","modified":1638944857946},{"_id":"themes/next_8.8/source/css/_variables/Pisces.styl","hash":"58014a2d087c4126058a99b5b1cb7d8a2eb6224d","modified":1638944857947},{"_id":"themes/next_8.8/source/js/third-party/pace.js","hash":"0ebee77b2307bf4b260afb06c060171ef42b7141","modified":1638944857959},{"_id":"themes/next_8.8/source/css/_variables/base.styl","hash":"0876b50a58f114bc0b7982b85c5e5011730253b8","modified":1638944857947},{"_id":"themes/next_8.8/source/js/schemes/muse.js","hash":"e1b4bf9aa47d14c790a0920d7dbb3e9812d4358b","modified":1638944857953},{"_id":"themes/next_8.8/source/js/third-party/rating.js","hash":"a1f44247c18ac00ee3e0026560398429e4c77dd7","modified":1638944857960},{"_id":"themes/next_8.8/source/css/_common/outline/index.styl","hash":"7782dfae7a0f8cd61b936fa8ac980440a7bbd3bb","modified":1638944857927},{"_id":"themes/next_8.8/source/js/third-party/fancybox.js","hash":"8a847a7bbdbc0086dd1de12b82107a854b43f5e5","modified":1638944857958},{"_id":"themes/next_8.8/source/js/third-party/quicklink.js","hash":"539c5bb51244f7f4aa98884f3229d128c1cefc40","modified":1638944857960},{"_id":"themes/next_8.8/source/css/_common/components/index.styl","hash":"991c1f80995cec418dc00d3d6b13e2d911ac9894","modified":1638944857913},{"_id":"themes/next_8.8/source/css/_common/components/back-to-top.styl","hash":"2bbf9046ef2a8f99ef3668bbb8be4e52e9d97bb7","modified":1638944857913},{"_id":"themes/next_8.8/source/css/_common/scaffolding/base.styl","hash":"e2da25ff86d2be5ff0a0cee33c7d4c5e11046736","modified":1639712427384},{"_id":"themes/next_8.8/source/css/_common/outline/mobile.styl","hash":"2db4462e9cb87b8aef3f50f850fed407de16da3e","modified":1638944857927},{"_id":"themes/next_8.8/source/css/_common/scaffolding/base.styl_backup","hash":"1239f1b432a6932b2bb9ebcfbaabf724b8f4e59a","modified":1639711097302},{"_id":"themes/next_8.8/source/css/_common/scaffolding/index.styl","hash":"43045d115f8fe95732c446aa45bf1c97609ff2a5","modified":1638944857933},{"_id":"themes/next_8.8/source/css/_common/scaffolding/toggles.styl","hash":"90f7d3baab061e860172b536c9edc38c7fd2ef5c","modified":1638944857938},{"_id":"themes/next_8.8/source/css/_common/components/reading-progress.styl","hash":"f3defd56be33dba4866a695396d96c767ce63182","modified":1638944857920},{"_id":"themes/next_8.8/source/css/_common/scaffolding/comments.styl","hash":"cf8446f4378dcab27b55ede1635c608ae6b8a5c8","modified":1638944857932},{"_id":"themes/next_8.8/source/css/_common/scaffolding/buttons.styl","hash":"f768ecb2fe3e9384777c1c115cd7409e9155edd7","modified":1638944857932},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tables.styl","hash":"b9388016f8d9274703e77e306a1feaad1b7b9d6c","modified":1638944857934},{"_id":"themes/next_8.8/source/css/_schemes/Mist/_header.styl","hash":"b1054313ca9419e76fea0451417c881616f50a38","modified":1638944857939},{"_id":"themes/next_8.8/source/css/_common/scaffolding/pagination.styl","hash":"34416a5792d0235caa8c0c7e59725f2df0fa614c","modified":1638944857934},{"_id":"themes/next_8.8/source/css/_schemes/Mist/_layout.styl","hash":"00366a6bd1a66f99f845c5ebfc9e8cf56651b815","modified":1638944857939},{"_id":"themes/next_8.8/source/css/_schemes/Mist/_menu.styl","hash":"f337981f8f20944ed366694aea88146c7b0a13ab","modified":1638944857940},{"_id":"themes/next_8.8/source/css/_common/scaffolding/normalize.styl","hash":"6d740699fb6a7640647a8fd77c4ea4992d8d6437","modified":1638944857933},{"_id":"themes/next_8.8/source/css/_schemes/Mist/index.styl","hash":"89bf3f6b82cb0fafbbd483431df8f450857c5a0b","modified":1638944857941},{"_id":"themes/next_8.8/source/css/_schemes/Muse/_header.styl","hash":"fd89988442f380cba907752fe3f608e3498f8c93","modified":1638944857941},{"_id":"themes/next_8.8/source/css/_schemes/Muse/_layout.styl","hash":"018b6a761e197086174c9f06b4d5ea21cc230951","modified":1638944857941},{"_id":"themes/next_8.8/source/css/_schemes/Mist/_posts-expand.styl","hash":"c9a9e07b721bb2376e24753ae0a9452431439114","modified":1638944857941},{"_id":"themes/next_8.8/source/css/_schemes/Muse/index.styl","hash":"25c2a7930da14f023329df20f38df2728057fb4d","modified":1638944857943},{"_id":"themes/next_8.8/source/css/_schemes/Muse/_menu.styl","hash":"28030c61288cc0e1321b18373a5c79029fd76a53","modified":1638944857942},{"_id":"themes/next_8.8/source/css/_schemes/Muse/_sidebar.styl","hash":"134272cb8096156c9e32fbbe085394633c7509cd","modified":1638944857942},{"_id":"themes/next_8.8/source/css/_schemes/Muse/_sub-menu.styl","hash":"2d3e05015796a790abd9d68957a5c698c0c9f9b6","modified":1638944857943},{"_id":"themes/next_8.8/source/css/_schemes/Pisces/_header.styl","hash":"9b2cba0c9aa5a64957294f7548c199db1f63f0f4","modified":1638944857943},{"_id":"themes/next_8.8/source/css/_schemes/Gemini/index.styl","hash":"f51b6a4f06359ed56b2d10caa6f15362d3b3751d","modified":1638944857939},{"_id":"themes/next_8.8/source/css/_schemes/Pisces/_layout.styl","hash":"9f60d501808f67d151af437221d0dfacc27c180c","modified":1638944857944},{"_id":"themes/next_8.8/source/css/_schemes/Pisces/_sub-menu.styl","hash":"b5c3dd08c520a16ee49f85fa12b4935e725ef261","modified":1638944857945},{"_id":"themes/next_8.8/source/css/_schemes/Pisces/index.styl","hash":"7905f428b46d100ac5928875cb1e2b99fa86fc0b","modified":1638944857945},{"_id":"themes/next_8.8/source/css/_schemes/Pisces/_menu.styl","hash":"1d29eca70fa686d895f8e98a283e4a159e40905a","modified":1638944857944},{"_id":"themes/next_8.8/source/css/_schemes/Pisces/_sidebar.styl","hash":"42bf453def88da82c842dca84e8f47087091f08e","modified":1638944857944},{"_id":"themes/next_8.8/source/js/third-party/analytics/growingio.js","hash":"f755e8537ccbbb0bd84c26923f320d4e206e7428","modified":1638944857954},{"_id":"themes/next_8.8/source/js/third-party/analytics/baidu-analytics.js","hash":"f9579a02599de063ccff336177ba964a2931a6e9","modified":1638944857954},{"_id":"themes/next_8.8/source/js/third-party/analytics/google-analytics.js","hash":"d77d4934d959e7125128754b568f1d041c3fbfff","modified":1638944857954},{"_id":"themes/next_8.8/source/js/third-party/comments/disqusjs.js","hash":"1c282d6c2151346d1f0aa95055d17abe77054ec9","modified":1638944857957},{"_id":"themes/next_8.8/source/js/third-party/comments/gitalk.js","hash":"1e8509356fb027d948d118ab220d9631f4d482fa","modified":1638944857957},{"_id":"themes/next_8.8/source/js/third-party/comments/disqus.js","hash":"5460de247c038d6cfbe774d7f8747f0a958d9017","modified":1638944857956},{"_id":"themes/next_8.8/source/js/third-party/comments/changyan.js","hash":"b1dd519dc3b1153c9d2ba2d35f68ca8f73f33bae","modified":1638944857956},{"_id":"themes/next_8.8/source/js/third-party/comments/livere.js","hash":"68892d74ef5fc308c6e7e6b4f190826d79f3055d","modified":1638944857957},{"_id":"themes/next_8.8/source/js/third-party/comments/isso.js","hash":"b9b9fd2f0e098a123b34a4932da912a9485ffe6c","modified":1638944857957},{"_id":"themes/next_8.8/source/js/third-party/comments/utterances.js","hash":"ec44d7f1c8b51b0aa3cccba099a78f3575ac828c","modified":1638944857958},{"_id":"themes/next_8.8/source/js/third-party/chat/gitter.js","hash":"14b024c920a8b359777d79dd8e1a849387f8f3ad","modified":1638944857955},{"_id":"themes/next_8.8/source/js/third-party/chat/chatra.js","hash":"72e0766752b78a723fb30e92d533a8b353104e2d","modified":1638944857955},{"_id":"themes/next_8.8/source/js/third-party/math/katex.js","hash":"5c63ec71458b4fe0cd98fd4a04e11c3746764f11","modified":1638944857959},{"_id":"themes/next_8.8/source/js/third-party/chat/tidio.js","hash":"77c231bcd64f1c09bd9989909e9fee703b65f47f","modified":1638944857955},{"_id":"themes/next_8.8/source/js/third-party/search/local-search.js","hash":"dc2b0e89aa32afc7f7a7e2d7a277dadb7f96e06d","modified":1638944857961},{"_id":"themes/next_8.8/source/js/third-party/statistics/firestore.js","hash":"d0829fe41d2fe86b8499e2a896556c1275ea0066","modified":1638944857961},{"_id":"themes/next_8.8/source/js/third-party/math/mathjax.js","hash":"d93556184b2c0aa1dbc4a6fb892d2f77b80d7d9f","modified":1638944857959},{"_id":"themes/next_8.8/source/js/third-party/search/algolia-search.js","hash":"ea94731438d8c518d946601f8f46a65b92381fac","modified":1638944857960},{"_id":"themes/next_8.8/source/js/third-party/tags/pdf.js","hash":"e109c2d6828f527f0289d5fa3bb02fce63ee6d93","modified":1638944857962},{"_id":"themes/next_8.8/source/css/_common/outline/header/bookmark.styl","hash":"c8648c8ea3105556be0068d9fb2735261d0d94bc","modified":1638944857923},{"_id":"themes/next_8.8/source/css/_common/outline/header/index.styl","hash":"67fc7a1eb59c8451eec34e572cbb2fd1424757bc","modified":1638944857924},{"_id":"themes/next_8.8/source/js/third-party/statistics/lean-analytics.js","hash":"6abdc209f4503d4efd676e18bc30ddea813b6ff9","modified":1638944857961},{"_id":"themes/next_8.8/source/css/_common/outline/header/menu.styl","hash":"2db695204d39e4c7daa7b91585a0ea4b06b49f11","modified":1638944857926},{"_id":"themes/next_8.8/source/css/_common/outline/footer/index.styl","hash":"02b6d1a53f7a02c6b0929b11f3ab904b5b873a0e","modified":1638944857923},{"_id":"themes/next_8.8/source/js/third-party/tags/mermaid.js","hash":"2618135cbcee6bf228f6734767de1995e5eaaac6","modified":1638944857962},{"_id":"themes/next_8.8/source/css/_common/outline/header/github-banner.styl","hash":"05af22f3edc2383a3d97ec4c05e9ac43b014bead","modified":1638944857924},{"_id":"themes/next_8.8/source/css/_common/outline/header/site-nav.styl","hash":"d9bc2b520636b9df7f946295cd430593df4118ff","modified":1638944857926},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/sidebar-author-links.styl","hash":"2c2bfbc34b6f19d262ae7c041474985e12f4f4ad","modified":1638944857928},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/sidebar-dimmer.styl","hash":"57ed6770535ecb2e6485a0c87d4de6d6476368b9","modified":1638944857929},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/sidebar-button.styl","hash":"63d8f5f169c2b1c969928fc79244c5fe89ee484e","modified":1638944857929},{"_id":"themes/next_8.8/source/css/_common/outline/header/site-meta.styl","hash":"86b0925e968f35bbc76b473a861e8f9797f7580e","modified":1638944857926},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/sidebar-author.styl","hash":"d8a028f532d562e6a86bb3b9c7b992e4b6dbbb51","modified":1638944857929},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/index.styl","hash":"9964a96f9a647cfb16b97679eced79d07e084e6d","modified":1638944857928},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/sidebar-blogroll.styl","hash":"1c324d56ae83e96db2c4c6d63edd7ee51c936fc1","modified":1638944857929},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/sidebar-toggle.styl","hash":"6681ffe283f8a7e3c86310ef4f6ca1e499c1a19f","modified":1638944857930},{"_id":"themes/next_8.8/source/css/_common/components/pages/breadcrumb.styl","hash":"fde10ce94e9ae21a03b60d41d532835b54abdcb1","modified":1638944857914},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/site-state.styl","hash":"2de038def2cb91da143b14696366c14a66e0e569","modified":1638944857931},{"_id":"themes/next_8.8/source/css/_common/components/pages/index.styl","hash":"6cf78a379bb656cc0abb4ab80fcae60152ce41ad","modified":1638944857915},{"_id":"themes/next_8.8/source/css/_common/components/pages/categories.styl","hash":"80595d274f593b321c0b644a06f3165fe07b16f5","modified":1638944857914},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/sidebar-nav.styl","hash":"db4f3263b2b6551dd56bfdf33cceaf81661a3611","modified":1638944857930},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/sidebar-toc.styl","hash":"081345490271840855d1238b969dbf2e0a2bba8f","modified":1638944857930},{"_id":"themes/next_8.8/source/css/_common/components/pages/schedule.styl","hash":"091b8c763e43447d087c122a86538f290f83136a","modified":1638944857915},{"_id":"themes/next_8.8/source/css/_common/components/pages/tag-cloud.styl","hash":"56d719bcdcba3d725141c55bbd4b168f3942f912","modified":1638944857916},{"_id":"themes/next_8.8/source/css/_common/components/third-party/gitalk.styl","hash":"fb165c1a0d990c5cf98b87773e0dc50410229b96","modified":1638944857920},{"_id":"themes/next_8.8/source/css/_common/components/third-party/math.styl","hash":"1e5776ad4c5c8bcf7596ac74dcabc30704b3f5a0","modified":1638944857921},{"_id":"themes/next_8.8/source/css/_common/components/third-party/index.styl","hash":"25ea9a0af888355b3a046db1100b5cb0e2d6ef6e","modified":1638944857921},{"_id":"themes/next_8.8/source/css/_common/components/third-party/related-posts.styl","hash":"0527153aa821bdbdb84c7b47f60e3cefd95a742f","modified":1638944857921},{"_id":"themes/next_8.8/source/css/_common/components/third-party/utterances.styl","hash":"d28856f365a9373c4ae6fe1e5673d63df2dfd65f","modified":1638944857922},{"_id":"themes/next_8.8/source/css/_common/components/third-party/disqusjs.styl","hash":"c1e9edbfd1c3696b35d5452ae2e6d766f3fe91aa","modified":1638944857920},{"_id":"themes/next_8.8/source/css/_common/components/post/index.styl","hash":"df2fbd0ada00f37439b0de965c6f1c29d3c97429","modified":1638944857916},{"_id":"themes/next_8.8/source/css/_common/components/third-party/search.styl","hash":"49c26184580fde8a732899a4de5aae8662e289b8","modified":1638944857922},{"_id":"themes/next_8.8/source/css/_common/components/post/post-body.styl","hash":"7a34d020877273dcf11c25fa481409300efb8659","modified":1638944857917},{"_id":"themes/next_8.8/source/css/_common/components/post/post-gallery.styl","hash":"c34936a17c3d8af6c0988ac6746d7509dc0b50eb","modified":1638944857918},{"_id":"themes/next_8.8/source/css/_common/components/post/post-followme.styl","hash":"791bc9befb0d4d06e3e517eccfe0bc3551a02a60","modified":1638944857917},{"_id":"themes/next_8.8/source/css/_common/components/post/post-nav.styl","hash":"69dff7cf231d01f85671758455726dd666664a73","modified":1638944857918},{"_id":"themes/next_8.8/source/css/_common/components/post/post-collapse.styl","hash":"eebe3013a9a976011570dce2d04dfeae4c31d790","modified":1638944857917},{"_id":"themes/next_8.8/source/css/_common/components/post/post-footer.styl","hash":"e53a5eb1d1771e284044bdb0bc0ed2de27923669","modified":1638944857918},{"_id":"themes/next_8.8/source/css/_common/components/post/post-widgets.styl","hash":"0a779f955a0e25df0852e0731517dadb234aa181","modified":1638944857919},{"_id":"themes/next_8.8/source/css/_common/components/post/post-reward.styl","hash":"9043d9bc2db35ca000c79258ef89fdb161dc43fb","modified":1638944857919},{"_id":"themes/next_8.8/source/css/_common/components/post/post-header.styl","hash":"4d29b6ae7ed3dc44b10df851a4128b6441efa8be","modified":1638944857918},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tags/group-pictures.styl","hash":"8d9218980e185210ce034e9769ab639b9630fd88","modified":1638944857935},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tags/index.styl","hash":"e22fde6f1657d311d46f64d868c4491d535c8caa","modified":1638944857935},{"_id":"themes/next_8.8/source/css/_common/scaffolding/highlight/copy-code.styl","hash":"a4003e1408844568cb5102a5a111046cb19b2d31","modified":1638944857933},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tags/mermaid.styl","hash":"c7754dc6c866928b538f0863a05b96ec44b5e986","modified":1638944857936},{"_id":"themes/next_8.8/source/css/_common/scaffolding/highlight/index.styl","hash":"5f706f3382652835379cf9b9fec24ccd4513ab65","modified":1638944857933},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tags/blockquote-center.styl","hash":"6b3680e0dbea8e14c1cec24ef63b7fae5e37f7ef","modified":1638944857935},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tags/label.styl","hash":"531daf2612c6217950677a2d03924459ce57c291","modified":1638944857936},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tags/pdf.styl","hash":"77122986509a6b4968bae2729417b7016137534c","modified":1638944857937},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tags/note.styl","hash":"2e9dc3b3546e19e9de18050ad04b1741841116bc","modified":1638944857937},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tags/link-grid.styl","hash":"7075dd32dd70da1e161e4bd14b46f1e8be62fa3c","modified":1638944857936},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tags/tabs.styl","hash":"40a38f2129617ffd4e8d5cd78e982fdfc9941acf","modified":1638944857937},{"_id":"public/sitemap.xml","hash":"d89096fbb3fffd398a7e2e1b5ed5751fb72540f4","modified":1640330174162},{"_id":"public/about/index.html","hash":"d28cb64ed42ec6525218eb0124c251a9f04e54fa","modified":1640330174162},{"_id":"public/tags/index.html","hash":"340fe3077c90ae15e792e14a2c082cb5320d9931","modified":1640330174162},{"_id":"public/categories/index.html","hash":"5ea42781517321598531135fd506ce0f53bf2cd8","modified":1640330174162},{"_id":"public/2021/12/18/Transformer-and-BERT/index.html","hash":"3963ceeb8ce6eb3c4e8ca009a6335c24624f2711","modified":1640330174162},{"_id":"public/2021/12/17/Algorithm/index.html","hash":"387bcce2e8a3b832f6f520e3525d92d317d3c765","modified":1640330174162},{"_id":"public/2021/12/10/An-Introduction-to-Git/index.html","hash":"199457bc1a10af0f1ecdaa2786bc7c1bb63ae709","modified":1640330174162},{"_id":"public/archives/index.html","hash":"a9b9e6ede9b08c1ee70f4c19fea79e2f012dc49e","modified":1640330174162},{"_id":"public/archives/2021/index.html","hash":"555c7173f577a41d6a9eaaad27f2c2e73cfc29bb","modified":1640330174162},{"_id":"public/archives/2021/11/index.html","hash":"1f14703964511a24f2b142afe3995e07313c4020","modified":1640330174162},{"_id":"public/archives/2021/12/index.html","hash":"032f7a07ba4ba0e45d747bbd4368365d7ba04bd3","modified":1640330174162},{"_id":"public/categories/Programming/index.html","hash":"f636f89a7d42a7007e6dd0e6bd9b4fd9476efa04","modified":1640330174162},{"_id":"public/categories/Little-Things/index.html","hash":"4dd1c0699d34f7a6477ca4bc2da82660b36a553a","modified":1640330174162},{"_id":"public/categories/Experiments/index.html","hash":"fa28bd576ce3cb6e3fec75f7454a76509ee95df4","modified":1640330174162},{"_id":"public/categories/Reinforcement-Learning/index.html","hash":"8277b43d2467b80a3fcfbd69ea043ad575c74030","modified":1640330174162},{"_id":"public/categories/About-Papers/index.html","hash":"715e2d9e722c601b3deb1fd7443f0800ff03cda7","modified":1640330174162},{"_id":"public/categories/Little-Things/Git/index.html","hash":"d6cde484d91b845205ae5ddc3ba9ace84a3dcc77","modified":1640330174162},{"_id":"public/categories/Natural-Language-Processing/index.html","hash":"7ae2c9af999506c35274a7b1d0043e24bb715b06","modified":1640330174162},{"_id":"public/categories/Little-Things/Hexo/index.html","hash":"05a95d88b96eccc566ed734d8fa126f649ca7ac6","modified":1640330174162},{"_id":"public/tags/Algorithm/index.html","hash":"d2b7dc4ffda784fde361f2cba675f81567ee8b17","modified":1640330174162},{"_id":"public/tags/Programming/index.html","hash":"001a2cd69de113e5c16caafb96943cbeb836bb54","modified":1640330174162},{"_id":"public/tags/Git/index.html","hash":"8c0200599169235237ae2e4f3ab9d4dc3c927cdc","modified":1640330174162},{"_id":"public/tags/Hexo/index.html","hash":"53632b50fd5fe7190c1821f27b697cdebfd094a0","modified":1640330174162},{"_id":"public/tags/Personal-Thought/index.html","hash":"337469a02ee2b166077b5b83f21759636aa6d95b","modified":1640330174162},{"_id":"public/tags/Experiments/index.html","hash":"6e089dcd5c63bbf5ae6c54b6eef397528f1d07ca","modified":1640330174162},{"_id":"public/tags/private/index.html","hash":"8faf220ad372e236efd40ea2b1d29ed7ee916f3d","modified":1640330174162},{"_id":"public/tags/Reinforcement-Learning/index.html","hash":"b0f883719c55e54381555ac2add832a6ef47f5af","modified":1640330174162},{"_id":"public/tags/Papers/index.html","hash":"20f6bd59b8e988ad2a8ea986d87a11c4e091ad13","modified":1640330174162},{"_id":"public/tags/Natural-Language-Processing/index.html","hash":"8e09f99c2d03cc09bae1263d603b80b477a7854c","modified":1640330174162},{"_id":"public/tags/Transformer/index.html","hash":"f6d5ac3047804f34300ed48267117820740ec765","modified":1640330174162},{"_id":"public/2021/12/17/Experiments/index.html","hash":"c67d0d20f1898a99ad8de76d92788bb79a873e3d","modified":1640330174162},{"_id":"public/2021/12/15/Personal-Thought/index.html","hash":"8a70650f504cb9119b71a6e541fca361ed6994d5","modified":1640330174162},{"_id":"public/2021/12/14/Tips-in-Papers/index.html","hash":"4469bd91b5514ff8ecca6e30fdd6011870286399","modified":1640330174162},{"_id":"public/2021/12/03/First-Step-to-RL/index.html","hash":"5f2a8e64aa8959283bcac107bab0581c03aa4feb","modified":1640330174162},{"_id":"public/2021/11/24/Construct-Your-Blog-with-Hexo-and-Github/index.html","hash":"fb1fc1c72c5f5533acbea0ae24c1d46fbe2ac11c","modified":1640330174162},{"_id":"public/index.html","hash":"e16cc64f9323544ea180687be8ef01933f0d8c3e","modified":1640330174162},{"_id":"public/tags/BERT/index.html","hash":"9627ca05f62a34823516da7f9056822bf696a4c9","modified":1640330174162},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1640330174162},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1640330174162},{"_id":"public/images/avatar.gif","hash":"2dbc3e2f2d624b2ca1afe6edc2ca17307f1950c8","modified":1640330174162},{"_id":"public/images/logo-algolia-nebula-blue-full.svg","hash":"b85e274207b1392782476a0430feac98db1e7da0","modified":1640330174162},{"_id":"public/images/logo.svg","hash":"2cb74fd3ea2635e015eabc58a8d488aed6cf6417","modified":1640330174162},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1640330174162},{"_id":"public/images/ye_16.ico","hash":"b8fb01b5361da89831d232a831a1532e9822bd72","modified":1640330174162},{"_id":"public/images/ye_32.ico","hash":"375c99cd785d93dd989c36604ffbd10ada71322a","modified":1640330174162},{"_id":"public/2021/12/03/First-Step-to-RL/q_learning.py","hash":"3a68ec1153d26f2c18c9d34ee5deb5e495b8dca9","modified":1640330174162},{"_id":"public/2021/12/03/First-Step-to-RL/policy_network.py","hash":"b3e8f06360cad0084a0656edaed4539f1b01e327","modified":1640330174162},{"_id":"public/2021/12/14/Tips-in-Papers/ADCM.jpg","hash":"ee5b75f3a063e07624c5f03111f98902d885cd8e","modified":1640330174162},{"_id":"public/2021/12/14/Tips-in-Papers/Saccader_eq1.jpg","hash":"2ea2a74f960377a6a9d0bc98ecfadd8ba09cd316","modified":1640330174162},{"_id":"public/2021/12/14/Tips-in-Papers/Saccader_eq3.jpg","hash":"8fa845242bb6f737736d91485ca237e7132e19b5","modified":1640330174162},{"_id":"public/2021/12/14/Tips-in-Papers/Saccader_gl.jpg","hash":"fb8050f8b71a7c667cfa85672b505b3340d2231b","modified":1640330174162},{"_id":"public/2021/12/14/Tips-in-Papers/Saccader_eq2.jpg","hash":"cf5832e89b29e39275d2fa4a2e39592e97f9a39e","modified":1640330174162},{"_id":"public/lib/hbe.js","hash":"136dba00826bdd086153bf0acb5473aea7183ad1","modified":1640330174162},{"_id":"public/2021/12/14/Tips-in-Papers/Saccader_Cell.jpg","hash":"b17e79bf86bb3422527bf51d200ceceeb53ce936","modified":1640330174162},{"_id":"public/2021/12/14/Tips-in-Papers/Saccader_Over.jpg","hash":"deaccd942ffca9e163a9f2c9bdfb69251d1004cb","modified":1640330174162},{"_id":"public/css/hbe.style.css","hash":"b0a0077cb588c0941823905fcc383aa7509ade73","modified":1640330174162},{"_id":"public/css/noscript.css","hash":"54d14cd43dc297950a4a8d39ec9644dd5fc3499f","modified":1640330174162},{"_id":"public/js/bookmark.js","hash":"0f563ffbf05fad30e854e413ab17ff7164ab5a53","modified":1640330174162},{"_id":"public/js/motion.js","hash":"6d4bd07a6f8e1b4083119dca0acb5b289533b619","modified":1640330174162},{"_id":"public/js/comments.js","hash":"66ae2e26ea36a41b72c638ea8b220296638ae952","modified":1640330174162},{"_id":"public/js/schedule.js","hash":"2b43e2d576a308289880befc27580dbb2aa34439","modified":1640330174162},{"_id":"public/js/config.js","hash":"4c4ebbe3b3f3841a26f9d5af6d0ba8bc6da01c54","modified":1640330174162},{"_id":"public/js/next-boot.js","hash":"48497e2156a10155dc42311633a110c9685692c9","modified":1640330174162},{"_id":"public/js/pjax.js","hash":"919f5281c4a04d11cfd94573ecf57b3dbabd3cc8","modified":1640330174162},{"_id":"public/js/comments-buttons.js","hash":"1a7344440321713426a0b2ab17e276b5bdf85ade","modified":1640330174162},{"_id":"public/js/utils.js","hash":"e447160d342b1f93df5214b6a733441039ced439","modified":1640330174162},{"_id":"public/js/third-party/fancybox.js","hash":"c098d14e65dd170537134358d4b8359ad0539c2c","modified":1640330174162},{"_id":"public/js/schemes/muse.js","hash":"9a836d2bcc3defe4bd1ee51f5f4eb7006ebdd41b","modified":1640330174162},{"_id":"public/js/third-party/quicklink.js","hash":"6f58cd7aa8f6f1ab92d5a96551add293f4e55312","modified":1640330174162},{"_id":"public/js/third-party/rating.js","hash":"4e92c2d107ba47b47826829f9668030d5ea9bfb8","modified":1640330174162},{"_id":"public/js/third-party/pace.js","hash":"0ef04218b93561ba4d0ff420d556c3d90a756d32","modified":1640330174162},{"_id":"public/js/third-party/analytics/google-analytics.js","hash":"59684383385059dc4f8a1ff85dbbeb703bcdbcb5","modified":1640330174162},{"_id":"public/js/third-party/analytics/growingio.js","hash":"78dd3cf04082b7dbe6246e404b2aa8e726922402","modified":1640330174162},{"_id":"public/js/third-party/analytics/baidu-analytics.js","hash":"f629acc46ff40c071ffd31b77d5c7616f0fdd778","modified":1640330174162},{"_id":"public/js/third-party/comments/disqus.js","hash":"e1cc671b0d524864fd445e3ab4ade9ee6d07e565","modified":1640330174162},{"_id":"public/js/third-party/comments/disqusjs.js","hash":"b6c58f098473b526d6a3cd35655caf34b77f7cff","modified":1640330174162},{"_id":"public/js/third-party/comments/changyan.js","hash":"8c8ebec444c727b704ea41ad88b0b96ed2e4b8d4","modified":1640330174162},{"_id":"public/js/third-party/comments/gitalk.js","hash":"0ec038cf83e8ec067534f16a54041e47a3c1e59a","modified":1640330174162},{"_id":"public/js/third-party/comments/utterances.js","hash":"f67f90eb03e284c82da2b8cf2f1e31801813c16d","modified":1640330174162},{"_id":"public/js/third-party/comments/isso.js","hash":"753a873b6f566aff5ba77ca23f91b78eb880ca64","modified":1640330174162},{"_id":"public/js/third-party/comments/livere.js","hash":"2247d88c934c765c43013337860774aaa99f0b31","modified":1640330174162},{"_id":"public/js/third-party/chat/gitter.js","hash":"cc38c94125f90dadde11b5ebac7d8bf99a1a08a2","modified":1640330174162},{"_id":"public/js/third-party/chat/chatra.js","hash":"c32180522788c10e51df1803aa6842ef0432ddc9","modified":1640330174162},{"_id":"public/js/third-party/chat/tidio.js","hash":"b0079f6a4601e06ca6fe46e83a2f5af553e9bc3c","modified":1640330174162},{"_id":"public/js/third-party/math/katex.js","hash":"83c54ee536e487a1031783443fe0cb63b1b4767e","modified":1640330174162},{"_id":"public/js/third-party/search/local-search.js","hash":"45c485f82258d246f37deb66884bd2643323ef3a","modified":1640330174162},{"_id":"public/js/third-party/math/mathjax.js","hash":"5c749b9c1c3bb738122d0516211ecff6496d4907","modified":1640330174162},{"_id":"public/js/third-party/statistics/firestore.js","hash":"0960f16107ed61452fb0dffc6ed22dc143de34ef","modified":1640330174162},{"_id":"public/js/third-party/search/algolia-search.js","hash":"ac401e3736d56a3c9cb85ab885744cce0b813c55","modified":1640330174162},{"_id":"public/js/third-party/statistics/lean-analytics.js","hash":"5a928990856b8e456f0663cf3b6b406733672e39","modified":1640330174162},{"_id":"public/js/third-party/tags/mermaid.js","hash":"3dc4628efa6debd6490fc0ebddff2424a7b319d8","modified":1640330174162},{"_id":"public/js/third-party/tags/pdf.js","hash":"af78c22f0e61c8c8aa8794e585e0d632c6d4fcb8","modified":1640330174162},{"_id":"public/css/main.css","hash":"a11ba827a33ddb3529cc9d6ab60b000b66656094","modified":1640330174162},{"_id":"public/2021/12/14/Tips-in-Papers/Tnet_over.jpg","hash":"ed04ca4049c6f909b0bf51ea57ffa9a2b445e1c8","modified":1640330174162},{"_id":"public/2021/12/10/An-Introduction-to-Git/git.jpg","hash":"db9ed8bb86df7e73d5be3bcae4cc8656e4a7a0ed","modified":1640330174162}],"Category":[{"name":"Programming","_id":"ckxk28f620004vgulbzdg99xl"},{"name":"Little Things","_id":"ckxk28f67000avgul58bubcpm"},{"name":"Experiments","_id":"ckxk28f6c000kvguld0yedyr8"},{"name":"Reinforcement Learning","_id":"ckxk28f6d000nvguldl55d8rm"},{"name":"About Papers","_id":"ckxk28f6e000rvgulecm21eqa"},{"name":"Git","parent":"ckxk28f67000avgul58bubcpm","_id":"ckxk28f6f000uvgul8ijm9jpg"},{"name":"Natural Language Processing","_id":"ckxk28f6i0010vgul60i0cjje"},{"name":"Hexo","parent":"ckxk28f67000avgul58bubcpm","_id":"ckxk28f6j0018vgul53s85s60"}],"Data":[],"Page":[{"title":"about","date":"2021-12-12T13:52:49.000Z","_content":"\n\n\nI received the B.S. degree in the School of Engineering and Technology, China University of Geosciences (Beijing) in 2015. I am currently working towards the Ph.D degree in the Beijing Key Laboratory of Work Safety Intelligent Monitoring, the Department of EE, BUPT.\n\nI major in deep learning, computer vision, natural language processing, and reinforcement learning.\n\nI am also interested in high performance computing.\n\n<br/>\n\n<br/>\n\n---\n\n<br/>\n\n<br/>\n\n>      *There is a pleasure in the pathless woods;*\n>      *there is a rapture on the lonely shore;*\n>      *there is society, where none intrudes,*\n>      *by the deep sea, and music in its roar;*\n>      *I love not man the less, but nature more...*\n>                           *by George Gordon Byron* \n\n","source":"about/index.md","raw":"---\ntitle: about\ndate: 2021-12-12 21:52:49\n---\n\n\n\nI received the B.S. degree in the School of Engineering and Technology, China University of Geosciences (Beijing) in 2015. I am currently working towards the Ph.D degree in the Beijing Key Laboratory of Work Safety Intelligent Monitoring, the Department of EE, BUPT.\n\nI major in deep learning, computer vision, natural language processing, and reinforcement learning.\n\nI am also interested in high performance computing.\n\n<br/>\n\n<br/>\n\n---\n\n<br/>\n\n<br/>\n\n>      *There is a pleasure in the pathless woods;*\n>      *there is a rapture on the lonely shore;*\n>      *there is society, where none intrudes,*\n>      *by the deep sea, and music in its roar;*\n>      *I love not man the less, but nature more...*\n>                           *by George Gordon Byron* \n\n","updated":"2021-12-17T08:03:56.458Z","path":"about/index.html","comments":1,"layout":"page","_id":"ckxk28f5t0000vgul90qh97xg","content":"<html><head></head><body><p>I received the B.S. degree in the School of Engineering and Technology, China University of Geosciences (Beijing) in 2015. I am currently working towards the Ph.D degree in the Beijing Key Laboratory of Work Safety Intelligent Monitoring, the Department of EE, BUPT.</p>\n<p>I major in deep learning, computer vision, natural language processing, and reinforcement learning.</p>\n<p>I am also interested in high performance computing.</p>\n<br>\n\n<br>\n\n<hr>\n<br>\n\n<br>\n\n<blockquote>\n<p>     <em>There is a pleasure in the pathless woods;</em><br>     <em>there is a rapture on the lonely shore;</em><br>     <em>there is society, where none intrudes,</em><br>     <em>by the deep sea, and music in its roar;</em><br>     <em>I love not man the less, but nature more</em><br>                          <em>by George Gordon Byron</em> </p>\n</blockquote>\n</body></html>","site":{"data":{}},"excerpt":"<html><head></head><body></body></html>","more":"<p>I received the B.S. degree in the School of Engineering and Technology, China University of Geosciences (Beijing) in 2015. I am currently working towards the Ph.D degree in the Beijing Key Laboratory of Work Safety Intelligent Monitoring, the Department of EE, BUPT.</p>\n<p>I major in deep learning, computer vision, natural language processing, and reinforcement learning.</p>\n<p>I am also interested in high performance computing.</p>\n<br/>\n\n<br/>\n\n<hr>\n<br/>\n\n<br/>\n\n<blockquote>\n<p>     <em>There is a pleasure in the pathless woods;</em><br>     <em>there is a rapture on the lonely shore;</em><br>     <em>there is society, where none intrudes,</em><br>     <em>by the deep sea, and music in its roar;</em><br>     <em>I love not man the less, but nature more</em><br>                          <em>by George Gordon Byron</em> </p>\n</blockquote>\n"},{"title":"tags","date":"2021-12-08T06:48:06.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2021-12-08 14:48:06\ntype: \"tags\"\n---\n","updated":"2021-12-08T06:48:39.840Z","path":"tags/index.html","comments":1,"layout":"page","_id":"ckxk28f5z0002vgulhrb44u31","content":"<html><head></head><body></body></html>","site":{"data":{}},"excerpt":"<html><head></head><body></body></html>","more":""},{"title":"categories","date":"2021-11-24T11:46:36.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2021-11-24 19:46:36\ntype: \"categories\"\n---\n","updated":"2021-11-24T11:51:36.896Z","path":"categories/index.html","comments":1,"layout":"page","_id":"ckxk28f630006vgulgvfq5c19","content":"<html><head></head><body></body></html>","site":{"data":{}},"excerpt":"<html><head></head><body></body></html>","more":""}],"Post":[{"title":"Algorithm","top":false,"cover":false,"toc":true,"mathjax":true,"date":"2021-12-17T08:26:10.000Z","password":null,"summary":null,"description":"","_content":"","source":"_posts/Algorithm.md","raw":"---\ntitle: Algorithm\ntop: false\ncover: false\ntoc: true\nmathjax: true\ndate: 2021-12-17 16:26:10\npassword:\nsummary:\ndescription: \ncategories:\n- Programming\ntags:\n- Algorithm\n- Programming\n---\n","slug":"Algorithm","published":1,"updated":"2021-12-17T08:46:22.799Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckxk28f5w0001vgul3cz5ei18","content":"<html><head></head><body></body></html>","site":{"data":{}},"excerpt":"<html><head></head><body></body></html>","more":""},{"title":"An Introduction to Git","date":"2021-12-10T13:55:33.000Z","description":"gitgit","summary":null,"_content":"\n\n\n\n\n\n\n![The Structure of Git](git.jpg)\n\ngit checkout\n\ngit ls-files\n\n--cached (-c) \n\n--midified (-m)\n\n--delete (-d)\n\n--other (-o)git\n\n","source":"_posts/An-Introduction-to-Git.md","raw":"---\ntitle: An Introduction to Git\ndate: 2021-12-10 21:55:33\ndescription: gitgit\nsummary:\ncategories:\n- Little Things\n- Git\ntags:\n- Git\n---\n\n\n\n\n\n\n\n![The Structure of Git](git.jpg)\n\ngit checkout\n\ngit ls-files\n\n--cached (-c) \n\n--midified (-m)\n\n--delete (-d)\n\n--other (-o)git\n\n","slug":"An-Introduction-to-Git","published":1,"updated":"2021-12-17T08:46:26.683Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckxk28f600003vgulcnsib3rm","content":"<html><head></head><body><p><img src=\"git.jpg\" alt=\"The Structure of Git\"></p>\n<p>git checkout</p>\n<p>git ls-files</p>\n<p>cached (-c) </p>\n<p>midified (-m)</p>\n<p>delete (-d)</p>\n<p>other (-o)git</p>\n</body></html>","site":{"data":{}},"excerpt":"<html><head></head><body></body></html>","more":"<p><img src=\"git.jpg\" alt=\"The Structure of Git\"></p>\n<p>git checkout</p>\n<p>git ls-files</p>\n<p>cached (-c) </p>\n<p>midified (-m)</p>\n<p>delete (-d)</p>\n<p>other (-o)git</p>\n"},{"title":"Construct Your Blog with Hexo and Github","date":"2021-11-24T08:20:43.000Z","description":"Hexo, Next","_content":"\n\n# \n\nhttps://segmentfault.com/a/1190000017986794\n\nhttps://godweiyang.com/2018/04/13/hexo-blog/\n\nhttps://blog.guaoxiaohei.com/posts/Hexo-Level/\n\nhttps://www.itfanr.cc/2021/04/16/hexo-blog-article-encryption/\n\nhttps://www.zhangxinxu.com/wordpress/2021/05/css-html-hr/\n\n# \n\n## hexo d GitHub\n\n_config.yml\n\n\n\nurl: github,  https://xyegithub.github.io/myBlog/\n\nroot: url/myBlog/\n\nhexo clean; hexo g; hexo d\n\n##  git\n\n\n\nhttps://juejin.cn/post/6844904193170341896\n\ndnscmd ipconfig /flushdns\n\n\n\n### \n\nhexo _config.ymlgit`https://github.com/xxx`ssh `git@github.com:xxx/xxx`\n\n## github page \n\n\n\nhttps://mizeri.github.io/2021/04/18/hexo-sitemap-google/\n","source":"_posts/Construct-Your-Blog-with-Hexo-and-Github.md","raw":"---\ntitle: Construct Your Blog with Hexo and Github\ndate: 2021-11-24 16:20:43\ndescription:  Hexo, Next\ntags: \n- Hexo\ncategories:\n- Little Things\n- Hexo\n---\n\n\n# \n\nhttps://segmentfault.com/a/1190000017986794\n\nhttps://godweiyang.com/2018/04/13/hexo-blog/\n\nhttps://blog.guaoxiaohei.com/posts/Hexo-Level/\n\nhttps://www.itfanr.cc/2021/04/16/hexo-blog-article-encryption/\n\nhttps://www.zhangxinxu.com/wordpress/2021/05/css-html-hr/\n\n# \n\n## hexo d GitHub\n\n_config.yml\n\n\n\nurl: github,  https://xyegithub.github.io/myBlog/\n\nroot: url/myBlog/\n\nhexo clean; hexo g; hexo d\n\n##  git\n\n\n\nhttps://juejin.cn/post/6844904193170341896\n\ndnscmd ipconfig /flushdns\n\n\n\n### \n\nhexo _config.ymlgit`https://github.com/xxx`ssh `git@github.com:xxx/xxx`\n\n## github page \n\n\n\nhttps://mizeri.github.io/2021/04/18/hexo-sitemap-google/\n","slug":"Construct-Your-Blog-with-Hexo-and-Github","published":1,"updated":"2021-12-17T03:37:51.057Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckxk28f640007vgul77d21ugl","content":"<html><head></head><body><h1 id=\"\"><span class=\"post-title-index\">1. </span><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p><a href=\"https://segmentfault.com/a/1190000017986794\">https://segmentfault.com/a/1190000017986794</a></p>\n<p><a href=\"https://godweiyang.com/2018/04/13/hexo-blog/\">https://godweiyang.com/2018/04/13/hexo-blog/</a></p>\n<p><a href=\"https://blog.guaoxiaohei.com/posts/Hexo-Level/\">https://blog.guaoxiaohei.com/posts/Hexo-Level/</a></p>\n<p><a href=\"https://www.itfanr.cc/2021/04/16/hexo-blog-article-encryption/\">https://www.itfanr.cc/2021/04/16/hexo-blog-article-encryption/</a></p>\n<p><a href=\"https://www.zhangxinxu.com/wordpress/2021/05/css-html-hr/\">https://www.zhangxinxu.com/wordpress/2021/05/css-html-hr/</a></p>\n<h1 id=\"\"><span class=\"post-title-index\">2. </span><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><h2 id=\"hexo-d-GitHub\"><span class=\"post-title-index\">2.1. </span><a href=\"#hexo-d-GitHub\" class=\"headerlink\" title=\"hexo d GitHub\"></a>hexo d GitHub</h2><p>_config.yml</p>\n<p>url: github,  <a href=\"https://xyegithub.github.io/myBlog/\">https://xyegithub.github.io/myBlog/</a></p>\n<p>root: url/myBlog/</p>\n<p>hexo clean; hexo g; hexo d</p>\n<h2 id=\"git\"><span class=\"post-title-index\">2.2. </span><a href=\"#git\" class=\"headerlink\" title=\"git\"></a>git</h2><p></p>\n<p><a href=\"https://juejin.cn/post/6844904193170341896\">https://juejin.cn/post/6844904193170341896</a></p>\n<p>dnscmd ipconfig /flushdns</p>\n<p></p>\n<h3 id=\"\"><span class=\"post-title-index\">2.2.1. </span><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>hexo _config.ymlgit<code>https://github.com/xxx</code>ssh <code>git@github.com:xxx/xxx</code></p>\n<h2 id=\"github-page-\"><span class=\"post-title-index\">2.3. </span><a href=\"#github-page-\" class=\"headerlink\" title=\"github page \"></a>github page </h2><p></p>\n<p><a href=\"https://mizeri.github.io/2021/04/18/hexo-sitemap-google/\">https://mizeri.github.io/2021/04/18/hexo-sitemap-google/</a></p>\n</body></html>","site":{"data":{}},"excerpt":"<html><head></head><body></body></html>","more":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p><a href=\"https://segmentfault.com/a/1190000017986794\">https://segmentfault.com/a/1190000017986794</a></p>\n<p><a href=\"https://godweiyang.com/2018/04/13/hexo-blog/\">https://godweiyang.com/2018/04/13/hexo-blog/</a></p>\n<p><a href=\"https://blog.guaoxiaohei.com/posts/Hexo-Level/\">https://blog.guaoxiaohei.com/posts/Hexo-Level/</a></p>\n<p><a href=\"https://www.itfanr.cc/2021/04/16/hexo-blog-article-encryption/\">https://www.itfanr.cc/2021/04/16/hexo-blog-article-encryption/</a></p>\n<p><a href=\"https://www.zhangxinxu.com/wordpress/2021/05/css-html-hr/\">https://www.zhangxinxu.com/wordpress/2021/05/css-html-hr/</a></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><h2 id=\"hexo-d-GitHub\"><a href=\"#hexo-d-GitHub\" class=\"headerlink\" title=\"hexo d GitHub\"></a>hexo d GitHub</h2><p>_config.yml</p>\n<p>url: github,  <a href=\"https://xyegithub.github.io/myBlog/\">https://xyegithub.github.io/myBlog/</a></p>\n<p>root: url/myBlog/</p>\n<p>hexo clean; hexo g; hexo d</p>\n<h2 id=\"git\"><a href=\"#git\" class=\"headerlink\" title=\"git\"></a>git</h2><p></p>\n<p><a href=\"https://juejin.cn/post/6844904193170341896\">https://juejin.cn/post/6844904193170341896</a></p>\n<p>dnscmd ipconfig /flushdns</p>\n<p></p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>hexo _config.ymlgit<code>https://github.com/xxx</code>ssh <code>git@github.com:xxx/xxx</code></p>\n<h2 id=\"github-page-\"><a href=\"#github-page-\" class=\"headerlink\" title=\"github page \"></a>github page </h2><p></p>\n<p><a href=\"https://mizeri.github.io/2021/04/18/hexo-sitemap-google/\">https://mizeri.github.io/2021/04/18/hexo-sitemap-google/</a></p>\n"},{"title":"Experiments","top":false,"cover":false,"toc":true,"mathjax":true,"date":"2021-12-17T02:00:56.000Z","password":null,"summary":null,"description":"diy","_content":"\n\n\n# Deep Learning\n\n## Feature Map Multiplication\n\n[source code](https://github.com/xyegithub/Featrue-map-multiplication)\n\ndataset: Caltech101\n\n**shortcutbnRessigmoid**\n\n|                                                          | accuracy |\n| ------------------------------------------------------------ | -------- |\n| `out = self.bn(out) + self.shortcut(x) `                     | 87.62    |\n| `out = (out.sigmoid() + 1)* self.bn_s(self.shortcut(x))`     | 78.23    |\n| `out = (self.bn(out).sigmoid() + 1) * self.bn_s(self.shortcut(x))` | 78.69    |\n| `(self.bn(out).sigmoid() + 0.5) * self.bn_s(self.shortcut(x)) ` | 78.57    |\n| `self.bn_s.bias.data[:]=1`<br>`out = (self.bn(out).sigmoid() + 1) * self.bn_s(self.shortcut(x))    ` | 82.26    |\n| `self.bn_s.bias.data[:]=1`  <br>`out = (self.bn(out).sigmoid() + 0.5) * self.bn_s(self.shortcut(x)) ` | 84.10    |\n| ` self.bn.bias.data[:]=0`<br>`self.bn_s.bias.data[:]=1`<br>` out = (self.bn(out).sigmoid() + 0.5) * self.bn_s(self.shortcut(x))             ` | 84.85    |\n| `self.bn.bias.data[:]=0`<br>`self.bn_s.bias.data[:]=1`<br>`out = (self.bn(out).sigmoid()) * self.bn_s(self.shortcut(x))` | 85.83    |\n| `self.bn.bias.data[:]=0`<br>`self.bn.weight.data[:]=1`<br>`self.bn_s.bias.data[:]=1`<br>`out = (self.bn(out).sigmoid()) * self.bn_s(self.shortcut(x))` | 81.57    |\n\n1. shortcut1\n2. Ressigmoid0.51sigmoidsigmoid0sigmoid0.5\n3. Ressigmoidoutbn0bn\n\n**shortcutbnRessigmoid**\n\n|                                                          | accuracy |\n| ------------------------------------------------------------ | -------- |\n| `out = self.bn(out) + self.shortcut(x) `                     | 87.62    |\n| `out = (self.shortcut(x).sigmoid() + 1)* self.bn_s(out)`     | 83.93    |\n| `out = (self.bn(self.shortcut(x)).sigmoid() + 1) * self.bn_s(out)` | 85.54    |\n| `(self.bn(self.shortcut(x)).sigmoid() + 0.5) * self.bn_s(out) ` | 85.14    |\n| `self.bn_s.bias.data[:]=1`<br>`out = (self.bn(self.shortcut(x)).sigmoid() + 1) * self.bn_s(out)    ` | 85.43    |\n| `self.bn_s.bias.data[:]=1`  <br>`out = (self.bn(self.shortcut(x)).sigmoid() + 0.5) * self.bn_s(out) ` | 87.44    |\n| ` self.bn.bias.data[:]=0`<br>`self.bn_s.bias.data[:]=1`<br>` out = (self.bn(self.shortcut(x)).sigmoid() + 0.5) * self.bn_s(out)             ` | 84.39    |\n| `self.bn.bias.data[:]=0`<br>`self.bn_s.bias.data[:]=1`<br>`out = (self.bn(self.shortcut(x)).sigmoid()) * self.bn_s(out)` | 84.39    |\n| `self.bn.bias.data[:]=0`<br>`self.bn.weight.data[:]=1`<br>`self.bn_s.bias.data[:]=1`<br>`out = (self.bn(self.shortcut(x)).sigmoid()) * self.bn_s(out)` | 84.91    |\n\n**shortcutbnResbn**\n\n|                                                          | Accuracy |\n| ------------------------------------------------------------ | -------- |\n| `out = self.bn(out)* self.bn_s(self.shortcut(x))`            | 77.13    |\n| `self.bn_s.bias.data[:]=1`<br>`out = self.bn(out)* self.bn_s(self.shortcut(x)) ` | 75.75    |\n| `self.bn.bias.data[:] = 1`<br>`out = self.bn(out)* self.bn_s(self.shortcut(x))` | 79.55    |\n| `self.bn.bias.data[:]=1`<br>`self.bn_s.bias.data[:]=1`<br>`out = self.bn(out)* self.bn_s(self.shortcut(x))` | 81.39    |\n| `self.bn.bias.data[:]=1`<br>`self.bn.weight.data[:]=1`<br>`self.bn_s.bias.data[:]=1`<br>`out = self.bn(out)* self.bn_s(self.shortcut(x))` | 81.05    |\n| `self.bn.bias.data[:]=1`<br>`self.bn_s.bias.data[:]=1`<br>`self.bn_s.weight.data[:]=1`<br>`out = self.bn(out)* self.bn_s(self.shortcut(x))` | 80.24    |\n| `self.bn.bias.data[:]=1`<br>`self.bn.weight.data[:]=1`<br>`self.bn_s.bias.data[:]=1`<br>`self.bn_s.weight.data[:]=1`<br>`out = self.bn(out)* self.bn_s(self.shortcut(x))` | 81.22    |\n\n1bn\n\n**shortcutsigmoidRessigmoid**\n\n|                                                          | Accuracy |\n| ------------------------------------------------------------ | -------- |\n| `out = (self.shortcut(x).sigmoid()) * out.sigmoid())`        | 79.44    |\n| `out = (self.shortcut(x).sigmoid()) * (out.sigmoid() + 0.5)` | 70.05    |\n| `out = (self.shortcut(x).sigmoid() + 0.5) * (out.sigmoid())` | 76.61    |\n| `out = (self.shortcut(x).sigmoid()) * (out.sigmoid() + 1)`   | 72.64    |\n| `out = (self.shortcut(x).sigmoid() + 1) * (out.sigmoid())`   | 71.77    |\n\nbnsigmoidbn\n\n|                                                          | Accuracy |\n| ------------------------------------------------------------ | -------- |\n| `out = (self.shortcut(x).sigmoid()) * self.bn(out).sigmoid()` | 86.69    |\n| `out = (self.bn_s(self.shortcut(x)).sigmoid()) * out.sigmoid()` | 79.90    |\n| `self.bn_s.bias.data[:]=0`<br>`out = (self.bn_s(self.shortcut(x)).sigmoid()) * out.sigmoid()` | 76.32    |\n| `self.bn.bias.data[:]=0`<br>`out = (self.shortcut(x).sigmoid()) * self.bn(out).sigmoid()` | 83.99    |\n| `self.bn.bias.data[:]=0 `<br>` out = (self.shortcut(x).sigmoid()) * (self.bn(out).sigmoid() + 0.5)` | 86.52    |\n| `self.bn.bias.data[:]=0 `<br>`out = (self.shortcut(x).sigmoid()) * (self.bn(out).sigmoid() + 1) ` | 82.83    |\n| `self.bn.weight.data[:]=1`<br>`out = (self.shortcut(x).sigmoid()) * self.bn(out).sigmoid()` | 78.74    |\n| `self.bn_s.bias.data[:]=0`<br>`out = (self.bn_s(self.shortcut(x)).sigmoid() + 0.5) * out.sigmoid()` | 73.39    |\n\nbn\n\n|                                                          | Accuracy |\n| ------------------------------------------------------------ | -------- |\n| ` out = (self.bn_s(self.shortcut(x)).sigmoid()) * self.bn(out).sigmoid()` | 86.23    |\n| `self.bn.bias.data[:]=0`<br>`out = (self.bn_s(self.shortcut(x)).sigmoid()) * self.bn(out).sigmoid()` | 86.41    |\n\n","source":"_posts/Experiments.md","raw":"---\ntitle: Experiments\ntop: false\ncover: false\ntoc: true\nmathjax: true\ndate: 2021-12-17 10:00:56\npassword:\nsummary:\ndescription: diy\ncategories:\n- Experiments\ntags:\n- Personal Thought\n- Experiments\n- private\n---\n\n\n\n# Deep Learning\n\n## Feature Map Multiplication\n\n[source code](https://github.com/xyegithub/Featrue-map-multiplication)\n\ndataset: Caltech101\n\n**shortcutbnRessigmoid**\n\n|                                                          | accuracy |\n| ------------------------------------------------------------ | -------- |\n| `out = self.bn(out) + self.shortcut(x) `                     | 87.62    |\n| `out = (out.sigmoid() + 1)* self.bn_s(self.shortcut(x))`     | 78.23    |\n| `out = (self.bn(out).sigmoid() + 1) * self.bn_s(self.shortcut(x))` | 78.69    |\n| `(self.bn(out).sigmoid() + 0.5) * self.bn_s(self.shortcut(x)) ` | 78.57    |\n| `self.bn_s.bias.data[:]=1`<br>`out = (self.bn(out).sigmoid() + 1) * self.bn_s(self.shortcut(x))    ` | 82.26    |\n| `self.bn_s.bias.data[:]=1`  <br>`out = (self.bn(out).sigmoid() + 0.5) * self.bn_s(self.shortcut(x)) ` | 84.10    |\n| ` self.bn.bias.data[:]=0`<br>`self.bn_s.bias.data[:]=1`<br>` out = (self.bn(out).sigmoid() + 0.5) * self.bn_s(self.shortcut(x))             ` | 84.85    |\n| `self.bn.bias.data[:]=0`<br>`self.bn_s.bias.data[:]=1`<br>`out = (self.bn(out).sigmoid()) * self.bn_s(self.shortcut(x))` | 85.83    |\n| `self.bn.bias.data[:]=0`<br>`self.bn.weight.data[:]=1`<br>`self.bn_s.bias.data[:]=1`<br>`out = (self.bn(out).sigmoid()) * self.bn_s(self.shortcut(x))` | 81.57    |\n\n1. shortcut1\n2. Ressigmoid0.51sigmoidsigmoid0sigmoid0.5\n3. Ressigmoidoutbn0bn\n\n**shortcutbnRessigmoid**\n\n|                                                          | accuracy |\n| ------------------------------------------------------------ | -------- |\n| `out = self.bn(out) + self.shortcut(x) `                     | 87.62    |\n| `out = (self.shortcut(x).sigmoid() + 1)* self.bn_s(out)`     | 83.93    |\n| `out = (self.bn(self.shortcut(x)).sigmoid() + 1) * self.bn_s(out)` | 85.54    |\n| `(self.bn(self.shortcut(x)).sigmoid() + 0.5) * self.bn_s(out) ` | 85.14    |\n| `self.bn_s.bias.data[:]=1`<br>`out = (self.bn(self.shortcut(x)).sigmoid() + 1) * self.bn_s(out)    ` | 85.43    |\n| `self.bn_s.bias.data[:]=1`  <br>`out = (self.bn(self.shortcut(x)).sigmoid() + 0.5) * self.bn_s(out) ` | 87.44    |\n| ` self.bn.bias.data[:]=0`<br>`self.bn_s.bias.data[:]=1`<br>` out = (self.bn(self.shortcut(x)).sigmoid() + 0.5) * self.bn_s(out)             ` | 84.39    |\n| `self.bn.bias.data[:]=0`<br>`self.bn_s.bias.data[:]=1`<br>`out = (self.bn(self.shortcut(x)).sigmoid()) * self.bn_s(out)` | 84.39    |\n| `self.bn.bias.data[:]=0`<br>`self.bn.weight.data[:]=1`<br>`self.bn_s.bias.data[:]=1`<br>`out = (self.bn(self.shortcut(x)).sigmoid()) * self.bn_s(out)` | 84.91    |\n\n**shortcutbnResbn**\n\n|                                                          | Accuracy |\n| ------------------------------------------------------------ | -------- |\n| `out = self.bn(out)* self.bn_s(self.shortcut(x))`            | 77.13    |\n| `self.bn_s.bias.data[:]=1`<br>`out = self.bn(out)* self.bn_s(self.shortcut(x)) ` | 75.75    |\n| `self.bn.bias.data[:] = 1`<br>`out = self.bn(out)* self.bn_s(self.shortcut(x))` | 79.55    |\n| `self.bn.bias.data[:]=1`<br>`self.bn_s.bias.data[:]=1`<br>`out = self.bn(out)* self.bn_s(self.shortcut(x))` | 81.39    |\n| `self.bn.bias.data[:]=1`<br>`self.bn.weight.data[:]=1`<br>`self.bn_s.bias.data[:]=1`<br>`out = self.bn(out)* self.bn_s(self.shortcut(x))` | 81.05    |\n| `self.bn.bias.data[:]=1`<br>`self.bn_s.bias.data[:]=1`<br>`self.bn_s.weight.data[:]=1`<br>`out = self.bn(out)* self.bn_s(self.shortcut(x))` | 80.24    |\n| `self.bn.bias.data[:]=1`<br>`self.bn.weight.data[:]=1`<br>`self.bn_s.bias.data[:]=1`<br>`self.bn_s.weight.data[:]=1`<br>`out = self.bn(out)* self.bn_s(self.shortcut(x))` | 81.22    |\n\n1bn\n\n**shortcutsigmoidRessigmoid**\n\n|                                                          | Accuracy |\n| ------------------------------------------------------------ | -------- |\n| `out = (self.shortcut(x).sigmoid()) * out.sigmoid())`        | 79.44    |\n| `out = (self.shortcut(x).sigmoid()) * (out.sigmoid() + 0.5)` | 70.05    |\n| `out = (self.shortcut(x).sigmoid() + 0.5) * (out.sigmoid())` | 76.61    |\n| `out = (self.shortcut(x).sigmoid()) * (out.sigmoid() + 1)`   | 72.64    |\n| `out = (self.shortcut(x).sigmoid() + 1) * (out.sigmoid())`   | 71.77    |\n\nbnsigmoidbn\n\n|                                                          | Accuracy |\n| ------------------------------------------------------------ | -------- |\n| `out = (self.shortcut(x).sigmoid()) * self.bn(out).sigmoid()` | 86.69    |\n| `out = (self.bn_s(self.shortcut(x)).sigmoid()) * out.sigmoid()` | 79.90    |\n| `self.bn_s.bias.data[:]=0`<br>`out = (self.bn_s(self.shortcut(x)).sigmoid()) * out.sigmoid()` | 76.32    |\n| `self.bn.bias.data[:]=0`<br>`out = (self.shortcut(x).sigmoid()) * self.bn(out).sigmoid()` | 83.99    |\n| `self.bn.bias.data[:]=0 `<br>` out = (self.shortcut(x).sigmoid()) * (self.bn(out).sigmoid() + 0.5)` | 86.52    |\n| `self.bn.bias.data[:]=0 `<br>`out = (self.shortcut(x).sigmoid()) * (self.bn(out).sigmoid() + 1) ` | 82.83    |\n| `self.bn.weight.data[:]=1`<br>`out = (self.shortcut(x).sigmoid()) * self.bn(out).sigmoid()` | 78.74    |\n| `self.bn_s.bias.data[:]=0`<br>`out = (self.bn_s(self.shortcut(x)).sigmoid() + 0.5) * out.sigmoid()` | 73.39    |\n\nbn\n\n|                                                          | Accuracy |\n| ------------------------------------------------------------ | -------- |\n| ` out = (self.bn_s(self.shortcut(x)).sigmoid()) * self.bn(out).sigmoid()` | 86.23    |\n| `self.bn.bias.data[:]=0`<br>`out = (self.bn_s(self.shortcut(x)).sigmoid()) * self.bn(out).sigmoid()` | 86.41    |\n\n","slug":"Experiments","published":1,"updated":"2021-12-24T02:26:16.249Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckxk28f650008vgulfvge1air","content":"<div class=\"hbe hbe-container\" id=\"hexo-blog-encrypt\" data-wpm=\"Oh, this is an invalid password. Check and try again, please.\" data-whm=\"OOPS, these decrypted content may changed, but you can still have a look.\">\n  <script id=\"hbeData\" type=\"hbeData\" data-hmacdigest=\"43f76d29e12f98024948db4784d2fbf850ed17de1e50d322dd0ae255a3fffa3b\">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</script>\n  <div class=\"hbe hbe-content\">\n    <div class=\"hbe hbe-input hbe-input-default\">\n      <input class=\"hbe hbe-input-field hbe-input-field-default\" type=\"password\" id=\"hbePass\">\n      <label class=\"hbe hbe-input-label hbe-input-label-default\" for=\"hbePass\">\n        <span class=\"hbe hbe-input-label-content hbe-input-label-content-default\">Hey, password is required here.</span>\n      </label>\n    </div>\n  </div>\n</div>\n<script data-pjax src=\"/myBlog/lib/hbe.js\"></script><link href=\"/myBlog/css/hbe.style.css\" rel=\"stylesheet\" type=\"text/css\">","site":{"data":{}},"excerpt":"","more":"","origin":"<html><head></head><body><h1 id=\"Deep-Learning\"><span class=\"post-title-index\">1. </span><a href=\"#Deep-Learning\" class=\"headerlink\" title=\"Deep Learning\"></a>Deep Learning</h1><h2 id=\"Feature-Map-Multiplication\"><span class=\"post-title-index\">1.1. </span><a href=\"#Feature-Map-Multiplication\" class=\"headerlink\" title=\"Feature Map Multiplication\"></a>Feature Map Multiplication</h2><p><a href=\"https://github.com/xyegithub/Featrue-map-multiplication\">source code</a></p>\n<p>dataset: Caltech101</p>\n<p><strong>shortcutbnRessigmoid</strong></p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>accuracy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>out = self.bn(out) + self.shortcut(x) </code></td>\n<td>87.62</td>\n</tr>\n<tr>\n<td><code>out = (out.sigmoid() + 1)* self.bn_s(self.shortcut(x))</code></td>\n<td>78.23</td>\n</tr>\n<tr>\n<td><code>out = (self.bn(out).sigmoid() + 1) * self.bn_s(self.shortcut(x))</code></td>\n<td>78.69</td>\n</tr>\n<tr>\n<td><code>(self.bn(out).sigmoid() + 0.5) * self.bn_s(self.shortcut(x)) </code></td>\n<td>78.57</td>\n</tr>\n<tr>\n<td><code>self.bn_s.bias.data[:]=1</code><br><code>out = (self.bn(out).sigmoid() + 1) * self.bn_s(self.shortcut(x))    </code></td>\n<td>82.26</td>\n</tr>\n<tr>\n<td><code>self.bn_s.bias.data[:]=1</code>  <br><code>out = (self.bn(out).sigmoid() + 0.5) * self.bn_s(self.shortcut(x)) </code></td>\n<td>84.10</td>\n</tr>\n<tr>\n<td><code> self.bn.bias.data[:]=0</code><br><code>self.bn_s.bias.data[:]=1</code><br><code>out = (self.bn(out).sigmoid() + 0.5) * self.bn_s(self.shortcut(x))            </code></td>\n<td>84.85</td>\n</tr>\n<tr>\n<td><code>self.bn.bias.data[:]=0</code><br><code>self.bn_s.bias.data[:]=1</code><br><code>out = (self.bn(out).sigmoid()) * self.bn_s(self.shortcut(x))</code></td>\n<td>85.83</td>\n</tr>\n<tr>\n<td><code>self.bn.bias.data[:]=0</code><br><code>self.bn.weight.data[:]=1</code><br><code>self.bn_s.bias.data[:]=1</code><br><code>out = (self.bn(out).sigmoid()) * self.bn_s(self.shortcut(x))</code></td>\n<td>81.57</td>\n</tr>\n</tbody></table>\n<ol>\n<li>shortcut1</li>\n<li>Ressigmoid0.51sigmoidsigmoid0sigmoid0.5</li>\n<li>Ressigmoidoutbn0bn</li>\n</ol>\n<p><strong>shortcutbnRessigmoid</strong></p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>accuracy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>out = self.bn(out) + self.shortcut(x) </code></td>\n<td>87.62</td>\n</tr>\n<tr>\n<td><code>out = (self.shortcut(x).sigmoid() + 1)* self.bn_s(out)</code></td>\n<td>83.93</td>\n</tr>\n<tr>\n<td><code>out = (self.bn(self.shortcut(x)).sigmoid() + 1) * self.bn_s(out)</code></td>\n<td>85.54</td>\n</tr>\n<tr>\n<td><code>(self.bn(self.shortcut(x)).sigmoid() + 0.5) * self.bn_s(out) </code></td>\n<td>85.14</td>\n</tr>\n<tr>\n<td><code>self.bn_s.bias.data[:]=1</code><br><code>out = (self.bn(self.shortcut(x)).sigmoid() + 1) * self.bn_s(out)    </code></td>\n<td>85.43</td>\n</tr>\n<tr>\n<td><code>self.bn_s.bias.data[:]=1</code>  <br><code>out = (self.bn(self.shortcut(x)).sigmoid() + 0.5) * self.bn_s(out) </code></td>\n<td>87.44</td>\n</tr>\n<tr>\n<td><code> self.bn.bias.data[:]=0</code><br><code>self.bn_s.bias.data[:]=1</code><br><code>out = (self.bn(self.shortcut(x)).sigmoid() + 0.5) * self.bn_s(out)            </code></td>\n<td>84.39</td>\n</tr>\n<tr>\n<td><code>self.bn.bias.data[:]=0</code><br><code>self.bn_s.bias.data[:]=1</code><br><code>out = (self.bn(self.shortcut(x)).sigmoid()) * self.bn_s(out)</code></td>\n<td>84.39</td>\n</tr>\n<tr>\n<td><code>self.bn.bias.data[:]=0</code><br><code>self.bn.weight.data[:]=1</code><br><code>self.bn_s.bias.data[:]=1</code><br><code>out = (self.bn(self.shortcut(x)).sigmoid()) * self.bn_s(out)</code></td>\n<td>84.91</td>\n</tr>\n</tbody></table>\n<p><strong>shortcutbnResbn</strong></p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Accuracy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>out = self.bn(out)* self.bn_s(self.shortcut(x))</code></td>\n<td>77.13</td>\n</tr>\n<tr>\n<td><code>self.bn_s.bias.data[:]=1</code><br><code>out = self.bn(out)* self.bn_s(self.shortcut(x)) </code></td>\n<td>75.75</td>\n</tr>\n<tr>\n<td><code>self.bn.bias.data[:] = 1</code><br><code>out = self.bn(out)* self.bn_s(self.shortcut(x))</code></td>\n<td>79.55</td>\n</tr>\n<tr>\n<td><code>self.bn.bias.data[:]=1</code><br><code>self.bn_s.bias.data[:]=1</code><br><code>out = self.bn(out)* self.bn_s(self.shortcut(x))</code></td>\n<td>81.39</td>\n</tr>\n<tr>\n<td><code>self.bn.bias.data[:]=1</code><br><code>self.bn.weight.data[:]=1</code><br><code>self.bn_s.bias.data[:]=1</code><br><code>out = self.bn(out)* self.bn_s(self.shortcut(x))</code></td>\n<td>81.05</td>\n</tr>\n<tr>\n<td><code>self.bn.bias.data[:]=1</code><br><code>self.bn_s.bias.data[:]=1</code><br><code>self.bn_s.weight.data[:]=1</code><br><code>out = self.bn(out)* self.bn_s(self.shortcut(x))</code></td>\n<td>80.24</td>\n</tr>\n<tr>\n<td><code>self.bn.bias.data[:]=1</code><br><code>self.bn.weight.data[:]=1</code><br><code>self.bn_s.bias.data[:]=1</code><br><code>self.bn_s.weight.data[:]=1</code><br><code>out = self.bn(out)* self.bn_s(self.shortcut(x))</code></td>\n<td>81.22</td>\n</tr>\n</tbody></table>\n<p>1bn</p>\n<p><strong>shortcutsigmoidRessigmoid</strong></p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Accuracy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>out = (self.shortcut(x).sigmoid()) * out.sigmoid())</code></td>\n<td>79.44</td>\n</tr>\n<tr>\n<td><code>out = (self.shortcut(x).sigmoid()) * (out.sigmoid() + 0.5)</code></td>\n<td>70.05</td>\n</tr>\n<tr>\n<td><code>out = (self.shortcut(x).sigmoid() + 0.5) * (out.sigmoid())</code></td>\n<td>76.61</td>\n</tr>\n<tr>\n<td><code>out = (self.shortcut(x).sigmoid()) * (out.sigmoid() + 1)</code></td>\n<td>72.64</td>\n</tr>\n<tr>\n<td><code>out = (self.shortcut(x).sigmoid() + 1) * (out.sigmoid())</code></td>\n<td>71.77</td>\n</tr>\n</tbody></table>\n<p>bnsigmoidbn</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Accuracy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>out = (self.shortcut(x).sigmoid()) * self.bn(out).sigmoid()</code></td>\n<td>86.69</td>\n</tr>\n<tr>\n<td><code>out = (self.bn_s(self.shortcut(x)).sigmoid()) * out.sigmoid()</code></td>\n<td>79.90</td>\n</tr>\n<tr>\n<td><code>self.bn_s.bias.data[:]=0</code><br><code>out = (self.bn_s(self.shortcut(x)).sigmoid()) * out.sigmoid()</code></td>\n<td>76.32</td>\n</tr>\n<tr>\n<td><code>self.bn.bias.data[:]=0</code><br><code>out = (self.shortcut(x).sigmoid()) * self.bn(out).sigmoid()</code></td>\n<td>83.99</td>\n</tr>\n<tr>\n<td><code>self.bn.bias.data[:]=0 </code><br><code> out = (self.shortcut(x).sigmoid()) * (self.bn(out).sigmoid() + 0.5)</code></td>\n<td>86.52</td>\n</tr>\n<tr>\n<td><code>self.bn.bias.data[:]=0 </code><br><code>out = (self.shortcut(x).sigmoid()) * (self.bn(out).sigmoid() + 1) </code></td>\n<td>82.83</td>\n</tr>\n<tr>\n<td><code>self.bn.weight.data[:]=1</code><br><code>out = (self.shortcut(x).sigmoid()) * self.bn(out).sigmoid()</code></td>\n<td>78.74</td>\n</tr>\n<tr>\n<td><code>self.bn_s.bias.data[:]=0</code><br><code>out = (self.bn_s(self.shortcut(x)).sigmoid() + 0.5) * out.sigmoid()</code></td>\n<td>73.39</td>\n</tr>\n</tbody></table>\n<p>bn</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Accuracy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code> out = (self.bn_s(self.shortcut(x)).sigmoid()) * self.bn(out).sigmoid()</code></td>\n<td>86.23</td>\n</tr>\n<tr>\n<td><code>self.bn.bias.data[:]=0</code><br><code>out = (self.bn_s(self.shortcut(x)).sigmoid()) * self.bn(out).sigmoid()</code></td>\n<td>86.41</td>\n</tr>\n</tbody></table>\n</body></html>","encrypt":true},{"title":"First Step to Reinforcement Learning","date":"2021-12-03T08:48:41.000Z","description":"(Policy Network)(Value Network)","_content":"\n\n\n# \n\n<font color=green size=3></font>\n\n<font color=green size=3></font>\n\n<font color=green size=3></font>\n\n\n\n* S\n* A\n* \n* \n* \n* /\n\n## \n\n****\n\n1\n\n2 agent\n\n<font color=red> agent</font>\n\n\n\n\n\n<font color=red>label</font>\n\n\n\n \n\n## \n\n\n\n## \n\n\n\n\n\n\n\n## \n\n\n\n\n\n\n\n<font color=green></font>\n\n## \n\n1. targetActionPS. targetAction\n\n2. lossloss\n\n   0.1\n\n3. 2\n\n4. ActionActionrewardreward\n\n   ActionrewardActionreward\n\n   <font color=green>Actionrewardreward</font>\n\n5. \n\n   \n\n\n\n\n\n**<font color=green>Actionadvatagereward</font>**\n\n## \n\n\n\nActionscore = 0.1 score\n\n\n\nreward\n\n**<font color=green>actionaction ActionrewardActionactionrewardActionActionAction(reward)</font>**\n\nrewardrewardActionrewardAction\n\n\n\n**<font color=green>  rewardAction</font>**\n\n**<font color=green>rewardrewardrewardAction</font>**\n\n\n\n**<font color=green></font>**\n\n# (Policy Network)(Value Network)\n\nAlphaGo \n\n<font color=green></font>ActionActionAction.\n\n\n\n**Policy-based,Value-based**Actionq\n\n\n\n<font color=red>Action+Action</font>ActionActionAction\n\n<font color=green></font>\n\n<font color=green>Value BasedActionPolicy BasedActionActionPolicy BasedValue Based</font>\n\n## (Policy Network)\n\n[Policy_Network.py](https://github.com/xyegithub/myBlog/blob/main/2021/12/03/First-Step-to-RL/policy_network.py) [Policy_Network.py](policy_network.py)\n\n\n\n ```python\n score = tf.matmul(layer1,W2)\n probability = tf.nn.sigmoid(score)#Action 1\n input_y = tf.placeholder(tf.float32,[None,1], \\\\\n                          name=\"input_y\")# \n advantages = tf.placeholder(tf.float32,name=\"reward_signal\") \n # \n loglik = tf.log(input_y*(input_y - probability) + \\\\\n                 (1 - input_y)*(input_y + probability)) \n # 1\n \n # 00\n #\n loss = -tf.reduce_mean(loglik * advantages) # \n #advantages\n #advantages \n #\n \n #\n #\n \n ##  \n #Policy_Network.py ,\n #0.501\n #\n \n ## action\n # + \n \n ## n\n #actionadvantages\n \n ## \n #\n #\n ```\n\n---\n\n\n\n0.5Action\n\n### \n\n\n\n```python\n#xAction 1\ntfprob = sess.run(probability,feed_dict={observations: x})\n# \naction = 1 if np.random.uniform() < tfprob else 0\n```\n\n\n\n```python\n# \n# tfprob = sess.run(probability,feed_dict={observations: x})\n# 0.5\ntfprob = 0.5\naction = 1 if np.random.uniform() < tfprob else 0\n```\n\n\n\n\n\n actionactionaction\n\n\n\n\n\n## (Value NetworkQ-learning)\n\nQ-Learing\n\n[Value_Network.py](https://github.com/xyegithub/myBlog/blob/main/2021/12/03/First-Step-to-RL/policy_network.py)\n\nActionreward\n\n\n\n```python\n#Save the experience to our episode buffer.\nepisodeBuffer.add(np.reshape(np.array([s,a,r,s1,d]),[1,5]))\n# saActionrActionreward\n# s1Actiond  \n```\n\n1. Q(s<sub>t</sub>, a<sub>t</sub>)areward\n\n2. Q<sub>desird</sub>rewardQ<sub>desired</sub>(s<sub>t</sub>, a<sub>t</sub>) = r + $\\lambda$ Max<sub>a</sub> Q<sub>desired</sub>(s<sub>t+1</sub>, a)\n\n3.   Q<sub>desired</sub>\n\n   \n\n\n\n```python\n#Choose an action by greedily (with e chance of random action)\n# from the Q-network\nif np.random.rand(1) < e or total_steps < pre_train_steps:\n    a = np.random.randint(0,4)\nelse:\n    a = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:[s]})[0]\n# total_steps < pre_train_steps\n# total_steps >= pre_train_steps\n# e1-eAction \n# \n# epre_train_steps\n# Action\n# \n# Action\n# \n```\n\n**\"by greedily (with e chance of random action) from the Q-network\"**\n\n**<font color=green></font>**\n\n\n\n```python\nif total_steps > pre_train_steps:\n    if e > endE:\n        e -= stepDrop\n# \n# \n# \n# pass\n# endE=0.1 \n#\n```\n\n\n\n\n\n```python \nif total_steps % (update_freq) == 0:\n    trainBatch = myBuffer.sample(batch_size) #Get a random batch of experiences.\n    #Below we perform the Double-DQN update to the target Q-values\n    # ActiontrainBatch[:,3]\n    # Q(st, at) = r + $\\lambda$ Max Q(s_t+1, a)\n    # Actiont+1Action\n    Q1 = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,3])})\n    # targetreward\n    Q2 = sess.run(targetQN.Qout,feed_dict={targetQN.scalarInput:np.vstack(trainBatch[:,3])})\n    end_multiplier = -(trainBatch[:,4] - 1)\n    # Actiontargetreward\n    # reward, Max Q(s_t+1, a)\n    doubleQ = Q2[range(batch_size),Q1]\n    # Q(st, at) = r + $\\lambda$ Max Q(s_t+1, a)\n    # rewardreward\n    targetQ = trainBatch[:,2] + (y*doubleQ * end_multiplier)\n    #Update the network with our target values.\n    # \n    # \n    _ = sess.run(mainQN.updateModel, \\\n                 feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,0]),mainQN.targetQ:targetQ, mainQN.actions:trainBatch[:,1]})\n    # target\n    updateTarget(targetOps,sess) #Update the target network toward the primary network.\n```\n\n\n\ntarget\n\n```python \ndef updateTargetGraph(tfVars,tau):\n    total_vars = len(tfVars)\n    op_holder = [] \n    # target\n    for idx,var in enumerate(tfVars[0:total_vars//2]):\n        # idx+total_varstarget\n        # target =  * tau + 1- tau*target\n        # target\n        # target\n        # target\n        # targetActionrewardAction\n        # target\n        # target\n        # \n        op_holder.append(tfVars[idx+total_vars//2].assign\\\\\n                         ((var.value()*tau) \\\\\n                      + ((1-tau)*tfVars[idx+total_vars//2].value())))\n        return op_holder\n\n    def updateTarget(op_holder,sess):\n        for op in op_holder:\n            sess.run(op)\n\n```\n\nstate of the arttrick\n\n1. CNN\n2. Experience replayNN batchsizeN=1\n3. target**targetQ-Learing********DQNQQ**targettargettargetQ\n4. Double DQNtricktargetActionactiontargetreward\n5. Dueling DQN\n\n```python \nself.AW = tf.Variable(xavier_init([h_size//2,env.actions]))\nself.VW = tf.Variable(xavier_init([h_size//2,1]))\nself.Advantage = tf.matmul(self.streamA,self.AW)\nself.Value = tf.matmul(self.streamV,self.VW)\n\n#Then combine them together to get our final Q-values.\n# QoutActionrewardValueAdvantage\n# Valueadvantage#action\n# Dueling DQNrewardValue\n# ActionAction\n# \n#out\n# Action\n# \nself.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keep_dims=True))\n\n```\n\n## \n\nreward\n\n1. Action\n2. Actionreward\n3. \n4. ActionAction1ActionrewardAction**rewardreward**\n\n\n\n# \n\n\n\n\n\nsgd\n\n\n\n\n\n\n\n\n\nreward \n\nQ<sub>desired</sub>(s<sub>t</sub>, a<sub>t</sub>) = r + $\\lambda$ Max<sub>a</sub> Q<sub>desired</sub>(s<sub>t+1</sub>, a)\n\n\n\n\n\n```python \nif done:\n    episode_number += 1\n    epx = np.vstack(xs)\n    epy = np.vstack(ys)\n    epr = np.vstack(drs)\n    tfp = tfps\n    xs,hs,dlogps,drs,ys,tfps = [],[],[],[],[],[]\n\n    discounted_epr = discount_rewards(epr)\n    discounted_epr -= np.mean(discounted_epr)\n    discounted_epr //= np.std(discounted_epr)\n\n    tGrad = sess.run(newGrads,feed_dict={observations: epx, input_y: epy, advantages: discounted_epr})\n    for ix,grad in enumerate(tGrad):\n        gradBuffer[ix] += grad\n\n        if episode_number % batch_size == 0:\n            sess.run(updateGrads,feed_dict={W1Grad: gradBuffer[0],W1_1Grad:gradBuffer[1],W2Grad:gradBuffer[2]})\n```\n\n\n\n\n\n","source":"_posts/First-Step-to-RL.md","raw":"---\ntitle: First Step to Reinforcement Learning\ndate: 2021-12-03 16:48:41\ndescription: (Policy Network)(Value Network)\ntags:\n- Reinforcement Learning\ncategories:\n- Reinforcement Learning\n\n\n---\n\n\n\n# \n\n<font color=green size=3></font>\n\n<font color=green size=3></font>\n\n<font color=green size=3></font>\n\n\n\n* S\n* A\n* \n* \n* \n* /\n\n## \n\n****\n\n1\n\n2 agent\n\n<font color=red> agent</font>\n\n\n\n\n\n<font color=red>label</font>\n\n\n\n \n\n## \n\n\n\n## \n\n\n\n\n\n\n\n## \n\n\n\n\n\n\n\n<font color=green></font>\n\n## \n\n1. targetActionPS. targetAction\n\n2. lossloss\n\n   0.1\n\n3. 2\n\n4. ActionActionrewardreward\n\n   ActionrewardActionreward\n\n   <font color=green>Actionrewardreward</font>\n\n5. \n\n   \n\n\n\n\n\n**<font color=green>Actionadvatagereward</font>**\n\n## \n\n\n\nActionscore = 0.1 score\n\n\n\nreward\n\n**<font color=green>actionaction ActionrewardActionactionrewardActionActionAction(reward)</font>**\n\nrewardrewardActionrewardAction\n\n\n\n**<font color=green>  rewardAction</font>**\n\n**<font color=green>rewardrewardrewardAction</font>**\n\n\n\n**<font color=green></font>**\n\n# (Policy Network)(Value Network)\n\nAlphaGo \n\n<font color=green></font>ActionActionAction.\n\n\n\n**Policy-based,Value-based**Actionq\n\n\n\n<font color=red>Action+Action</font>ActionActionAction\n\n<font color=green></font>\n\n<font color=green>Value BasedActionPolicy BasedActionActionPolicy BasedValue Based</font>\n\n## (Policy Network)\n\n[Policy_Network.py](https://github.com/xyegithub/myBlog/blob/main/2021/12/03/First-Step-to-RL/policy_network.py) [Policy_Network.py](policy_network.py)\n\n\n\n ```python\n score = tf.matmul(layer1,W2)\n probability = tf.nn.sigmoid(score)#Action 1\n input_y = tf.placeholder(tf.float32,[None,1], \\\\\n                          name=\"input_y\")# \n advantages = tf.placeholder(tf.float32,name=\"reward_signal\") \n # \n loglik = tf.log(input_y*(input_y - probability) + \\\\\n                 (1 - input_y)*(input_y + probability)) \n # 1\n \n # 00\n #\n loss = -tf.reduce_mean(loglik * advantages) # \n #advantages\n #advantages \n #\n \n #\n #\n \n ##  \n #Policy_Network.py ,\n #0.501\n #\n \n ## action\n # + \n \n ## n\n #actionadvantages\n \n ## \n #\n #\n ```\n\n---\n\n\n\n0.5Action\n\n### \n\n\n\n```python\n#xAction 1\ntfprob = sess.run(probability,feed_dict={observations: x})\n# \naction = 1 if np.random.uniform() < tfprob else 0\n```\n\n\n\n```python\n# \n# tfprob = sess.run(probability,feed_dict={observations: x})\n# 0.5\ntfprob = 0.5\naction = 1 if np.random.uniform() < tfprob else 0\n```\n\n\n\n\n\n actionactionaction\n\n\n\n\n\n## (Value NetworkQ-learning)\n\nQ-Learing\n\n[Value_Network.py](https://github.com/xyegithub/myBlog/blob/main/2021/12/03/First-Step-to-RL/policy_network.py)\n\nActionreward\n\n\n\n```python\n#Save the experience to our episode buffer.\nepisodeBuffer.add(np.reshape(np.array([s,a,r,s1,d]),[1,5]))\n# saActionrActionreward\n# s1Actiond  \n```\n\n1. Q(s<sub>t</sub>, a<sub>t</sub>)areward\n\n2. Q<sub>desird</sub>rewardQ<sub>desired</sub>(s<sub>t</sub>, a<sub>t</sub>) = r + $\\lambda$ Max<sub>a</sub> Q<sub>desired</sub>(s<sub>t+1</sub>, a)\n\n3.   Q<sub>desired</sub>\n\n   \n\n\n\n```python\n#Choose an action by greedily (with e chance of random action)\n# from the Q-network\nif np.random.rand(1) < e or total_steps < pre_train_steps:\n    a = np.random.randint(0,4)\nelse:\n    a = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:[s]})[0]\n# total_steps < pre_train_steps\n# total_steps >= pre_train_steps\n# e1-eAction \n# \n# epre_train_steps\n# Action\n# \n# Action\n# \n```\n\n**\"by greedily (with e chance of random action) from the Q-network\"**\n\n**<font color=green></font>**\n\n\n\n```python\nif total_steps > pre_train_steps:\n    if e > endE:\n        e -= stepDrop\n# \n# \n# \n# pass\n# endE=0.1 \n#\n```\n\n\n\n\n\n```python \nif total_steps % (update_freq) == 0:\n    trainBatch = myBuffer.sample(batch_size) #Get a random batch of experiences.\n    #Below we perform the Double-DQN update to the target Q-values\n    # ActiontrainBatch[:,3]\n    # Q(st, at) = r + $\\lambda$ Max Q(s_t+1, a)\n    # Actiont+1Action\n    Q1 = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,3])})\n    # targetreward\n    Q2 = sess.run(targetQN.Qout,feed_dict={targetQN.scalarInput:np.vstack(trainBatch[:,3])})\n    end_multiplier = -(trainBatch[:,4] - 1)\n    # Actiontargetreward\n    # reward, Max Q(s_t+1, a)\n    doubleQ = Q2[range(batch_size),Q1]\n    # Q(st, at) = r + $\\lambda$ Max Q(s_t+1, a)\n    # rewardreward\n    targetQ = trainBatch[:,2] + (y*doubleQ * end_multiplier)\n    #Update the network with our target values.\n    # \n    # \n    _ = sess.run(mainQN.updateModel, \\\n                 feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,0]),mainQN.targetQ:targetQ, mainQN.actions:trainBatch[:,1]})\n    # target\n    updateTarget(targetOps,sess) #Update the target network toward the primary network.\n```\n\n\n\ntarget\n\n```python \ndef updateTargetGraph(tfVars,tau):\n    total_vars = len(tfVars)\n    op_holder = [] \n    # target\n    for idx,var in enumerate(tfVars[0:total_vars//2]):\n        # idx+total_varstarget\n        # target =  * tau + 1- tau*target\n        # target\n        # target\n        # target\n        # targetActionrewardAction\n        # target\n        # target\n        # \n        op_holder.append(tfVars[idx+total_vars//2].assign\\\\\n                         ((var.value()*tau) \\\\\n                      + ((1-tau)*tfVars[idx+total_vars//2].value())))\n        return op_holder\n\n    def updateTarget(op_holder,sess):\n        for op in op_holder:\n            sess.run(op)\n\n```\n\nstate of the arttrick\n\n1. CNN\n2. Experience replayNN batchsizeN=1\n3. target**targetQ-Learing********DQNQQ**targettargettargetQ\n4. Double DQNtricktargetActionactiontargetreward\n5. Dueling DQN\n\n```python \nself.AW = tf.Variable(xavier_init([h_size//2,env.actions]))\nself.VW = tf.Variable(xavier_init([h_size//2,1]))\nself.Advantage = tf.matmul(self.streamA,self.AW)\nself.Value = tf.matmul(self.streamV,self.VW)\n\n#Then combine them together to get our final Q-values.\n# QoutActionrewardValueAdvantage\n# Valueadvantage#action\n# Dueling DQNrewardValue\n# ActionAction\n# \n#out\n# Action\n# \nself.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keep_dims=True))\n\n```\n\n## \n\nreward\n\n1. Action\n2. Actionreward\n3. \n4. ActionAction1ActionrewardAction**rewardreward**\n\n\n\n# \n\n\n\n\n\nsgd\n\n\n\n\n\n\n\n\n\nreward \n\nQ<sub>desired</sub>(s<sub>t</sub>, a<sub>t</sub>) = r + $\\lambda$ Max<sub>a</sub> Q<sub>desired</sub>(s<sub>t+1</sub>, a)\n\n\n\n\n\n```python \nif done:\n    episode_number += 1\n    epx = np.vstack(xs)\n    epy = np.vstack(ys)\n    epr = np.vstack(drs)\n    tfp = tfps\n    xs,hs,dlogps,drs,ys,tfps = [],[],[],[],[],[]\n\n    discounted_epr = discount_rewards(epr)\n    discounted_epr -= np.mean(discounted_epr)\n    discounted_epr //= np.std(discounted_epr)\n\n    tGrad = sess.run(newGrads,feed_dict={observations: epx, input_y: epy, advantages: discounted_epr})\n    for ix,grad in enumerate(tGrad):\n        gradBuffer[ix] += grad\n\n        if episode_number % batch_size == 0:\n            sess.run(updateGrads,feed_dict={W1Grad: gradBuffer[0],W1_1Grad:gradBuffer[1],W2Grad:gradBuffer[2]})\n```\n\n\n\n\n\n","slug":"First-Step-to-RL","published":1,"updated":"2021-12-16T02:39:14.208Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckxk28f660009vgul7dlkdslm","content":"<html><head></head><body><h1 id=\"\"><span class=\"post-title-index\">1. </span><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p><font color=\"green\" size=\"3\"></font></p>\n<p><font color=\"green\" size=\"3\"></font></p>\n<p><font color=\"green\" size=\"3\"></font></p>\n<p></p>\n<ul>\n<li>S</li>\n<li>A</li>\n<li></li>\n<li></li>\n<li></li>\n<li>/</li>\n</ul>\n<h2 id=\"\"><span class=\"post-title-index\">1.1. </span><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p><strong></strong></p>\n<p>1</p>\n<p>2 agent</p>\n<p><font color=\"red\"> agent</font></p>\n<p></p>\n<p></p>\n<p><font color=\"red\">label</font></p>\n<p></p>\n<p> </p>\n<h2 id=\"\"><span class=\"post-title-index\">1.2. </span><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p></p>\n<h2 id=\"\"><span class=\"post-title-index\">1.3. </span><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p></p>\n<p></p>\n<p></p>\n<h2 id=\"\"><span class=\"post-title-index\">1.4. </span><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p></p>\n<p></p>\n<p></p>\n<p><font color=\"green\"></font></p>\n<h2 id=\"\"><span class=\"post-title-index\">1.5. </span><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><ol>\n<li><p>targetActionPS. targetAction</p>\n</li>\n<li><p>lossloss</p>\n<p>0.1</p>\n</li>\n<li><p>2</p>\n</li>\n<li><p>ActionActionrewardreward</p>\n<p>ActionrewardActionreward</p>\n<p><font color=\"green\">Actionrewardreward</font></p>\n</li>\n<li><p></p>\n</li>\n</ol>\n<p></p>\n<p></p>\n<p><strong><font color=\"green\">Actionadvatagereward</font></strong></p>\n<h2 id=\"\"><span class=\"post-title-index\">1.6. </span><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p></p>\n<p>Actionscore = 0.1 score</p>\n<p></p>\n<p>reward</p>\n<p><strong><font color=\"green\">actionaction ActionrewardActionactionrewardActionActionAction(reward)</font></strong></p>\n<p>rewardrewardActionrewardAction</p>\n<p></p>\n<p><strong><font color=\"green\">  rewardAction</font></strong></p>\n<p><strong><font color=\"green\">rewardrewardrewardAction</font></strong></p>\n<p></p>\n<p><strong><font color=\"green\"></font></strong></p>\n<h1 id=\"-Policy-Network--Value-Network\"><span class=\"post-title-index\">2. </span><a href=\"#-Policy-Network--Value-Network\" class=\"headerlink\" title=\"(Policy Network)(Value Network)\"></a>(Policy Network)(Value Network)</h1><p>AlphaGo </p>\n<p><font color=\"green\"></font>ActionActionAction.</p>\n<p></p>\n<p><strong>Policy-based,Value-based</strong>Actionq</p>\n<p></p>\n<p><font color=\"red\">Action+Action</font>ActionActionAction</p>\n<p><font color=\"green\"></font></p>\n<p><font color=\"green\">Value BasedActionPolicy BasedActionActionPolicy BasedValue Based</font></p>\n<h2 id=\"-Policy-Network\"><span class=\"post-title-index\">2.1. </span><a href=\"#-Policy-Network\" class=\"headerlink\" title=\"(Policy Network)\"></a>(Policy Network)</h2><p><a href=\"https://github.com/xyegithub/myBlog/blob/main/2021/12/03/First-Step-to-RL/policy_network.py\">Policy_Network.py</a> <a href=\"policy_network.py\">Policy_Network.py</a></p>\n<p></p>\n <figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">score = tf.matmul(layer1,W2)</span><br><span class=\"line\">probability = tf.nn.sigmoid(score)<span class=\"comment\">#Action 1</span></span><br><span class=\"line\">input_y = tf.placeholder(tf.float32,[<span class=\"literal\">None</span>,<span class=\"number\">1</span>], \\\\</span><br><span class=\"line\">                         name=<span class=\"string\">\"input_y\"</span>)<span class=\"comment\"># </span></span><br><span class=\"line\">advantages = tf.placeholder(tf.float32,name=<span class=\"string\">\"reward_signal\"</span>) </span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">loglik = tf.log(input_y*(input_y - probability) + \\\\</span><br><span class=\"line\">                (<span class=\"number\">1</span> - input_y)*(input_y + probability)) </span><br><span class=\"line\"><span class=\"comment\"># 1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 00</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\">loss = -tf.reduce_mean(loglik * advantages) <span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\">#advantages</span></span><br><span class=\"line\"><span class=\"comment\">#advantages </span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">##  </span></span><br><span class=\"line\"><span class=\"comment\">#Policy_Network.py ,</span></span><br><span class=\"line\"><span class=\"comment\">#0.501</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## action</span></span><br><span class=\"line\"><span class=\"comment\"># + </span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## n</span></span><br><span class=\"line\"><span class=\"comment\">#actionadvantages</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## </span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br></pre></td></tr></tbody></table></figure>\n\n<p></p>\n<p></p>\n<p>0.5Action</p>\n<h3 id=\"\"><span class=\"post-title-index\">2.1.1. </span><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p></p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#xAction 1</span></span><br><span class=\"line\">tfprob = sess.run(probability,feed_dict={observations: x})</span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">action = <span class=\"number\">1</span> <span class=\"keyword\">if</span> np.random.uniform() &lt; tfprob <span class=\"keyword\">else</span> <span class=\"number\">0</span></span><br></pre></td></tr></tbody></table></figure>\n\n<p></p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\"># tfprob = sess.run(probability,feed_dict={observations: x})</span></span><br><span class=\"line\"><span class=\"comment\"># 0.5</span></span><br><span class=\"line\">tfprob = <span class=\"number\">0.5</span></span><br><span class=\"line\">action = <span class=\"number\">1</span> <span class=\"keyword\">if</span> np.random.uniform() &lt; tfprob <span class=\"keyword\">else</span> <span class=\"number\">0</span></span><br></pre></td></tr></tbody></table></figure>\n\n<p></p>\n<p></p>\n<p> actionactionaction</p>\n<p></p>\n<p></p>\n<h2 id=\"-Value-NetworkQ-learning\"><span class=\"post-title-index\">2.2. </span><a href=\"#-Value-NetworkQ-learning\" class=\"headerlink\" title=\"(Value NetworkQ-learning)\"></a>(Value NetworkQ-learning)</h2><p>Q-Learing</p>\n<p><a href=\"https://github.com/xyegithub/myBlog/blob/main/2021/12/03/First-Step-to-RL/policy_network.py\">Value_Network.py</a></p>\n<p>Actionreward</p>\n<p></p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#Save the experience to our episode buffer.</span></span><br><span class=\"line\">episodeBuffer.add(np.reshape(np.array([s,a,r,s1,d]),[<span class=\"number\">1</span>,<span class=\"number\">5</span>]))</span><br><span class=\"line\"><span class=\"comment\"># saActionrActionreward</span></span><br><span class=\"line\"><span class=\"comment\"># s1Actiond  </span></span><br></pre></td></tr></tbody></table></figure>\n\n<ol>\n<li><p>Q(s<sub>t</sub>, a<sub>t</sub>)areward</p>\n</li>\n<li><p>Q<sub>desird</sub>rewardQ<sub>desired</sub>(s<sub>t</sub>, a<sub>t</sub>) = r + $\\lambda$ Max<sub>a</sub> Q<sub>desired</sub>(s<sub>t+1</sub>, a)</p>\n</li>\n<li><p>  Q<sub>desired</sub></p>\n</li>\n</ol>\n<p></p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#Choose an action by greedily (with e chance of random action)</span></span><br><span class=\"line\"><span class=\"comment\"># from the Q-network</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> np.random.rand(<span class=\"number\">1</span>) &lt; e <span class=\"keyword\">or</span> total_steps &lt; pre_train_steps:</span><br><span class=\"line\">    a = np.random.randint(<span class=\"number\">0</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\"><span class=\"keyword\">else</span>:</span><br><span class=\"line\">    a = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:[s]})[<span class=\"number\">0</span>]</span><br><span class=\"line\"><span class=\"comment\"># total_steps &lt; pre_train_steps</span></span><br><span class=\"line\"><span class=\"comment\"># total_steps &gt;= pre_train_steps</span></span><br><span class=\"line\"><span class=\"comment\"># e1-eAction </span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\"># epre_train_steps</span></span><br><span class=\"line\"><span class=\"comment\"># Action</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\"># Action</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br></pre></td></tr></tbody></table></figure>\n\n<p><strong>by greedily (with e chance of random action) from the Q-network</strong></p>\n<p><strong><font color=\"green\"></font></strong></p>\n<p></p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> total_steps &gt; pre_train_steps:</span><br><span class=\"line\">    <span class=\"keyword\">if</span> e &gt; endE:</span><br><span class=\"line\">        e -= stepDrop</span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\"># pass</span></span><br><span class=\"line\"><span class=\"comment\"># endE=0.1 </span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br></pre></td></tr></tbody></table></figure>\n\n\n\n<p></p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> total_steps % (update_freq) == <span class=\"number\">0</span>:</span><br><span class=\"line\">    trainBatch = myBuffer.sample(batch_size) <span class=\"comment\">#Get a random batch of experiences.</span></span><br><span class=\"line\">    <span class=\"comment\">#Below we perform the Double-DQN update to the target Q-values</span></span><br><span class=\"line\">    <span class=\"comment\"># ActiontrainBatch[:,3]</span></span><br><span class=\"line\">    <span class=\"comment\"># Q(st, at) = r + $\\lambda$ Max Q(s_t+1, a)</span></span><br><span class=\"line\">    <span class=\"comment\"># Actiont+1Action</span></span><br><span class=\"line\">    Q1 = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,<span class=\"number\">3</span>])})</span><br><span class=\"line\">    <span class=\"comment\"># targetreward</span></span><br><span class=\"line\">    Q2 = sess.run(targetQN.Qout,feed_dict={targetQN.scalarInput:np.vstack(trainBatch[:,<span class=\"number\">3</span>])})</span><br><span class=\"line\">    end_multiplier = -(trainBatch[:,<span class=\"number\">4</span>] - <span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"comment\"># Actiontargetreward</span></span><br><span class=\"line\">    <span class=\"comment\"># reward, Max Q(s_t+1, a)</span></span><br><span class=\"line\">    doubleQ = Q2[<span class=\"built_in\">range</span>(batch_size),Q1]</span><br><span class=\"line\">    <span class=\"comment\"># Q(st, at) = r + $\\lambda$ Max Q(s_t+1, a)</span></span><br><span class=\"line\">    <span class=\"comment\"># rewardreward</span></span><br><span class=\"line\">    targetQ = trainBatch[:,<span class=\"number\">2</span>] + (y*doubleQ * end_multiplier)</span><br><span class=\"line\">    <span class=\"comment\">#Update the network with our target values.</span></span><br><span class=\"line\">    <span class=\"comment\"># </span></span><br><span class=\"line\">    <span class=\"comment\"># </span></span><br><span class=\"line\">    _ = sess.run(mainQN.updateModel, \\</span><br><span class=\"line\">                 feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,<span class=\"number\">0</span>]),mainQN.targetQ:targetQ, mainQN.actions:trainBatch[:,<span class=\"number\">1</span>]})</span><br><span class=\"line\">    <span class=\"comment\"># target</span></span><br><span class=\"line\">    updateTarget(targetOps,sess) <span class=\"comment\">#Update the target network toward the primary network.</span></span><br></pre></td></tr></tbody></table></figure>\n\n\n\n<p>target</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">updateTargetGraph</span>(<span class=\"params\">tfVars,tau</span>):</span></span><br><span class=\"line\">    total_vars = <span class=\"built_in\">len</span>(tfVars)</span><br><span class=\"line\">    op_holder = [] </span><br><span class=\"line\">    <span class=\"comment\"># target</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> idx,var <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(tfVars[<span class=\"number\">0</span>:total_vars//<span class=\"number\">2</span>]):</span><br><span class=\"line\">        <span class=\"comment\"># idx+total_varstarget</span></span><br><span class=\"line\">        <span class=\"comment\"># target =  * tau + 1- tau*target</span></span><br><span class=\"line\">        <span class=\"comment\"># target</span></span><br><span class=\"line\">        <span class=\"comment\"># target</span></span><br><span class=\"line\">        <span class=\"comment\"># target</span></span><br><span class=\"line\">        <span class=\"comment\"># targetActionrewardAction</span></span><br><span class=\"line\">        <span class=\"comment\"># target</span></span><br><span class=\"line\">        <span class=\"comment\"># target</span></span><br><span class=\"line\">        <span class=\"comment\"># </span></span><br><span class=\"line\">        op_holder.append(tfVars[idx+total_vars//<span class=\"number\">2</span>].assign\\\\</span><br><span class=\"line\">                         ((var.value()*tau) \\\\</span><br><span class=\"line\">                      + ((<span class=\"number\">1</span>-tau)*tfVars[idx+total_vars//<span class=\"number\">2</span>].value())))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> op_holder</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">updateTarget</span>(<span class=\"params\">op_holder,sess</span>):</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> op <span class=\"keyword\">in</span> op_holder:</span><br><span class=\"line\">            sess.run(op)</span><br><span class=\"line\"></span><br></pre></td></tr></tbody></table></figure>\n\n<p>state of the arttrick</p>\n<ol>\n<li>CNN</li>\n<li>Experience replayNN batchsizeN=1</li>\n<li>target<strong>targetQ-Learing</strong><strong></strong><strong>DQNQQ</strong>targettargettargetQ</li>\n<li>Double DQNtricktargetActionactiontargetreward</li>\n<li>Dueling DQN</li>\n</ol>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">self.AW = tf.Variable(xavier_init([h_size//<span class=\"number\">2</span>,env.actions]))</span><br><span class=\"line\">self.VW = tf.Variable(xavier_init([h_size//<span class=\"number\">2</span>,<span class=\"number\">1</span>]))</span><br><span class=\"line\">self.Advantage = tf.matmul(self.streamA,self.AW)</span><br><span class=\"line\">self.Value = tf.matmul(self.streamV,self.VW)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#Then combine them together to get our final Q-values.</span></span><br><span class=\"line\"><span class=\"comment\"># QoutActionrewardValueAdvantage</span></span><br><span class=\"line\"><span class=\"comment\"># Valueadvantage#action</span></span><br><span class=\"line\"><span class=\"comment\"># Dueling DQNrewardValue</span></span><br><span class=\"line\"><span class=\"comment\"># ActionAction</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\">#out</span></span><br><span class=\"line\"><span class=\"comment\"># Action</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">self.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=<span class=\"number\">1</span>,keep_dims=<span class=\"literal\">True</span>))</span><br><span class=\"line\"></span><br></pre></td></tr></tbody></table></figure>\n\n<h2 id=\"\"><span class=\"post-title-index\">2.3. </span><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>reward</p>\n<ol>\n<li>Action</li>\n<li>Actionreward</li>\n<li></li>\n<li>ActionAction1ActionrewardAction<strong>rewardreward</strong></li>\n</ol>\n<h1 id=\"\"><span class=\"post-title-index\">3. </span><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<p></p>\n<p>sgd</p>\n<p></p>\n<p></p>\n<p>reward </p>\n<p>Q<sub>desired</sub>(s<sub>t</sub>, a<sub>t</sub>) = r + $\\lambda$ Max<sub>a</sub> Q<sub>desired</sub>(s<sub>t+1</sub>, a)</p>\n<p></p>\n<p></p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> done:</span><br><span class=\"line\">    episode_number += <span class=\"number\">1</span></span><br><span class=\"line\">    epx = np.vstack(xs)</span><br><span class=\"line\">    epy = np.vstack(ys)</span><br><span class=\"line\">    epr = np.vstack(drs)</span><br><span class=\"line\">    tfp = tfps</span><br><span class=\"line\">    xs,hs,dlogps,drs,ys,tfps = [],[],[],[],[],[]</span><br><span class=\"line\"></span><br><span class=\"line\">    discounted_epr = discount_rewards(epr)</span><br><span class=\"line\">    discounted_epr -= np.mean(discounted_epr)</span><br><span class=\"line\">    discounted_epr //= np.std(discounted_epr)</span><br><span class=\"line\"></span><br><span class=\"line\">    tGrad = sess.run(newGrads,feed_dict={observations: epx, input_y: epy, advantages: discounted_epr})</span><br><span class=\"line\">    <span class=\"keyword\">for</span> ix,grad <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(tGrad):</span><br><span class=\"line\">        gradBuffer[ix] += grad</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> episode_number % batch_size == <span class=\"number\">0</span>:</span><br><span class=\"line\">            sess.run(updateGrads,feed_dict={W1Grad: gradBuffer[<span class=\"number\">0</span>],W1_1Grad:gradBuffer[<span class=\"number\">1</span>],W2Grad:gradBuffer[<span class=\"number\">2</span>]})</span><br></pre></td></tr></tbody></table></figure>\n\n<p></p>\n</body></html>","site":{"data":{}},"excerpt":"<html><head></head><body></body></html>","more":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p><font color=green size=3></font></p>\n<p><font color=green size=3></font></p>\n<p><font color=green size=3></font></p>\n<p></p>\n<ul>\n<li>S</li>\n<li>A</li>\n<li></li>\n<li></li>\n<li></li>\n<li>/</li>\n</ul>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p><strong></strong></p>\n<p>1</p>\n<p>2 agent</p>\n<p><font color=red> agent</font></p>\n<p></p>\n<p></p>\n<p><font color=red>label</font></p>\n<p></p>\n<p> </p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p></p>\n<p></p>\n<p></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p></p>\n<p></p>\n<p></p>\n<p><font color=green></font></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><ol>\n<li><p>targetActionPS. targetAction</p>\n</li>\n<li><p>lossloss</p>\n<p>0.1</p>\n</li>\n<li><p>2</p>\n</li>\n<li><p>ActionActionrewardreward</p>\n<p>ActionrewardActionreward</p>\n<p><font color=green>Actionrewardreward</font></p>\n</li>\n<li><p></p>\n</li>\n</ol>\n<p></p>\n<p></p>\n<p><strong><font color=green>Actionadvatagereward</font></strong></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p></p>\n<p>Actionscore = 0.1 score</p>\n<p></p>\n<p>reward</p>\n<p><strong><font color=green>actionaction ActionrewardActionactionrewardActionActionAction(reward)</font></strong></p>\n<p>rewardrewardActionrewardAction</p>\n<p></p>\n<p><strong><font color=green>  rewardAction</font></strong></p>\n<p><strong><font color=green>rewardrewardrewardAction</font></strong></p>\n<p></p>\n<p><strong><font color=green></font></strong></p>\n<h1 id=\"-Policy-Network--Value-Network\"><a href=\"#-Policy-Network--Value-Network\" class=\"headerlink\" title=\"(Policy Network)(Value Network)\"></a>(Policy Network)(Value Network)</h1><p>AlphaGo </p>\n<p><font color=green></font>ActionActionAction.</p>\n<p></p>\n<p><strong>Policy-based,Value-based</strong>Actionq</p>\n<p></p>\n<p><font color=red>Action+Action</font>ActionActionAction</p>\n<p><font color=green></font></p>\n<p><font color=green>Value BasedActionPolicy BasedActionActionPolicy BasedValue Based</font></p>\n<h2 id=\"-Policy-Network\"><a href=\"#-Policy-Network\" class=\"headerlink\" title=\"(Policy Network)\"></a>(Policy Network)</h2><p><a href=\"https://github.com/xyegithub/myBlog/blob/main/2021/12/03/First-Step-to-RL/policy_network.py\">Policy_Network.py</a> <a href=\"policy_network.py\">Policy_Network.py</a></p>\n<p></p>\n <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">score = tf.matmul(layer1,W2)</span><br><span class=\"line\">probability = tf.nn.sigmoid(score)<span class=\"comment\">#Action 1</span></span><br><span class=\"line\">input_y = tf.placeholder(tf.float32,[<span class=\"literal\">None</span>,<span class=\"number\">1</span>], \\\\</span><br><span class=\"line\">                         name=<span class=\"string\">&quot;input_y&quot;</span>)<span class=\"comment\"># </span></span><br><span class=\"line\">advantages = tf.placeholder(tf.float32,name=<span class=\"string\">&quot;reward_signal&quot;</span>) </span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">loglik = tf.log(input_y*(input_y - probability) + \\\\</span><br><span class=\"line\">                (<span class=\"number\">1</span> - input_y)*(input_y + probability)) </span><br><span class=\"line\"><span class=\"comment\"># 1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 00</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\">loss = -tf.reduce_mean(loglik * advantages) <span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\">#advantages</span></span><br><span class=\"line\"><span class=\"comment\">#advantages </span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">##  </span></span><br><span class=\"line\"><span class=\"comment\">#Policy_Network.py ,</span></span><br><span class=\"line\"><span class=\"comment\">#0.501</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## action</span></span><br><span class=\"line\"><span class=\"comment\"># + </span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## n</span></span><br><span class=\"line\"><span class=\"comment\">#actionadvantages</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## </span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br></pre></td></tr></table></figure>\n\n<p></p>\n<p></p>\n<p>0.5Action</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#xAction 1</span></span><br><span class=\"line\">tfprob = sess.run(probability,feed_dict=&#123;observations: x&#125;)</span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">action = <span class=\"number\">1</span> <span class=\"keyword\">if</span> np.random.uniform() &lt; tfprob <span class=\"keyword\">else</span> <span class=\"number\">0</span></span><br></pre></td></tr></table></figure>\n\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\"># tfprob = sess.run(probability,feed_dict=&#123;observations: x&#125;)</span></span><br><span class=\"line\"><span class=\"comment\"># 0.5</span></span><br><span class=\"line\">tfprob = <span class=\"number\">0.5</span></span><br><span class=\"line\">action = <span class=\"number\">1</span> <span class=\"keyword\">if</span> np.random.uniform() &lt; tfprob <span class=\"keyword\">else</span> <span class=\"number\">0</span></span><br></pre></td></tr></table></figure>\n\n<p></p>\n<p></p>\n<p> actionactionaction</p>\n<p></p>\n<p></p>\n<h2 id=\"-Value-NetworkQ-learning\"><a href=\"#-Value-NetworkQ-learning\" class=\"headerlink\" title=\"(Value NetworkQ-learning)\"></a>(Value NetworkQ-learning)</h2><p>Q-Learing</p>\n<p><a href=\"https://github.com/xyegithub/myBlog/blob/main/2021/12/03/First-Step-to-RL/policy_network.py\">Value_Network.py</a></p>\n<p>Actionreward</p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#Save the experience to our episode buffer.</span></span><br><span class=\"line\">episodeBuffer.add(np.reshape(np.array([s,a,r,s1,d]),[<span class=\"number\">1</span>,<span class=\"number\">5</span>]))</span><br><span class=\"line\"><span class=\"comment\"># saActionrActionreward</span></span><br><span class=\"line\"><span class=\"comment\"># s1Actiond  </span></span><br></pre></td></tr></table></figure>\n\n<ol>\n<li><p>Q(s<sub>t</sub>, a<sub>t</sub>)areward</p>\n</li>\n<li><p>Q<sub>desird</sub>rewardQ<sub>desired</sub>(s<sub>t</sub>, a<sub>t</sub>) = r + $\\lambda$ Max<sub>a</sub> Q<sub>desired</sub>(s<sub>t+1</sub>, a)</p>\n</li>\n<li><p>  Q<sub>desired</sub></p>\n</li>\n</ol>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#Choose an action by greedily (with e chance of random action)</span></span><br><span class=\"line\"><span class=\"comment\"># from the Q-network</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> np.random.rand(<span class=\"number\">1</span>) &lt; e <span class=\"keyword\">or</span> total_steps &lt; pre_train_steps:</span><br><span class=\"line\">    a = np.random.randint(<span class=\"number\">0</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\"><span class=\"keyword\">else</span>:</span><br><span class=\"line\">    a = sess.run(mainQN.predict,feed_dict=&#123;mainQN.scalarInput:[s]&#125;)[<span class=\"number\">0</span>]</span><br><span class=\"line\"><span class=\"comment\"># total_steps &lt; pre_train_steps</span></span><br><span class=\"line\"><span class=\"comment\"># total_steps &gt;= pre_train_steps</span></span><br><span class=\"line\"><span class=\"comment\"># e1-eAction </span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\"># epre_train_steps</span></span><br><span class=\"line\"><span class=\"comment\"># Action</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\"># Action</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br></pre></td></tr></table></figure>\n\n<p><strong>by greedily (with e chance of random action) from the Q-network</strong></p>\n<p><strong><font color=green></font></strong></p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> total_steps &gt; pre_train_steps:</span><br><span class=\"line\">    <span class=\"keyword\">if</span> e &gt; endE:</span><br><span class=\"line\">        e -= stepDrop</span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\"># pass</span></span><br><span class=\"line\"><span class=\"comment\"># endE=0.1 </span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br></pre></td></tr></table></figure>\n\n\n\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> total_steps % (update_freq) == <span class=\"number\">0</span>:</span><br><span class=\"line\">    trainBatch = myBuffer.sample(batch_size) <span class=\"comment\">#Get a random batch of experiences.</span></span><br><span class=\"line\">    <span class=\"comment\">#Below we perform the Double-DQN update to the target Q-values</span></span><br><span class=\"line\">    <span class=\"comment\"># ActiontrainBatch[:,3]</span></span><br><span class=\"line\">    <span class=\"comment\"># Q(st, at) = r + $\\lambda$ Max Q(s_t+1, a)</span></span><br><span class=\"line\">    <span class=\"comment\"># Actiont+1Action</span></span><br><span class=\"line\">    Q1 = sess.run(mainQN.predict,feed_dict=&#123;mainQN.scalarInput:np.vstack(trainBatch[:,<span class=\"number\">3</span>])&#125;)</span><br><span class=\"line\">    <span class=\"comment\"># targetreward</span></span><br><span class=\"line\">    Q2 = sess.run(targetQN.Qout,feed_dict=&#123;targetQN.scalarInput:np.vstack(trainBatch[:,<span class=\"number\">3</span>])&#125;)</span><br><span class=\"line\">    end_multiplier = -(trainBatch[:,<span class=\"number\">4</span>] - <span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"comment\"># Actiontargetreward</span></span><br><span class=\"line\">    <span class=\"comment\"># reward, Max Q(s_t+1, a)</span></span><br><span class=\"line\">    doubleQ = Q2[<span class=\"built_in\">range</span>(batch_size),Q1]</span><br><span class=\"line\">    <span class=\"comment\"># Q(st, at) = r + $\\lambda$ Max Q(s_t+1, a)</span></span><br><span class=\"line\">    <span class=\"comment\"># rewardreward</span></span><br><span class=\"line\">    targetQ = trainBatch[:,<span class=\"number\">2</span>] + (y*doubleQ * end_multiplier)</span><br><span class=\"line\">    <span class=\"comment\">#Update the network with our target values.</span></span><br><span class=\"line\">    <span class=\"comment\"># </span></span><br><span class=\"line\">    <span class=\"comment\"># </span></span><br><span class=\"line\">    _ = sess.run(mainQN.updateModel, \\</span><br><span class=\"line\">                 feed_dict=&#123;mainQN.scalarInput:np.vstack(trainBatch[:,<span class=\"number\">0</span>]),mainQN.targetQ:targetQ, mainQN.actions:trainBatch[:,<span class=\"number\">1</span>]&#125;)</span><br><span class=\"line\">    <span class=\"comment\"># target</span></span><br><span class=\"line\">    updateTarget(targetOps,sess) <span class=\"comment\">#Update the target network toward the primary network.</span></span><br></pre></td></tr></table></figure>\n\n\n\n<p>target</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">updateTargetGraph</span>(<span class=\"params\">tfVars,tau</span>):</span></span><br><span class=\"line\">    total_vars = <span class=\"built_in\">len</span>(tfVars)</span><br><span class=\"line\">    op_holder = [] </span><br><span class=\"line\">    <span class=\"comment\"># target</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> idx,var <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(tfVars[<span class=\"number\">0</span>:total_vars//<span class=\"number\">2</span>]):</span><br><span class=\"line\">        <span class=\"comment\"># idx+total_varstarget</span></span><br><span class=\"line\">        <span class=\"comment\"># target =  * tau + 1- tau*target</span></span><br><span class=\"line\">        <span class=\"comment\"># target</span></span><br><span class=\"line\">        <span class=\"comment\"># target</span></span><br><span class=\"line\">        <span class=\"comment\"># target</span></span><br><span class=\"line\">        <span class=\"comment\"># targetActionrewardAction</span></span><br><span class=\"line\">        <span class=\"comment\"># target</span></span><br><span class=\"line\">        <span class=\"comment\"># target</span></span><br><span class=\"line\">        <span class=\"comment\"># </span></span><br><span class=\"line\">        op_holder.append(tfVars[idx+total_vars//<span class=\"number\">2</span>].assign\\\\</span><br><span class=\"line\">                         ((var.value()*tau) \\\\</span><br><span class=\"line\">                      + ((<span class=\"number\">1</span>-tau)*tfVars[idx+total_vars//<span class=\"number\">2</span>].value())))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> op_holder</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">updateTarget</span>(<span class=\"params\">op_holder,sess</span>):</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> op <span class=\"keyword\">in</span> op_holder:</span><br><span class=\"line\">            sess.run(op)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>state of the arttrick</p>\n<ol>\n<li>CNN</li>\n<li>Experience replayNN batchsizeN=1</li>\n<li>target<strong>targetQ-Learing</strong><strong></strong><strong>DQNQQ</strong>targettargettargetQ</li>\n<li>Double DQNtricktargetActionactiontargetreward</li>\n<li>Dueling DQN</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">self.AW = tf.Variable(xavier_init([h_size//<span class=\"number\">2</span>,env.actions]))</span><br><span class=\"line\">self.VW = tf.Variable(xavier_init([h_size//<span class=\"number\">2</span>,<span class=\"number\">1</span>]))</span><br><span class=\"line\">self.Advantage = tf.matmul(self.streamA,self.AW)</span><br><span class=\"line\">self.Value = tf.matmul(self.streamV,self.VW)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#Then combine them together to get our final Q-values.</span></span><br><span class=\"line\"><span class=\"comment\"># QoutActionrewardValueAdvantage</span></span><br><span class=\"line\"><span class=\"comment\"># Valueadvantage#action</span></span><br><span class=\"line\"><span class=\"comment\"># Dueling DQNrewardValue</span></span><br><span class=\"line\"><span class=\"comment\"># ActionAction</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\">#out</span></span><br><span class=\"line\"><span class=\"comment\"># Action</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">self.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=<span class=\"number\">1</span>,keep_dims=<span class=\"literal\">True</span>))</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>reward</p>\n<ol>\n<li>Action</li>\n<li>Actionreward</li>\n<li></li>\n<li>ActionAction1ActionrewardAction<strong>rewardreward</strong></li>\n</ol>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<p></p>\n<p>sgd</p>\n<p></p>\n<p></p>\n<p>reward </p>\n<p>Q<sub>desired</sub>(s<sub>t</sub>, a<sub>t</sub>) = r + $\\lambda$ Max<sub>a</sub> Q<sub>desired</sub>(s<sub>t+1</sub>, a)</p>\n<p></p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> done:</span><br><span class=\"line\">    episode_number += <span class=\"number\">1</span></span><br><span class=\"line\">    epx = np.vstack(xs)</span><br><span class=\"line\">    epy = np.vstack(ys)</span><br><span class=\"line\">    epr = np.vstack(drs)</span><br><span class=\"line\">    tfp = tfps</span><br><span class=\"line\">    xs,hs,dlogps,drs,ys,tfps = [],[],[],[],[],[]</span><br><span class=\"line\"></span><br><span class=\"line\">    discounted_epr = discount_rewards(epr)</span><br><span class=\"line\">    discounted_epr -= np.mean(discounted_epr)</span><br><span class=\"line\">    discounted_epr //= np.std(discounted_epr)</span><br><span class=\"line\"></span><br><span class=\"line\">    tGrad = sess.run(newGrads,feed_dict=&#123;observations: epx, input_y: epy, advantages: discounted_epr&#125;)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> ix,grad <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(tGrad):</span><br><span class=\"line\">        gradBuffer[ix] += grad</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> episode_number % batch_size == <span class=\"number\">0</span>:</span><br><span class=\"line\">            sess.run(updateGrads,feed_dict=&#123;W1Grad: gradBuffer[<span class=\"number\">0</span>],W1_1Grad:gradBuffer[<span class=\"number\">1</span>],W2Grad:gradBuffer[<span class=\"number\">2</span>]&#125;)</span><br></pre></td></tr></table></figure>\n\n<p></p>\n"},{"title":"Personal Thought","top":false,"cover":false,"toc":true,"mathjax":true,"date":"2021-12-15T08:01:28.000Z","password":null,"summary":null,"description":"","_content":"\n\n\n# Deep Learning\n\n## \n\n### \n\n\n\ni.e.\n\n\n\nA1A1\n\nmixup\n\n\n\n\n\n**MNIST, CIFAR-10100%**\n\n\n\n### neural tangent kernel\n\n**[^1]**\n\n[^1]:NEURAL TANGENT KERNEL EIGENVALUES ACCURATELY PREDICT GENERALIZATION\n\n\n\n## \n\n### \n\nResNet\n\n```python\nshortcut(x) + out\n# \nshortcut(x) * out\n```\n\n****\n\n1.  CNN\n2. ResNet3Dmaskattention module\n\n### \n\nCifar-100Cifar-10\n\n### \n\n\n\n1. \n2. \n\n\n\n\n\n1. bn``bn(shortcut(x)) * bn(out)``\n\n2. bnbias1bn(shortcut(x))bn(out)1 \n\n    **1. shortcut(x) + outshortcut(x)1out1**\n\n   **2. out 0shorcut**\n\n### sigmoid\n\n3. attentionsigmoid``bn(shortcut(x)) * out.sigmoid()``sigmoid sigmoid\n4. soft attentionsigmoid1.  2. 2d1dSE-Netattentionattention\n\n\n\n","source":"_posts/Personal-Thought.md","raw":"---\ntitle: Personal Thought\ntop: false\ncover: false\ntoc: true\nmathjax: true\ndate: 2021-12-15 16:01:28\npassword:\nsummary:\ndescription: \ncategories:\n- About Papers\ntags:\n- Personal Thought\n- Papers\n- private\n---\n\n\n\n# Deep Learning\n\n## \n\n### \n\n\n\ni.e.\n\n\n\nA1A1\n\nmixup\n\n\n\n\n\n**MNIST, CIFAR-10100%**\n\n\n\n### neural tangent kernel\n\n**[^1]**\n\n[^1]:NEURAL TANGENT KERNEL EIGENVALUES ACCURATELY PREDICT GENERALIZATION\n\n\n\n## \n\n### \n\nResNet\n\n```python\nshortcut(x) + out\n# \nshortcut(x) * out\n```\n\n****\n\n1.  CNN\n2. ResNet3Dmaskattention module\n\n### \n\nCifar-100Cifar-10\n\n### \n\n\n\n1. \n2. \n\n\n\n\n\n1. bn``bn(shortcut(x)) * bn(out)``\n\n2. bnbias1bn(shortcut(x))bn(out)1 \n\n    **1. shortcut(x) + outshortcut(x)1out1**\n\n   **2. out 0shorcut**\n\n### sigmoid\n\n3. attentionsigmoid``bn(shortcut(x)) * out.sigmoid()``sigmoid sigmoid\n4. soft attentionsigmoid1.  2. 2d1dSE-Netattentionattention\n\n\n\n","slug":"Personal-Thought","published":1,"updated":"2021-12-22T06:31:33.872Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckxk28f67000cvgulg32ravwr","content":"<div class=\"hbe hbe-container\" id=\"hexo-blog-encrypt\" data-wpm=\"Oh, this is an invalid password. Check and try again, please.\" data-whm=\"OOPS, these decrypted content may changed, but you can still have a look.\">\n  <script id=\"hbeData\" type=\"hbeData\" data-hmacdigest=\"98216d6198def0ba940ce9f6a3dfd619b8eb16d424225a3ab83f19f27dbb7633\">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</script>\n  <div class=\"hbe hbe-content\">\n    <div class=\"hbe hbe-input hbe-input-default\">\n      <input class=\"hbe hbe-input-field hbe-input-field-default\" type=\"password\" id=\"hbePass\">\n      <label class=\"hbe hbe-input-label hbe-input-label-default\" for=\"hbePass\">\n        <span class=\"hbe hbe-input-label-content hbe-input-label-content-default\">Hey, password is required here.</span>\n      </label>\n    </div>\n  </div>\n</div>\n<script data-pjax src=\"/myBlog/lib/hbe.js\"></script><link href=\"/myBlog/css/hbe.style.css\" rel=\"stylesheet\" type=\"text/css\">","site":{"data":{}},"excerpt":"","more":"","origin":"<html><head></head><body><h1 id=\"Deep-Learning\"><span class=\"post-title-index\">1. </span><a href=\"#Deep-Learning\" class=\"headerlink\" title=\"Deep Learning\"></a>Deep Learning</h1><h2 id=\"\"><span class=\"post-title-index\">1.1. </span><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><h3 id=\"\"><span class=\"post-title-index\">1.1.1. </span><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p></p>\n<p>i.e.</p>\n<p></p>\n<p>A1A1</p>\n<p>mixup</p>\n<p></p>\n<p></p>\n<p><strong>MNIST, CIFAR-10100%</strong></p>\n<p></p>\n<h3 id=\"neural-tangent-kernel\"><span class=\"post-title-index\">1.1.2. </span><a href=\"#neural-tangent-kernel\" class=\"headerlink\" title=\"neural tangent kernel\"></a>neural tangent kernel</h3><p><strong><sup id=\"fnref:1\"><a href=\"#fn:1\" rel=\"footnote\">1</a></sup></strong></p>\n<h2 id=\"\"><span class=\"post-title-index\">1.2. </span><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><h3 id=\"\"><span class=\"post-title-index\">1.2.1. </span><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>ResNet</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">shortcut(x) + out</span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">shortcut(x) * out</span><br></pre></td></tr></tbody></table></figure>\n\n<p><strong></strong></p>\n<ol>\n<li> CNN</li>\n<li>ResNet3Dmaskattention module</li>\n</ol>\n<h3 id=\"\"><span class=\"post-title-index\">1.2.2. </span><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>Cifar-100Cifar-10</p>\n<h3 id=\"\"><span class=\"post-title-index\">1.2.3. </span><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p></p>\n<ol>\n<li></li>\n<li></li>\n</ol>\n<p></p>\n<p></p>\n<ol>\n<li><p>bn<code>bn(shortcut(x)) * bn(out)</code></p>\n</li>\n<li><p>bnbias1bn(shortcut(x))bn(out)1 </p>\n<p> <strong>1. shortcut(x) + outshortcut(x)1out1</strong></p>\n<p><strong>2. out 0shorcut</strong></p>\n</li>\n</ol>\n<h3 id=\"sigmoid\"><span class=\"post-title-index\">1.2.4. </span><a href=\"#sigmoid\" class=\"headerlink\" title=\"sigmoid\"></a>sigmoid</h3><ol start=\"3\">\n<li>attentionsigmoid<code>bn(shortcut(x)) * out.sigmoid()</code>sigmoid sigmoid</li>\n<li>soft attentionsigmoid1.  2. 2d1dSE-Netattentionattention</li>\n</ol>\n<div id=\"footnotes\"><hr><div id=\"footnotelist\"><ol style=\"list-style:none; padding-left: 0;\"><li id=\"fn:1\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px;\">1.</span><span style=\"display: inline-block; vertical-align: top;\">NEURAL TANGENT KERNEL EIGENVALUES ACCURATELY PREDICT GENERALIZATION</span><a href=\"#fnref:1\" rev=\"footnote\"> </a></li></ol></div></div></body></html>","encrypt":true},{"title":"Tips in Papers","top":false,"cover":false,"toc":true,"mathjax":true,"date":"2021-12-14T08:33:03.000Z","password":null,"summary":null,"description":"","_content":"\n# Hard Attention\n\n## 2019 Scacader: Improving Accuracy of Hard Attention Models for Vision\n\n### Hard Attentionhard attentionSoft Attention<div id=\"ap\"></div> \n\n> Our best models narrow the gap to common ImageNet baselines, achieving 75% top-1 and 91% top-5 while attending to less than one-third of the image.We further demonstrate that occluding the image patches prposed by the Saccader model highly impairs classification, thus confirming these patches strong relevance to the classification task.\n\n> Typical soft attention mechanisms rescale features at one or more stages of the network. The soft mask used for rescaling often to provide some insight into the model's decision-making process, but the model's final decision may nonetheless rely on information provided by features with small weights [^1]\n\n[^1]:2018,Learn to pay attention.\n\n### glimpsepatchglimpse\n\n> Models that employ hard attention make decisions based on only a subset of pixel in the input image, typically in the form of a series of glimpses.\n\n### Hard AttentionHard Attention\n\n> Altough our aim in this work is to perform classification with only image-level class labels, out approach bears some resembalance to two-stage object detection models. \n\n> These models operate by generating many region proposals and then applying a classification model to each proposal. \n\n> Unlike our work, these approaches use ground-truth bounding boxes to train the classification model, and modern architectures also use bounding boxes to supervise the proposal generator.\n\n**Hard AttentionHard Attention, i.e., **\n\n### \n\n![](Saccader_Over.jpg)\n\nrep. netlogits per locationrepresentation networkBagNet[^2]\n\natten. netattentionSacadder cellwhatwhereconcatmixed\n\nSaccader cell\n\n**coordinate at time tslicetprediction**\n\n[^2]:2019, Approximating CNNs with bag-of-local-features models works surprisingly well on ImageNet.  \n\n### Saccader cell\n\n![](Saccader_Cell.jpg)\n\n\n\n1. Cell statestatestate2d softmaxlogitlogitstate\n\n   > This cell produces a sequence of locations. Elements in the sequence correctpond to target locations.\n\n2. cell state$C^t$t1$C^{t - 1}$$C^t$$-10^5$ 2d softmax\n\n   > The cell includes a 2D state ($C^t$) that keeps memory of the visited locations until time t by placing 1 in the corresponding location in the cell state. We use this state to prevent the network from returning to previously seen locations.\n\n   **statesequencestatestate011sequencelogicstatelogic**\n\n3. $C^t$mixed feature$C^{t - 1}$$C^t$1mixed featurechannel attentionchannel attentionSE-Netmask mask$C^{t - 1}$\n\n4. > At test time, the model extracts the logits at time $t$ from the representation network at location $argmax_{i,j}(\\hat{R}^t_{i,j})$.The final prediction is obtained by averaging the extracted logits across all times.\n\n**Saccader Cellsequencesequencefeature**\n\n### \n\n> The goal of our training is to learn a policy that predicts a sequence of visual attentnion locations that is useful to the downsteam task (here image classification) in absence of location labels.\n>\n> We performed a three step training procedure using only the training class lables as supervision.\n\n![](Saccader_eq1.jpg)\n\n1. representation network\n\n   $y_{target}$$y_{target}$region of interestposition\n\n   > Key to Saccader is a pretraining step that require only class lables and provides initial attention locations for policy gradient optimization.\n\n   > Our pretraining procedure overcomes the sparse-reward problem that makes hard attention models difficult to optimize. It requires access to only class lables and prvides initial attention locations.These initial locations provide better rewards for the policy gradient learning.\n\n![](Saccader_eq2.jpg)\n\n2. location network (attention network, $1 \\times 1$ mixing convolution and Sacader cell)\n\n   T = 12\n\n   <font color =red> </font>\n\n![](Saccader_eq3.jpg)\n\n3. > we trained the whole model to maximize the expected reward, where the reward ($r \\in \\{0, 1\\}$) represents whether the model final prediction after 6 glimpses (T = 6) is correct. \n\n<font color=red></font>\n\n$l^t_s$saccader cellrT$y_{target}$<div id=\"\"></div> \n\n**13loss$y_{target}$**\n\nsaccader cellsaccader cellpatch \n\n**[](#ap)**\n\n### ordered logits policySaccader\n\n> An ordered logits policy uses the BagNet model to pick the top K locations based on the largest class logits.\n\nOrdered logits policy\n\n> The ordered logits policy strats off with accuracy much higher than a random policy, suggesting that the patches it initially picks are meaningful to classification.\n>\n> However, accuracy is still lower than the learned Saccader model, and **performacne improves only slowly with additional glimpese. The ordered logits policy is able to capture some of the features relevant to classification, but it is a greedy policy that produces glimpses that cluster around a few top features (i.e., with low image coverage)**\n\nordered logits policyglimpeseperformanceSaccader\n\n### \n\n[](#)Sccaderglimplese\n\n> In fact, increasing the number of glimpses beyond the number used for DRAM policy training leads to drop in performane, ulike the Saccader model that generalizes to greater umbers of glimpses.\n\n![](Saccader_gl.jpg)\n\nglimpsesSaccader[](#)<font color=red></font>\n\n## Hard Attention for Scalable Image Classification\n\n### \n\n![](Tnet_over.jpg)\n\n> Muti-scale processing in the proposed TNet architecture. Starting from level 1, the image is processed in low resolution to get a coarse description of its content (red cube). Extracted features are used for (hard) selection of image regions worth processing in higher resolution. The process is repeated recursively (here to 2 additional levels). Features from all levels are combined (arrows on the right) to create the final image representation used for classificaiton (blue cube).\n\n### Saccader\n\n# Regularization \n\n## ADCM: Attentnion Dropout Convolutional Module\n\n![ADCM](ADCM.jpg)\n\nCBAMCBAMattention weightsDropfeature mapdropattentionhard attention\n","source":"_posts/Tips-in-Papers.md","raw":"---\ntitle: Tips in Papers\ntop: false\ncover: false\ntoc: true\nmathjax: true\ndate: 2021-12-14 16:33:03\npassword:\nsummary:\ndescription: \ncategories:\n- About Papers\ntags:\n- Papers\n- Personal Thought\n---\n\n# Hard Attention\n\n## 2019 Scacader: Improving Accuracy of Hard Attention Models for Vision\n\n### Hard Attentionhard attentionSoft Attention<div id=\"ap\"></div> \n\n> Our best models narrow the gap to common ImageNet baselines, achieving 75% top-1 and 91% top-5 while attending to less than one-third of the image.We further demonstrate that occluding the image patches prposed by the Saccader model highly impairs classification, thus confirming these patches strong relevance to the classification task.\n\n> Typical soft attention mechanisms rescale features at one or more stages of the network. The soft mask used for rescaling often to provide some insight into the model's decision-making process, but the model's final decision may nonetheless rely on information provided by features with small weights [^1]\n\n[^1]:2018,Learn to pay attention.\n\n### glimpsepatchglimpse\n\n> Models that employ hard attention make decisions based on only a subset of pixel in the input image, typically in the form of a series of glimpses.\n\n### Hard AttentionHard Attention\n\n> Altough our aim in this work is to perform classification with only image-level class labels, out approach bears some resembalance to two-stage object detection models. \n\n> These models operate by generating many region proposals and then applying a classification model to each proposal. \n\n> Unlike our work, these approaches use ground-truth bounding boxes to train the classification model, and modern architectures also use bounding boxes to supervise the proposal generator.\n\n**Hard AttentionHard Attention, i.e., **\n\n### \n\n![](Saccader_Over.jpg)\n\nrep. netlogits per locationrepresentation networkBagNet[^2]\n\natten. netattentionSacadder cellwhatwhereconcatmixed\n\nSaccader cell\n\n**coordinate at time tslicetprediction**\n\n[^2]:2019, Approximating CNNs with bag-of-local-features models works surprisingly well on ImageNet.  \n\n### Saccader cell\n\n![](Saccader_Cell.jpg)\n\n\n\n1. Cell statestatestate2d softmaxlogitlogitstate\n\n   > This cell produces a sequence of locations. Elements in the sequence correctpond to target locations.\n\n2. cell state$C^t$t1$C^{t - 1}$$C^t$$-10^5$ 2d softmax\n\n   > The cell includes a 2D state ($C^t$) that keeps memory of the visited locations until time t by placing 1 in the corresponding location in the cell state. We use this state to prevent the network from returning to previously seen locations.\n\n   **statesequencestatestate011sequencelogicstatelogic**\n\n3. $C^t$mixed feature$C^{t - 1}$$C^t$1mixed featurechannel attentionchannel attentionSE-Netmask mask$C^{t - 1}$\n\n4. > At test time, the model extracts the logits at time $t$ from the representation network at location $argmax_{i,j}(\\hat{R}^t_{i,j})$.The final prediction is obtained by averaging the extracted logits across all times.\n\n**Saccader Cellsequencesequencefeature**\n\n### \n\n> The goal of our training is to learn a policy that predicts a sequence of visual attentnion locations that is useful to the downsteam task (here image classification) in absence of location labels.\n>\n> We performed a three step training procedure using only the training class lables as supervision.\n\n![](Saccader_eq1.jpg)\n\n1. representation network\n\n   $y_{target}$$y_{target}$region of interestposition\n\n   > Key to Saccader is a pretraining step that require only class lables and provides initial attention locations for policy gradient optimization.\n\n   > Our pretraining procedure overcomes the sparse-reward problem that makes hard attention models difficult to optimize. It requires access to only class lables and prvides initial attention locations.These initial locations provide better rewards for the policy gradient learning.\n\n![](Saccader_eq2.jpg)\n\n2. location network (attention network, $1 \\times 1$ mixing convolution and Sacader cell)\n\n   T = 12\n\n   <font color =red> </font>\n\n![](Saccader_eq3.jpg)\n\n3. > we trained the whole model to maximize the expected reward, where the reward ($r \\in \\{0, 1\\}$) represents whether the model final prediction after 6 glimpses (T = 6) is correct. \n\n<font color=red></font>\n\n$l^t_s$saccader cellrT$y_{target}$<div id=\"\"></div> \n\n**13loss$y_{target}$**\n\nsaccader cellsaccader cellpatch \n\n**[](#ap)**\n\n### ordered logits policySaccader\n\n> An ordered logits policy uses the BagNet model to pick the top K locations based on the largest class logits.\n\nOrdered logits policy\n\n> The ordered logits policy strats off with accuracy much higher than a random policy, suggesting that the patches it initially picks are meaningful to classification.\n>\n> However, accuracy is still lower than the learned Saccader model, and **performacne improves only slowly with additional glimpese. The ordered logits policy is able to capture some of the features relevant to classification, but it is a greedy policy that produces glimpses that cluster around a few top features (i.e., with low image coverage)**\n\nordered logits policyglimpeseperformanceSaccader\n\n### \n\n[](#)Sccaderglimplese\n\n> In fact, increasing the number of glimpses beyond the number used for DRAM policy training leads to drop in performane, ulike the Saccader model that generalizes to greater umbers of glimpses.\n\n![](Saccader_gl.jpg)\n\nglimpsesSaccader[](#)<font color=red></font>\n\n## Hard Attention for Scalable Image Classification\n\n### \n\n![](Tnet_over.jpg)\n\n> Muti-scale processing in the proposed TNet architecture. Starting from level 1, the image is processed in low resolution to get a coarse description of its content (red cube). Extracted features are used for (hard) selection of image regions worth processing in higher resolution. The process is repeated recursively (here to 2 additional levels). Features from all levels are combined (arrows on the right) to create the final image representation used for classificaiton (blue cube).\n\n### Saccader\n\n# Regularization \n\n## ADCM: Attentnion Dropout Convolutional Module\n\n![ADCM](ADCM.jpg)\n\nCBAMCBAMattention weightsDropfeature mapdropattentionhard attention\n","slug":"Tips-in-Papers","published":1,"updated":"2021-12-22T08:29:56.008Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckxk28f69000dvgulgn16h2wx","content":"<html><head></head><body><h1 id=\"Hard-Attention\"><span class=\"post-title-index\">1. </span><a href=\"#Hard-Attention\" class=\"headerlink\" title=\"Hard Attention\"></a>Hard Attention</h1><h2 id=\"2019-Scacader-Improving-Accuracy-of-Hard-Attention-Models-for-Vision\"><span class=\"post-title-index\">1.1. </span><a href=\"#2019-Scacader-Improving-Accuracy-of-Hard-Attention-Models-for-Vision\" class=\"headerlink\" title=\"2019 Scacader: Improving Accuracy of Hard Attention Models for Vision\"></a>2019 Scacader: Improving Accuracy of Hard Attention Models for Vision</h2><h3 id=\"Hard-Attentionhard-attentionSoft-Attention\"><span class=\"post-title-index\">1.1.1. </span><a href=\"#Hard-Attentionhard-attentionSoft-Attention\" class=\"headerlink\" title=\"Hard Attentionhard attentionSoft Attention\"></a>Hard Attentionhard attentionSoft Attention<div id=\"ap\"></div></h3><blockquote>\n<p>Our best models narrow the gap to common ImageNet baselines, achieving 75% top-1 and 91% top-5 while attending to less than one-third of the image.We further demonstrate that occluding the image patches prposed by the Saccader model highly impairs classification, thus confirming these patches strong relevance to the classification task.</p>\n</blockquote>\n<blockquote>\n<p>Typical soft attention mechanisms rescale features at one or more stages of the network. The soft mask used for rescaling often to provide some insight into the models decision-making process, but the models final decision may nonetheless rely on information provided by features with small weights <sup id=\"fnref:1\"><a href=\"#fn:1\" rel=\"footnote\">1</a></sup></p>\n</blockquote>\n<h3 id=\"glimpsepatchglimpse\"><span class=\"post-title-index\">1.1.2. </span><a href=\"#glimpsepatchglimpse\" class=\"headerlink\" title=\"glimpsepatchglimpse\"></a>glimpsepatchglimpse</h3><blockquote>\n<p>Models that employ hard attention make decisions based on only a subset of pixel in the input image, typically in the form of a series of glimpses.</p>\n</blockquote>\n<h3 id=\"Hard-AttentionHard-Attention\"><span class=\"post-title-index\">1.1.3. </span><a href=\"#Hard-AttentionHard-Attention\" class=\"headerlink\" title=\"Hard AttentionHard Attention\"></a>Hard AttentionHard Attention</h3><blockquote>\n<p>Altough our aim in this work is to perform classification with only image-level class labels, out approach bears some resembalance to two-stage object detection models. </p>\n</blockquote>\n<blockquote>\n<p>These models operate by generating many region proposals and then applying a classification model to each proposal. </p>\n</blockquote>\n<blockquote>\n<p>Unlike our work, these approaches use ground-truth bounding boxes to train the classification model, and modern architectures also use bounding boxes to supervise the proposal generator.</p>\n</blockquote>\n<p><strong>Hard AttentionHard Attention, i.e., </strong></p>\n<h3 id=\"\"><span class=\"post-title-index\">1.1.4. </span><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p><img src=\"Saccader_Over.jpg\"></p>\n<p>rep. netlogits per locationrepresentation networkBagNet<sup id=\"fnref:2\"><a href=\"#fn:2\" rel=\"footnote\">2</a></sup></p>\n<p>atten. netattentionSacadder cellwhatwhereconcatmixed</p>\n<p>Saccader cell</p>\n<p><strong>coordinate at time tslicetprediction</strong></p>\n<h3 id=\"Saccader-cell\"><span class=\"post-title-index\">1.1.5. </span><a href=\"#Saccader-cell\" class=\"headerlink\" title=\"Saccader cell\"></a>Saccader cell</h3><p><img src=\"Saccader_Cell.jpg\"></p>\n<p></p>\n<ol>\n<li><p>Cell statestatestate2d softmaxlogitlogitstate</p>\n<blockquote>\n<p>This cell produces a sequence of locations. Elements in the sequence correctpond to target locations.</p>\n</blockquote>\n</li>\n<li><p>cell state$C^t$t1$C^{t - 1}$$C^t$$-10^5$ 2d softmax</p>\n<blockquote>\n<p>The cell includes a 2D state ($C^t$) that keeps memory of the visited locations until time t by placing 1 in the corresponding location in the cell state. We use this state to prevent the network from returning to previously seen locations.</p>\n</blockquote>\n<p><strong>statesequencestatestate011sequencelogicstatelogic</strong></p>\n</li>\n<li><p>$C^t$mixed feature$C^{t - 1}$$C^t$1mixed featurechannel attentionchannel attentionSE-Netmask mask$C^{t - 1}$</p>\n</li>\n<li><blockquote>\n<p>At test time, the model extracts the logits at time $t$ from the representation network at location $argmax_{i,j}(\\hat{R}^t_{i,j})$.The final prediction is obtained by averaging the extracted logits across all times.</p>\n</blockquote>\n</li>\n</ol>\n<p><strong>Saccader Cellsequencesequencefeature</strong></p>\n<h3 id=\"\"><span class=\"post-title-index\">1.1.6. </span><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><blockquote>\n<p>The goal of our training is to learn a policy that predicts a sequence of visual attentnion locations that is useful to the downsteam task (here image classification) in absence of location labels.</p>\n<p>We performed a three step training procedure using only the training class lables as supervision.</p>\n</blockquote>\n<p><img src=\"Saccader_eq1.jpg\"></p>\n<ol>\n<li><p>representation network</p>\n<p>$y_{target}$$y_{target}$region of interestposition</p>\n<blockquote>\n<p>Key to Saccader is a pretraining step that require only class lables and provides initial attention locations for policy gradient optimization.</p>\n</blockquote>\n<blockquote>\n<p>Our pretraining procedure overcomes the sparse-reward problem that makes hard attention models difficult to optimize. It requires access to only class lables and prvides initial attention locations.These initial locations provide better rewards for the policy gradient learning.</p>\n</blockquote>\n</li>\n</ol>\n<p><img src=\"Saccader_eq2.jpg\"></p>\n<ol start=\"2\">\n<li><p>location network (attention network, $1 \\times 1$ mixing convolution and Sacader cell)</p>\n<p>T = 12</p>\n<p><font color=\"red\"> </font></p>\n</li>\n</ol>\n<p><img src=\"Saccader_eq3.jpg\"></p>\n<ol start=\"3\">\n<li><blockquote>\n<p>we trained the whole model to maximize the expected reward, where the reward ($r \\in {0, 1}$) represents whether the model final prediction after 6 glimpses (T = 6) is correct. </p>\n</blockquote>\n</li>\n</ol>\n<p><font color=\"red\"></font></p>\n<p>$l^t_s$saccader cellrT$y_{target}$</p><div id=\"\"></div> <p></p>\n<p><strong>13loss$y_{target}$</strong></p>\n<p>saccader cellsaccader cellpatch </p>\n<p><strong><a href=\"#ap\"></a></strong></p>\n<h3 id=\"ordered-logits-policySaccader\"><span class=\"post-title-index\">1.1.7. </span><a href=\"#ordered-logits-policySaccader\" class=\"headerlink\" title=\"ordered logits policySaccader\"></a>ordered logits policySaccader</h3><blockquote>\n<p>An ordered logits policy uses the BagNet model to pick the top K locations based on the largest class logits.</p>\n</blockquote>\n<p>Ordered logits policy</p>\n<blockquote>\n<p>The ordered logits policy strats off with accuracy much higher than a random policy, suggesting that the patches it initially picks are meaningful to classification.</p>\n<p>However, accuracy is still lower than the learned Saccader model, and <strong>performacne improves only slowly with additional glimpese. The ordered logits policy is able to capture some of the features relevant to classification, but it is a greedy policy that produces glimpses that cluster around a few top features (i.e., with low image coverage)</strong></p>\n</blockquote>\n<p>ordered logits policyglimpeseperformanceSaccader</p>\n<h3 id=\"\"><span class=\"post-title-index\">1.1.8. </span><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p><a href=\"#%E7%A9%BA%E9%97%B4\"></a>Sccaderglimplese</p>\n<blockquote>\n<p>In fact, increasing the number of glimpses beyond the number used for DRAM policy training leads to drop in performane, ulike the Saccader model that generalizes to greater umbers of glimpses.</p>\n</blockquote>\n<p><img src=\"Saccader_gl.jpg\"></p>\n<p>glimpsesSaccader<a href=\"#%E7%A9%BA%E9%97%B4\"></a><font color=\"red\"></font></p>\n<h2 id=\"Hard-Attention-for-Scalable-Image-Classification\"><span class=\"post-title-index\">1.2. </span><a href=\"#Hard-Attention-for-Scalable-Image-Classification\" class=\"headerlink\" title=\"Hard Attention for Scalable Image Classification\"></a>Hard Attention for Scalable Image Classification</h2><h3 id=\"\"><span class=\"post-title-index\">1.2.1. </span><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p><img src=\"Tnet_over.jpg\"></p>\n<blockquote>\n<p>Muti-scale processing in the proposed TNet architecture. Starting from level 1, the image is processed in low resolution to get a coarse description of its content (red cube). Extracted features are used for (hard) selection of image regions worth processing in higher resolution. The process is repeated recursively (here to 2 additional levels). Features from all levels are combined (arrows on the right) to create the final image representation used for classificaiton (blue cube).</p>\n</blockquote>\n<h3 id=\"Saccader\"><span class=\"post-title-index\">1.2.2. </span><a href=\"#Saccader\" class=\"headerlink\" title=\"Saccader\"></a>Saccader</h3><h1 id=\"Regularization\"><span class=\"post-title-index\">2. </span><a href=\"#Regularization\" class=\"headerlink\" title=\"Regularization\"></a>Regularization</h1><h2 id=\"ADCM-Attentnion-Dropout-Convolutional-Module\"><span class=\"post-title-index\">2.1. </span><a href=\"#ADCM-Attentnion-Dropout-Convolutional-Module\" class=\"headerlink\" title=\"ADCM: Attentnion Dropout Convolutional Module\"></a>ADCM: Attentnion Dropout Convolutional Module</h2><p><img src=\"ADCM.jpg\" alt=\"ADCM\"></p>\n<p>CBAMCBAMattention weightsDropfeature mapdropattentionhard attention</p>\n<div id=\"footnotes\"><hr><div id=\"footnotelist\"><ol style=\"list-style:none; padding-left: 0;\"><li id=\"fn:1\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px;\">1.</span><span style=\"display: inline-block; vertical-align: top;\">2018,Learn to pay attention.</span><a href=\"#fnref:1\" rev=\"footnote\"> </a></li><li id=\"fn:2\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px;\">2.</span><span style=\"display: inline-block; vertical-align: top;\">2019, Approximating CNNs with bag-of-local-features models works surprisingly well on ImageNet.</span><a href=\"#fnref:2\" rev=\"footnote\"> </a></li></ol></div></div></body></html>","site":{"data":{}},"excerpt":"<html><head></head><body></body></html>","more":"<h1 id=\"Hard-Attention\"><a href=\"#Hard-Attention\" class=\"headerlink\" title=\"Hard Attention\"></a>Hard Attention</h1><h2 id=\"2019-Scacader-Improving-Accuracy-of-Hard-Attention-Models-for-Vision\"><a href=\"#2019-Scacader-Improving-Accuracy-of-Hard-Attention-Models-for-Vision\" class=\"headerlink\" title=\"2019 Scacader: Improving Accuracy of Hard Attention Models for Vision\"></a>2019 Scacader: Improving Accuracy of Hard Attention Models for Vision</h2><h3 id=\"Hard-Attentionhard-attentionSoft-Attention\"><a href=\"#Hard-Attentionhard-attentionSoft-Attention\" class=\"headerlink\" title=\"Hard Attentionhard attentionSoft Attention\"></a>Hard Attentionhard attentionSoft Attention<div id=\"ap\"></div></h3><blockquote>\n<p>Our best models narrow the gap to common ImageNet baselines, achieving 75% top-1 and 91% top-5 while attending to less than one-third of the image.We further demonstrate that occluding the image patches prposed by the Saccader model highly impairs classification, thus confirming these patches strong relevance to the classification task.</p>\n</blockquote>\n<blockquote>\n<p>Typical soft attention mechanisms rescale features at one or more stages of the network. The soft mask used for rescaling often to provide some insight into the models decision-making process, but the models final decision may nonetheless rely on information provided by features with small weights <sup id=\"fnref:1\"><a href=\"#fn:1\" rel=\"footnote\">1</a></sup></p>\n</blockquote>\n<h3 id=\"glimpsepatchglimpse\"><a href=\"#glimpsepatchglimpse\" class=\"headerlink\" title=\"glimpsepatchglimpse\"></a>glimpsepatchglimpse</h3><blockquote>\n<p>Models that employ hard attention make decisions based on only a subset of pixel in the input image, typically in the form of a series of glimpses.</p>\n</blockquote>\n<h3 id=\"Hard-AttentionHard-Attention\"><a href=\"#Hard-AttentionHard-Attention\" class=\"headerlink\" title=\"Hard AttentionHard Attention\"></a>Hard AttentionHard Attention</h3><blockquote>\n<p>Altough our aim in this work is to perform classification with only image-level class labels, out approach bears some resembalance to two-stage object detection models. </p>\n</blockquote>\n<blockquote>\n<p>These models operate by generating many region proposals and then applying a classification model to each proposal. </p>\n</blockquote>\n<blockquote>\n<p>Unlike our work, these approaches use ground-truth bounding boxes to train the classification model, and modern architectures also use bounding boxes to supervise the proposal generator.</p>\n</blockquote>\n<p><strong>Hard AttentionHard Attention, i.e., </strong></p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p><img src=\"Saccader_Over.jpg\"></p>\n<p>rep. netlogits per locationrepresentation networkBagNet<sup id=\"fnref:2\"><a href=\"#fn:2\" rel=\"footnote\">2</a></sup></p>\n<p>atten. netattentionSacadder cellwhatwhereconcatmixed</p>\n<p>Saccader cell</p>\n<p><strong>coordinate at time tslicetprediction</strong></p>\n<h3 id=\"Saccader-cell\"><a href=\"#Saccader-cell\" class=\"headerlink\" title=\"Saccader cell\"></a>Saccader cell</h3><p><img src=\"Saccader_Cell.jpg\"></p>\n<p></p>\n<ol>\n<li><p>Cell statestatestate2d softmaxlogitlogitstate</p>\n<blockquote>\n<p>This cell produces a sequence of locations. Elements in the sequence correctpond to target locations.</p>\n</blockquote>\n</li>\n<li><p>cell state$C^t$t1$C^{t - 1}$$C^t$$-10^5$ 2d softmax</p>\n<blockquote>\n<p>The cell includes a 2D state ($C^t$) that keeps memory of the visited locations until time t by placing 1 in the corresponding location in the cell state. We use this state to prevent the network from returning to previously seen locations.</p>\n</blockquote>\n<p><strong>statesequencestatestate011sequencelogicstatelogic</strong></p>\n</li>\n<li><p>$C^t$mixed feature$C^{t - 1}$$C^t$1mixed featurechannel attentionchannel attentionSE-Netmask mask$C^{t - 1}$</p>\n</li>\n<li><blockquote>\n<p>At test time, the model extracts the logits at time $t$ from the representation network at location $argmax_{i,j}(\\hat{R}^t_{i,j})$.The final prediction is obtained by averaging the extracted logits across all times.</p>\n</blockquote>\n</li>\n</ol>\n<p><strong>Saccader Cellsequencesequencefeature</strong></p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><blockquote>\n<p>The goal of our training is to learn a policy that predicts a sequence of visual attentnion locations that is useful to the downsteam task (here image classification) in absence of location labels.</p>\n<p>We performed a three step training procedure using only the training class lables as supervision.</p>\n</blockquote>\n<p><img src=\"Saccader_eq1.jpg\"></p>\n<ol>\n<li><p>representation network</p>\n<p>$y_{target}$$y_{target}$region of interestposition</p>\n<blockquote>\n<p>Key to Saccader is a pretraining step that require only class lables and provides initial attention locations for policy gradient optimization.</p>\n</blockquote>\n<blockquote>\n<p>Our pretraining procedure overcomes the sparse-reward problem that makes hard attention models difficult to optimize. It requires access to only class lables and prvides initial attention locations.These initial locations provide better rewards for the policy gradient learning.</p>\n</blockquote>\n</li>\n</ol>\n<p><img src=\"Saccader_eq2.jpg\"></p>\n<ol start=\"2\">\n<li><p>location network (attention network, $1 \\times 1$ mixing convolution and Sacader cell)</p>\n<p>T = 12</p>\n<p><font color =red> </font></p>\n</li>\n</ol>\n<p><img src=\"Saccader_eq3.jpg\"></p>\n<ol start=\"3\">\n<li><blockquote>\n<p>we trained the whole model to maximize the expected reward, where the reward ($r \\in {0, 1}$) represents whether the model final prediction after 6 glimpses (T = 6) is correct. </p>\n</blockquote>\n</li>\n</ol>\n<p><font color=red></font></p>\n<p>$l^t_s$saccader cellrT$y_{target}$<div id=\"\"></div> </p>\n<p><strong>13loss$y_{target}$</strong></p>\n<p>saccader cellsaccader cellpatch </p>\n<p><strong><a href=\"#ap\"></a></strong></p>\n<h3 id=\"ordered-logits-policySaccader\"><a href=\"#ordered-logits-policySaccader\" class=\"headerlink\" title=\"ordered logits policySaccader\"></a>ordered logits policySaccader</h3><blockquote>\n<p>An ordered logits policy uses the BagNet model to pick the top K locations based on the largest class logits.</p>\n</blockquote>\n<p>Ordered logits policy</p>\n<blockquote>\n<p>The ordered logits policy strats off with accuracy much higher than a random policy, suggesting that the patches it initially picks are meaningful to classification.</p>\n<p>However, accuracy is still lower than the learned Saccader model, and <strong>performacne improves only slowly with additional glimpese. The ordered logits policy is able to capture some of the features relevant to classification, but it is a greedy policy that produces glimpses that cluster around a few top features (i.e., with low image coverage)</strong></p>\n</blockquote>\n<p>ordered logits policyglimpeseperformanceSaccader</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p><a href=\"#%E7%A9%BA%E9%97%B4\"></a>Sccaderglimplese</p>\n<blockquote>\n<p>In fact, increasing the number of glimpses beyond the number used for DRAM policy training leads to drop in performane, ulike the Saccader model that generalizes to greater umbers of glimpses.</p>\n</blockquote>\n<p><img src=\"Saccader_gl.jpg\"></p>\n<p>glimpsesSaccader<a href=\"#%E7%A9%BA%E9%97%B4\"></a><font color=red></font></p>\n<h2 id=\"Hard-Attention-for-Scalable-Image-Classification\"><a href=\"#Hard-Attention-for-Scalable-Image-Classification\" class=\"headerlink\" title=\"Hard Attention for Scalable Image Classification\"></a>Hard Attention for Scalable Image Classification</h2><h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p><img src=\"Tnet_over.jpg\"></p>\n<blockquote>\n<p>Muti-scale processing in the proposed TNet architecture. Starting from level 1, the image is processed in low resolution to get a coarse description of its content (red cube). Extracted features are used for (hard) selection of image regions worth processing in higher resolution. The process is repeated recursively (here to 2 additional levels). Features from all levels are combined (arrows on the right) to create the final image representation used for classificaiton (blue cube).</p>\n</blockquote>\n<h3 id=\"Saccader\"><a href=\"#Saccader\" class=\"headerlink\" title=\"Saccader\"></a>Saccader</h3><h1 id=\"Regularization\"><a href=\"#Regularization\" class=\"headerlink\" title=\"Regularization\"></a>Regularization</h1><h2 id=\"ADCM-Attentnion-Dropout-Convolutional-Module\"><a href=\"#ADCM-Attentnion-Dropout-Convolutional-Module\" class=\"headerlink\" title=\"ADCM: Attentnion Dropout Convolutional Module\"></a>ADCM: Attentnion Dropout Convolutional Module</h2><p><img src=\"ADCM.jpg\" alt=\"ADCM\"></p>\n<p>CBAMCBAMattention weightsDropfeature mapdropattentionhard attention</p>\n<div id=\"footnotes\"><hr><div id=\"footnotelist\"><ol style=\"list-style:none; padding-left: 0;\"><li id=\"fn:1\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px;\">1.</span><span style=\"display: inline-block; vertical-align: top;\">2018,Learn to pay attention.</span><a href=\"#fnref:1\" rev=\"footnote\"> </a></li><li id=\"fn:2\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px;\">2.</span><span style=\"display: inline-block; vertical-align: top;\">2019, Approximating CNNs with bag-of-local-features models works surprisingly well on ImageNet.</span><a href=\"#fnref:2\" rev=\"footnote\"> </a></li></ol></div></div>"},{"title":"Transformer and BERT","top":false,"cover":false,"toc":true,"mathjax":true,"date":"2021-12-18T02:00:56.000Z","password":null,"summary":null,"description":"NLPTransformerBERT.","_content":"","source":"_posts/Transformer-and-BERT.md","raw":"---\ntitle: Transformer and BERT\ntop: false\ncover: false\ntoc: true\nmathjax: true\ndate: 2021-12-18 10:00:56\npassword:\nsummary:\ndescription: NLPTransformerBERT.\ncategories:\n- Natural Language Processing\ntags:\n- Natural Language Processing\n- Transformer\n- BERT\n---\n","slug":"Transformer-and-BERT","published":1,"updated":"2021-12-18T02:03:12.773Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckxk28f6b000hvgul9j0og86t","content":"<html><head></head><body></body></html>","site":{"data":{}},"excerpt":"<html><head></head><body></body></html>","more":""}],"PostAsset":[{"_id":"source/_posts/An-Introduction-to-Git/git.jpg","post":"ckxk28f600003vgulcnsib3rm","slug":"git.jpg","modified":1,"renderable":1},{"_id":"source/_posts/First-Step-to-RL/policy_network.py","post":"ckxk28f660009vgul7dlkdslm","slug":"policy_network.py","modified":1,"renderable":1},{"_id":"source/_posts/First-Step-to-RL/q_learning.py","post":"ckxk28f660009vgul7dlkdslm","slug":"q_learning.py","modified":1,"renderable":1},{"_id":"source/_posts/Tips-in-Papers/ADCM.jpg","post":"ckxk28f69000dvgulgn16h2wx","slug":"ADCM.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Tips-in-Papers/Saccader_Cell.jpg","post":"ckxk28f69000dvgulgn16h2wx","slug":"Saccader_Cell.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Tips-in-Papers/Saccader_eq1.jpg","post":"ckxk28f69000dvgulgn16h2wx","slug":"Saccader_eq1.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Tips-in-Papers/Saccader_eq2.jpg","post":"ckxk28f69000dvgulgn16h2wx","slug":"Saccader_eq2.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Tips-in-Papers/Saccader_eq3.jpg","post":"ckxk28f69000dvgulgn16h2wx","slug":"Saccader_eq3.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Tips-in-Papers/Saccader_gl.jpg","post":"ckxk28f69000dvgulgn16h2wx","slug":"Saccader_gl.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Tips-in-Papers/Saccader_Over.jpg","post":"ckxk28f69000dvgulgn16h2wx","slug":"Saccader_Over.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Tips-in-Papers/Tnet_over.jpg","post":"ckxk28f69000dvgulgn16h2wx","slug":"Tnet_over.jpg","modified":1,"renderable":1}],"PostCategory":[{"post_id":"ckxk28f5w0001vgul3cz5ei18","category_id":"ckxk28f620004vgulbzdg99xl","_id":"ckxk28f69000evgulghih80k5"},{"post_id":"ckxk28f650008vgulfvge1air","category_id":"ckxk28f6c000kvguld0yedyr8","_id":"ckxk28f6e000qvgul943e6p3r"},{"post_id":"ckxk28f660009vgul7dlkdslm","category_id":"ckxk28f6d000nvguldl55d8rm","_id":"ckxk28f6e000tvguld7y17v4t"},{"post_id":"ckxk28f67000cvgulg32ravwr","category_id":"ckxk28f6e000rvgulecm21eqa","_id":"ckxk28f6h000wvgula5on7jgi"},{"post_id":"ckxk28f600003vgulcnsib3rm","category_id":"ckxk28f67000avgul58bubcpm","_id":"ckxk28f6i0011vgul1ej49wni"},{"post_id":"ckxk28f600003vgulcnsib3rm","category_id":"ckxk28f6f000uvgul8ijm9jpg","_id":"ckxk28f6i0014vgul2k0uamn1"},{"post_id":"ckxk28f69000dvgulgn16h2wx","category_id":"ckxk28f6e000rvgulecm21eqa","_id":"ckxk28f6j0016vgul0qedbl6i"},{"post_id":"ckxk28f6b000hvgul9j0og86t","category_id":"ckxk28f6i0010vgul60i0cjje","_id":"ckxk28f6j001avgul9qiq6qx2"},{"post_id":"ckxk28f640007vgul77d21ugl","category_id":"ckxk28f67000avgul58bubcpm","_id":"ckxk28f6k001cvgulaywceds7"},{"post_id":"ckxk28f640007vgul77d21ugl","category_id":"ckxk28f6j0018vgul53s85s60","_id":"ckxk28f6k001evgul2nd62nr5"}],"PostTag":[{"post_id":"ckxk28f5w0001vgul3cz5ei18","tag_id":"ckxk28f630005vgul7b1g8o48","_id":"ckxk28f6c000ivgulh0xk5ksd"},{"post_id":"ckxk28f5w0001vgul3cz5ei18","tag_id":"ckxk28f67000bvgulhp7bdewl","_id":"ckxk28f6c000jvgul9ocb1osp"},{"post_id":"ckxk28f600003vgulcnsib3rm","tag_id":"ckxk28f69000gvgul5qspek8t","_id":"ckxk28f6d000mvgulgmcpcm4v"},{"post_id":"ckxk28f640007vgul77d21ugl","tag_id":"ckxk28f6c000lvgul06f017k1","_id":"ckxk28f6e000pvgulcvx35tl4"},{"post_id":"ckxk28f650008vgulfvge1air","tag_id":"ckxk28f6d000ovgul3g6m5a5t","_id":"ckxk28f6h000zvgul4mng3in1"},{"post_id":"ckxk28f650008vgulfvge1air","tag_id":"ckxk28f6e000svgul5izm0zxs","_id":"ckxk28f6i0012vgul6a6ydbzc"},{"post_id":"ckxk28f650008vgulfvge1air","tag_id":"ckxk28f6g000vvgulal2o9a9s","_id":"ckxk28f6i0015vgul8vcs7ato"},{"post_id":"ckxk28f660009vgul7dlkdslm","tag_id":"ckxk28f6h000yvgul27vm2ane","_id":"ckxk28f6j0017vgulaie4211c"},{"post_id":"ckxk28f67000cvgulg32ravwr","tag_id":"ckxk28f6d000ovgul3g6m5a5t","_id":"ckxk28f6k001fvgul7jkxanvc"},{"post_id":"ckxk28f67000cvgulg32ravwr","tag_id":"ckxk28f6j0019vgul3jk0atkl","_id":"ckxk28f6k001gvgul9hc39d15"},{"post_id":"ckxk28f67000cvgulg32ravwr","tag_id":"ckxk28f6g000vvgulal2o9a9s","_id":"ckxk28f6l001ivgul7rehcsis"},{"post_id":"ckxk28f69000dvgulgn16h2wx","tag_id":"ckxk28f6j0019vgul3jk0atkl","_id":"ckxk28f6l001kvgul7zcfceu3"},{"post_id":"ckxk28f69000dvgulgn16h2wx","tag_id":"ckxk28f6d000ovgul3g6m5a5t","_id":"ckxk28f6l001lvgulbild3jeg"},{"post_id":"ckxk28f6b000hvgul9j0og86t","tag_id":"ckxk28f6l001jvgul9bmoe1es","_id":"ckxk28f6m001ovgul3ba1062m"},{"post_id":"ckxk28f6b000hvgul9j0og86t","tag_id":"ckxk28f6m001mvgulbulk2n08","_id":"ckxk28f6m001pvgul99zwekk6"},{"post_id":"ckxk28f6b000hvgul9j0og86t","tag_id":"ckxk28f6m001nvgulfx5pc0xu","_id":"ckxk28f6m001qvgulf7wkgeix"}],"Tag":[{"name":"Algorithm","_id":"ckxk28f630005vgul7b1g8o48"},{"name":"Programming","_id":"ckxk28f67000bvgulhp7bdewl"},{"name":"Git","_id":"ckxk28f69000gvgul5qspek8t"},{"name":"Hexo","_id":"ckxk28f6c000lvgul06f017k1"},{"name":"Personal Thought","_id":"ckxk28f6d000ovgul3g6m5a5t"},{"name":"Experiments","_id":"ckxk28f6e000svgul5izm0zxs"},{"name":"private","_id":"ckxk28f6g000vvgulal2o9a9s"},{"name":"Reinforcement Learning","_id":"ckxk28f6h000yvgul27vm2ane"},{"name":"Papers","_id":"ckxk28f6j0019vgul3jk0atkl"},{"name":"Natural Language Processing","_id":"ckxk28f6l001jvgul9bmoe1es"},{"name":"Transformer","_id":"ckxk28f6m001mvgulbulk2n08"},{"name":"BERT","_id":"ckxk28f6m001nvgulfx5pc0xu"}]}}