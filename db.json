{"meta":{"version":1,"warehouse":"4.0.0"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":0,"renderable":0},{"_id":"themes/next_8.8/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/css/noscript.styl","path":"css/noscript.styl","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/bookmark.js","path":"js/bookmark.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/comments-buttons.js","path":"js/comments-buttons.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/comments.js","path":"js/comments.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/config.js","path":"js/config.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/motion.js","path":"js/motion.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/next-boot.js","path":"js/next-boot.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/pjax.js","path":"js/pjax.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/schedule.js","path":"js/schedule.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/utils.js","path":"js/utils.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/images/avatar_100.jpg","path":"images/avatar_100.jpg","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/images/avatar.jpg","path":"images/avatar.jpg","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/images/avatar_200.jpg","path":"images/avatar_200.jpg","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/images/avatar_300.jpg","path":"images/avatar_300.jpg","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/images/avatar_500.jpg","path":"images/avatar_500.jpg","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/images/avatar_80.jpg","path":"images/avatar_80.jpg","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/images/logo-algolia-nebula-blue-full.svg","path":"images/logo-algolia-nebula-blue-full.svg","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/images/ye_16.ico","path":"images/ye_16.ico","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/images/ye_32.ico","path":"images/ye_32.ico","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/schemes/muse.js","path":"js/schemes/muse.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/fancybox.js","path":"js/third-party/fancybox.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/pace.js","path":"js/third-party/pace.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/quicklink.js","path":"js/third-party/quicklink.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/rating.js","path":"js/third-party/rating.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/analytics/baidu-analytics.js","path":"js/third-party/analytics/baidu-analytics.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/analytics/google-analytics.js","path":"js/third-party/analytics/google-analytics.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/analytics/growingio.js","path":"js/third-party/analytics/growingio.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/chat/gitter.js","path":"js/third-party/chat/gitter.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/chat/chatra.js","path":"js/third-party/chat/chatra.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/chat/tidio.js","path":"js/third-party/chat/tidio.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/comments/changyan.js","path":"js/third-party/comments/changyan.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/comments/disqus.js","path":"js/third-party/comments/disqus.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/comments/gitalk.js","path":"js/third-party/comments/gitalk.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/comments/disqusjs.js","path":"js/third-party/comments/disqusjs.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/comments/isso.js","path":"js/third-party/comments/isso.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/comments/livere.js","path":"js/third-party/comments/livere.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/comments/utterances.js","path":"js/third-party/comments/utterances.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/math/mathjax.js","path":"js/third-party/math/mathjax.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/math/katex.js","path":"js/third-party/math/katex.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/search/algolia-search.js","path":"js/third-party/search/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/search/local-search.js","path":"js/third-party/search/local-search.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/statistics/lean-analytics.js","path":"js/third-party/statistics/lean-analytics.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/statistics/firestore.js","path":"js/third-party/statistics/firestore.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/tags/mermaid.js","path":"js/third-party/tags/mermaid.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/tags/pdf.js","path":"js/third-party/tags/pdf.js","modified":0,"renderable":1}],"Cache":[{"_id":"source/CNAME","hash":"55c55444033af34e797e2d3cb1c1b4cf7889b3db","modified":1642412587966},{"_id":"source/_posts/Algorithm.md","hash":"d3c8265f341b1f479f940fd01f845a33c26cbc93","modified":1642412587967},{"_id":"source/_posts/An-Introduction-to-Git.md","hash":"5bffb32f7ea47f70f18c5367a9654afc2457ea19","modified":1642412587967},{"_id":"source/_posts/Config-Vim.md","hash":"3396d8bd3980b8136a768449bf5076df8af8fd3c","modified":1644305443760},{"_id":"source/_posts/Construct-Your-Blog-with-Hexo-and-Github.md","hash":"294cabd5a5d7ece70c5bbbfeaa380867fab6ee67","modified":1642412587971},{"_id":"source/_posts/First-Step-to-RL.md","hash":"340b91cc1af7f8283e826cfe40fcf7725baa8fa8","modified":1642412587972},{"_id":"source/_posts/Experiments.md","hash":"4ebc470b4e205051bacff945d720fdb46ebef582","modified":1642412587972},{"_id":"source/_posts/Foundation-for-Topological-Data-Analysis.md","hash":"c6a8f658f8e0195e4c2dff516c014dd89844ec04","modified":1642412587975},{"_id":"source/_posts/How-to-Blance-Losses-in-Multi-Task-Training.md","hash":"7ff02c2b270e746db8e333eedfad6cf3d8046440","modified":1642412587977},{"_id":"source/_posts/Personal-Thought.md","hash":"16a7b4ab87f88a826c506fff8c68e8bd123df18b","modified":1642412587977},{"_id":"source/_posts/Tips-in-Papers.md","hash":"1272658464982b4ab97e8805fd5a151b62675999","modified":1642412587978},{"_id":"source/_posts/Transformer-and-BERT.md","hash":"9b91961c25864f2d2849ebb5421700af045308b3","modified":1644308548628},{"_id":"source/about/index.md","hash":"9aab11db0791297a1fa5e1b1767eb3a05bfb42ef","modified":1642412587988},{"_id":"source/categories/index.md","hash":"408ea9b07f3b1a1339731fe7b364d88bc5644aff","modified":1642412587989},{"_id":"source/tags/index.md","hash":"3207ebf9794561395cf0c54633880ab070040ade","modified":1642412587990},{"_id":"source/_posts/First-Step-to-RL/policy_network.py","hash":"b3e8f06360cad0084a0656edaed4539f1b01e327","modified":1642412587973},{"_id":"source/_posts/Tips-in-Papers/ADCM.jpg","hash":"ee5b75f3a063e07624c5f03111f98902d885cd8e","modified":1642412587979},{"_id":"source/_posts/First-Step-to-RL/q_learning.py","hash":"3a68ec1153d26f2c18c9d34ee5deb5e495b8dca9","modified":1642412587974},{"_id":"source/_posts/Tips-in-Papers/Saccader_eq1.jpg","hash":"2ea2a74f960377a6a9d0bc98ecfadd8ba09cd316","modified":1642412587982},{"_id":"source/_posts/Tips-in-Papers/Saccader_eq2.jpg","hash":"cf5832e89b29e39275d2fa4a2e39592e97f9a39e","modified":1642412587983},{"_id":"source/_posts/Tips-in-Papers/Saccader_gl.jpg","hash":"fb8050f8b71a7c667cfa85672b505b3340d2231b","modified":1642412587985},{"_id":"source/_posts/Tips-in-Papers/Saccader_eq3.jpg","hash":"8fa845242bb6f737736d91485ca237e7132e19b5","modified":1642412587984},{"_id":"source/_posts/Tips-in-Papers/Saccader_Cell.jpg","hash":"b17e79bf86bb3422527bf51d200ceceeb53ce936","modified":1642412587980},{"_id":"source/_posts/Tips-in-Papers/Saccader_Over.jpg","hash":"deaccd942ffca9e163a9f2c9bdfb69251d1004cb","modified":1642412587981},{"_id":"source/_posts/Tips-in-Papers/Tnet_over.jpg","hash":"ed04ca4049c6f909b0bf51ea57ffa9a2b445e1c8","modified":1642412587986},{"_id":"source/_posts/An-Introduction-to-Git/git.jpg","hash":"db9ed8bb86df7e73d5be3bcae4cc8656e4a7a0ed","modified":1642412587970},{"_id":"themes/next_8.8/.editorconfig","hash":"8570735a8d8d034a3a175afd1dd40b39140b3e6a","modified":1642412590952},{"_id":"themes/next_8.8/.eslintrc.json","hash":"9c0762486f24a8c5e60f8b6c875e4c4728942649","modified":1642412590953},{"_id":"themes/next_8.8/.gitattributes","hash":"ec43734985e1cafd53d88ded3020103f7416123c","modified":1642412590955},{"_id":"themes/next_8.8/LICENSE.md","hash":"68fc9a03d50fd4b5ea97092b05967d1819dea2c4","modified":1642412590975},{"_id":"themes/next_8.8/.gitignore","hash":"417520c4dbbeab9c7e3ab10d944da0886366a0ee","modified":1642412590974},{"_id":"themes/next_8.8/.stylintrc","hash":"2cf4d637b56d8eb423f59656a11f6403aa90f550","modified":1642412590974},{"_id":"themes/next_8.8/_config.yml","hash":"4b2aabf10ded904debaa2545b1efc8a5f2742553","modified":1642412590977},{"_id":"themes/next_8.8/_vendors.yml","hash":"c88f3a82361ddb32cf62846a2ae1b7192b7e3af2","modified":1642412590977},{"_id":"themes/next_8.8/README.md","hash":"fab15a85d9d8d90ecd8879525b9b74fb1c197978","modified":1642412590976},{"_id":"themes/next_8.8/.githooks/install.js","hash":"4d77dbddf2eac1f3fc78f151d12ed22208ed655b","modified":1642412590957},{"_id":"themes/next_8.8/package.json","hash":"cc9a8e5bd83dd293552ed7cc1d0d2304a6b448ba","modified":1642412591062},{"_id":"themes/next_8.8/.githooks/pre-commit","hash":"f473eac1aaaa96c947d67988bbed140bbab1a821","modified":1642412590957},{"_id":"themes/next_8.8/crowdin.yml","hash":"e026078448c77dcdd9ef50256bb6635a8f83dca6","modified":1642412590978},{"_id":"themes/next_8.8/.github/CODE_OF_CONDUCT.md","hash":"21cbff565a0445d3a880fff1ee417e309740a9ab","modified":1642412590958},{"_id":"themes/next_8.8/.github/CONTRIBUTING.md","hash":"330656d93b6c03df9fb1f2f0e3534c971969473b","modified":1642412590959},{"_id":"themes/next_8.8/renovate.json","hash":"cb29cc16e61b0b8a6dac34657d76822ae29ad5aa","modified":1642412591063},{"_id":"themes/next_8.8/.github/PULL_REQUEST_TEMPLATE.md","hash":"3e9fbb78e3dee0ca1dc886d0c28b0148ba0ca499","modified":1642412590963},{"_id":"themes/next_8.8/.github/config.yml","hash":"7984e665e9de481a0e0e51fca5668337713f810b","modified":1642412590964},{"_id":"themes/next_8.8/.github/label-commenter-config.yml","hash":"1097fc47beeacfc1edb0248c27b17bf64bde3565","modified":1642412590965},{"_id":"themes/next_8.8/.github/labeler.yml","hash":"5c4bc2bd561e6d9b33ee118cc12218c5de46f72d","modified":1642412590966},{"_id":"themes/next_8.8/.github/issue_label_bot.yaml","hash":"fca600ddef6f80c5e61aeed21722d191e5606e5b","modified":1642412590965},{"_id":"themes/next_8.8/.github/release-drafter.yml","hash":"423275ec021104b263cd88776936a8c8d6872b66","modified":1642412590967},{"_id":"themes/next_8.8/docs/AGPL3.md","hash":"0d2b8c5fa8a614723be0767cc3bca39c49578036","modified":1642412590979},{"_id":"themes/next_8.8/docs/AUTHORS.md","hash":"a648823121563c34a177ae91f5a774b5e29f01a0","modified":1642412590980},{"_id":"themes/next_8.8/docs/LICENSE.txt","hash":"f5b14f791b7cfa1d16da981d929152e088a5d1b8","modified":1642412590981},{"_id":"themes/next_8.8/languages/README.md","hash":"b2567e32805dda79601157351a07e5ca9fe01315","modified":1642412590986},{"_id":"themes/next_8.8/languages/de.yml","hash":"4be7b8b76c81bf1853eb36d2e874b17546a0e792","modified":1642412590987},{"_id":"themes/next_8.8/languages/ar.yml","hash":"bca66db21c015dbd32970d8708b898518a773e1e","modified":1642412590986},{"_id":"themes/next_8.8/languages/default.yml","hash":"ea5e6aee4cb14510793ac4593a3bddffe23e530c","modified":1642412590988},{"_id":"themes/next_8.8/languages/en.yml","hash":"814d81c27fed736055ee300e0a6505b26ff4313c","modified":1642412590988},{"_id":"themes/next_8.8/languages/es.yml","hash":"651e3b33d86a7cdb9fd7895ca28279f8b1a24faa","modified":1642412590989},{"_id":"themes/next_8.8/languages/fa.yml","hash":"6456d40dd42f44101d9d6e7054e9884e9163f948","modified":1642412590990},{"_id":"themes/next_8.8/languages/fr.yml","hash":"b15dc05afdc94de02e5d3fee4f8d3dc5594dd37e","modified":1642412590991},{"_id":"themes/next_8.8/languages/id.yml","hash":"14e794db4eca36b257994d81eb513e61d1edcbd6","modified":1642412590991},{"_id":"themes/next_8.8/languages/it.yml","hash":"c1eeab4992c76bfd436bb205ce58b1cfeef55ee6","modified":1642412590992},{"_id":"themes/next_8.8/languages/ja.yml","hash":"d48c4157e0e02e847aac7b513580d3364c81948c","modified":1642412590993},{"_id":"themes/next_8.8/languages/pt-BR.yml","hash":"a1f27b3a592fc58f17d247f5563ff4a90a3da5f2","modified":1642412590995},{"_id":"themes/next_8.8/languages/ko.yml","hash":"6387357ac2dd498e8b8d630d27050a59180d7e8f","modified":1642412590993},{"_id":"themes/next_8.8/languages/nl.yml","hash":"ecb8e39c6225f3c068a5fdd569ee7dafd5c41a1f","modified":1642412590994},{"_id":"themes/next_8.8/languages/ru.yml","hash":"e9af1afe529ca747a04b801401d394b2ad696fde","modified":1642412590996},{"_id":"themes/next_8.8/languages/pt.yml","hash":"63a3e1e728ba5e6e22150de7331bb8a654f34960","modified":1642412590995},{"_id":"themes/next_8.8/languages/si.yml","hash":"615d18d044f44df476d6bfbf73f7b0edc2632168","modified":1642412590997},{"_id":"themes/next_8.8/languages/tr.yml","hash":"0bebba73d6f06c7dad61f80c0d7ad5f6f1791a01","modified":1642412590997},{"_id":"themes/next_8.8/languages/uk.yml","hash":"7dd24580c0865c5a7bc4d391855045366a598936","modified":1642412590998},{"_id":"themes/next_8.8/languages/vi.yml","hash":"c669c34da544a563ceae3e196addc9df6a78e024","modified":1642412590999},{"_id":"themes/next_8.8/languages/zh-HK.yml","hash":"f195bb0502ffe66e850077a1af1033455ea65f93","modified":1642412591000},{"_id":"themes/next_8.8/languages/zh-TW.yml","hash":"92256b90028de9a1e79c6bc0e5885b93e7fb4b17","modified":1642412591001},{"_id":"themes/next_8.8/layout/_layout.njk","hash":"20e4160cd0deb4fa272cc3aed0f43520b3cf4a9c","modified":1642412591002},{"_id":"themes/next_8.8/layout/category.njk","hash":"c68b7343d0f8145010f93351908cc36ef6212ec1","modified":1642412591058},{"_id":"themes/next_8.8/layout/archive.njk","hash":"d759f4d2cf5ddc6875ea250113a00662c1caf6d1","modified":1642412591057},{"_id":"themes/next_8.8/languages/zh-CN.yml","hash":"5a3ab21210304efef736e96bad254f789f42c567","modified":1642412591000},{"_id":"themes/next_8.8/layout/index.njk","hash":"dd63e488ae8cc144335a5958acedf6a16edd7a92","modified":1642412591058},{"_id":"themes/next_8.8/layout/page.njk","hash":"6c40aa438c658eb7f0cd0f6a759f18b43e7e8f93","modified":1642412591059},{"_id":"themes/next_8.8/.github/ISSUE_TEMPLATE/config.yml","hash":"c40ae7903b6cc99f94c9d45ac7ba8c2850bb1309","modified":1642412590961},{"_id":"themes/next_8.8/layout/tag.njk","hash":"9e16ba20c28a7f2c6bc75aa427f48122301a30aa","modified":1642412591061},{"_id":"themes/next_8.8/layout/post.njk","hash":"6abeb85fb3e4c382ed4bb6049b12a807e6226e67","modified":1642412591060},{"_id":"themes/next_8.8/test/index.js","hash":"6bf0289846538be3e9a63809af98f00e1fbdd90b","modified":1642412591222},{"_id":"themes/next_8.8/.github/ISSUE_TEMPLATE/feature-request.md","hash":"4ecac91716eac59d7c2bc53cf6e95612d44da97b","modified":1642412590962},{"_id":"themes/next_8.8/.github/ISSUE_TEMPLATE/bug-report.md","hash":"fc4dce84ed9a5d21d3a8833ff6d776c46f876115","modified":1642412590960},{"_id":"themes/next_8.8/.github/ISSUE_TEMPLATE/other.md","hash":"8cc5b5c116f6a052865a324512362f145d699202","modified":1642412590963},{"_id":"themes/next_8.8/.github/workflows/label-commenter.yml","hash":"44405477660289d4ed9beba1d054b15bb67bba06","modified":1642412590968},{"_id":"themes/next_8.8/.github/workflows/labeler.yml","hash":"8b73c439dc796be141d521a4546bcfb7a5485534","modified":1642412590969},{"_id":"themes/next_8.8/.github/workflows/linter.yml","hash":"276a91c7179926f410c784c99fa635dc0a016c2d","modified":1642412590970},{"_id":"themes/next_8.8/.github/workflows/stale.yml","hash":"0feb3e1afd1b2dca9dbc7811ce4cf5520e2d186c","modified":1642412590972},{"_id":"themes/next_8.8/.github/workflows/lock.yml","hash":"e48d1ced9a673d3f0911a700d3e68c0f4ca79263","modified":1642412590971},{"_id":"themes/next_8.8/docs/ru/README.md","hash":"87edab5a3eb7577a409c01df3f1631de40f8956f","modified":1642412590982},{"_id":"themes/next_8.8/.github/workflows/tester.yml","hash":"22aaaa3eba1a7ebcf0f78417fd9a7113ee7b6c6c","modified":1642412590973},{"_id":"themes/next_8.8/.github/workflows/release-drafter.yml","hash":"4f3af81009cb922be91f718a67425377515ea69d","modified":1642412590972},{"_id":"themes/next_8.8/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"7a06d443f374bd1e84294067a0ac796afd9fbe60","modified":1642412590983},{"_id":"themes/next_8.8/docs/zh-CN/README.md","hash":"02bafc6ee86263790603861e356596f0c916e392","modified":1642412590984},{"_id":"themes/next_8.8/layout/_macro/post-collapse.njk","hash":"1a30d751871dabfa80940042ddb1f77d07d830b9","modified":1642412591003},{"_id":"themes/next_8.8/docs/zh-CN/CONTRIBUTING.md","hash":"a089f7a8368ab0b7d7b9b7ec0ac3767a453435df","modified":1642412590984},{"_id":"themes/next_8.8/layout/_macro/sidebar.njk_backup","hash":"eb786e8b35e354287cda345c524cd35ec955f692","modified":1642412591006},{"_id":"themes/next_8.8/layout/_macro/sidebar.njk","hash":"a1575da8a34d7b7b8cf37e23ffb5d0ed2edee0fe","modified":1642412591005},{"_id":"themes/next_8.8/layout/_macro/post.njk","hash":"d0ed41b9b05254e19d051b5f91fdcaa125ee7ca6","modified":1642412591004},{"_id":"themes/next_8.8/layout/_scripts/vendors.njk","hash":"be80b9fe415a9a09d74c28e230995fd292dfc123","modified":1642412591029},{"_id":"themes/next_8.8/layout/_scripts/index.njk","hash":"6668878a0f9a1166c6a879755f54a08d942da870","modified":1642412591028},{"_id":"themes/next_8.8/layout/_partials/comments.njk","hash":"c12f8a7497596441503f2541d2f746f2ee7dd594","modified":1642412591007},{"_id":"themes/next_8.8/layout/_partials/footer.njk","hash":"a6197cc0d418d87919f929905a5e90e21b969ffa","modified":1642412591008},{"_id":"themes/next_8.8/layout/_third-party/fancybox.njk","hash":"844559f46e2ff1c8be234d5763703106e2072a7b","modified":1642412591042},{"_id":"themes/next_8.8/layout/_partials/languages.njk","hash":"e43f22198cccb5f6e306b1ce0d28d12a4fb891f8","modified":1642412591014},{"_id":"themes/next_8.8/layout/_partials/pagination.njk","hash":"9876dbfc15713c7a47d4bcaa301f4757bd978269","modified":1642412591018},{"_id":"themes/next_8.8/layout/_third-party/pace.njk","hash":"d7ad5714079f7f65446f880baf14722435ca9061","modified":1642412591046},{"_id":"themes/next_8.8/layout/_third-party/quicklink.njk","hash":"0efed71ed530447718c4ea5bbd5fc8695b0b0d5f","modified":1642412591047},{"_id":"themes/next_8.8/layout/_third-party/index.njk","hash":"d41eeb262978e34de4679d8971a9e7ac5d90ecbc","modified":1642412591043},{"_id":"themes/next_8.8/scripts/events/index.js","hash":"1ce12eda88fa5df7e76ec7b78b8463fc6618410c","modified":1642412591065},{"_id":"themes/next_8.8/layout/_partials/widgets.njk","hash":"852a750524decf1efa587cd52b09e387ed8315de","modified":1642412591027},{"_id":"themes/next_8.8/scripts/filters/minify.js","hash":"0af64049db8188d5f8cc226b353e0d7909819feb","modified":1642412591080},{"_id":"themes/next_8.8/scripts/filters/default-injects.js","hash":"872f01cb10e422a648ea505436532e776e92926b","modified":1642412591078},{"_id":"themes/next_8.8/layout/_third-party/rating.njk","hash":"1bcdbc7fde26d6d9ef4e7fa43ffcff5a9506b20e","modified":1642412591048},{"_id":"themes/next_8.8/scripts/filters/locals.js","hash":"0cd7da6755459d60779f0a7ccf311e26e184d55d","modified":1642412591079},{"_id":"themes/next_8.8/scripts/helpers/engine.js","hash":"b9785bc737470e9b8e910e7da9e8c45c2ead58fa","modified":1642412591082},{"_id":"themes/next_8.8/scripts/filters/number.js","hash":"63735cb9d02921e25b2606490340a70db89abbec","modified":1642412591080},{"_id":"themes/next_8.8/scripts/helpers/next-config.js","hash":"9a07f2d979fc8fe0c5e07d48304187b9b03ea7ff","modified":1642412591083},{"_id":"themes/next_8.8/scripts/helpers/next-url.js","hash":"a11b71ba0c5012e2cdcab31c15439156b215563e","modified":1642412591084},{"_id":"themes/next_8.8/scripts/filters/post.js","hash":"ab8bb12e4d55640b1ac4252514468ce37ebcb0b0","modified":1642412591081},{"_id":"themes/next_8.8/scripts/helpers/font.js","hash":"3394185a7f0393c16ce52c8028f90da3e9239c55","modified":1642412591082},{"_id":"themes/next_8.8/scripts/helpers/next-vendors.js","hash":"afdd6a188a74c188f0dd154fac70efd4080ca262","modified":1642412591084},{"_id":"themes/next_8.8/scripts/tags/button.js","hash":"c6ad2ed544fbb25ecb5d820c36e76302504271b7","modified":1642412591086},{"_id":"themes/next_8.8/scripts/tags/label.js","hash":"8a73348186113bae0a51ea2f891c1bb882fab05a","modified":1642412591089},{"_id":"themes/next_8.8/scripts/tags/caniuse.js","hash":"935a311142a409c1896b3ae3f01fe7a9e2db1134","modified":1642412591087},{"_id":"themes/next_8.8/scripts/tags/group-pictures.js","hash":"9ed799c329abf830f623689d7e136991256a24ca","modified":1642412591088},{"_id":"themes/next_8.8/scripts/tags/center-quote.js","hash":"92c19d796bdb3320df9caea59bf52df7a95d9da9","modified":1642412591087},{"_id":"themes/next_8.8/scripts/tags/mermaid.js","hash":"4fb01ca650fa8b256b8d48f50dc1b18350bd3d6d","modified":1642412591090},{"_id":"themes/next_8.8/scripts/tags/link-grid.js","hash":"18a483c2d5afd701f6080ffdddf2d1321370336c","modified":1642412591090},{"_id":"themes/next_8.8/scripts/tags/index.js","hash":"17f9451ce1f10f78437f52218757d38d4e1591b0","modified":1642412591088},{"_id":"themes/next_8.8/scripts/tags/note.js","hash":"7b94ddb46b7d4b0fe815f2fbe4bd375f07f55363","modified":1642412591091},{"_id":"themes/next_8.8/scripts/tags/video.js","hash":"2ee926448583be8f95af1f2884ae2c9c4830151d","modified":1642412591094},{"_id":"themes/next_8.8/scripts/tags/tabs.js","hash":"0eabe51da40b4b13e16419c8fe02452d9a4fef73","modified":1642412591094},{"_id":"themes/next_8.8/scripts/tags/pdf.js","hash":"344636b6fd7e27e8831c1e194039afc0d61931cd","modified":1642412591092},{"_id":"themes/next_8.8/source/css/_mixins.styl","hash":"acef5acc728f24cb657be8d7010d836b4d556b0e","modified":1642412591151},{"_id":"themes/next_8.8/source/css/_colors.styl","hash":"3c6798c10cc220d83481cb3f3782e78558cee789","modified":1642412591096},{"_id":"themes/next_8.8/source/css/noscript.styl","hash":"76bba5d7916e9930e68215a0fce3a7d81c44510f","modified":1642412591172},{"_id":"themes/next_8.8/source/js/bookmark.js","hash":"0f563ffbf05fad30e854e413ab17ff7164ab5a53","modified":1642412591191},{"_id":"themes/next_8.8/source/css/main.styl","hash":"78ce791cc4ac95386cf6839ca72f5f7b51f86ee9","modified":1642412591171},{"_id":"themes/next_8.8/source/js/comments-buttons.js","hash":"1a7344440321713426a0b2ab17e276b5bdf85ade","modified":1642412591192},{"_id":"themes/next_8.8/source/js/motion.js","hash":"9c4c861dfb080b6244d4d9eba33ac686735754f3","modified":1642412591195},{"_id":"themes/next_8.8/source/js/config.js","hash":"4c4ebbe3b3f3841a26f9d5af6d0ba8bc6da01c54","modified":1642412591194},{"_id":"themes/next_8.8/source/js/comments.js","hash":"66ae2e26ea36a41b72c638ea8b220296638ae952","modified":1642412591193},{"_id":"themes/next_8.8/source/js/pjax.js","hash":"919f5281c4a04d11cfd94573ecf57b3dbabd3cc8","modified":1642412591196},{"_id":"themes/next_8.8/source/js/next-boot.js","hash":"48497e2156a10155dc42311633a110c9685692c9","modified":1642412591196},{"_id":"themes/next_8.8/source/js/utils.js","hash":"e447160d342b1f93df5214b6a733441039ced439","modified":1642412591220},{"_id":"themes/next_8.8/source/js/schedule.js","hash":"2b43e2d576a308289880befc27580dbb2aa34439","modified":1642412591197},{"_id":"themes/next_8.8/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1642412591173},{"_id":"themes/next_8.8/source/images/avatar_100.jpg","hash":"a0f0a00c9326c57a5cc181aeb80dfda8e947920a","modified":1642412591176},{"_id":"themes/next_8.8/source/images/avatar.gif","hash":"2dbc3e2f2d624b2ca1afe6edc2ca17307f1950c8","modified":1642412591174},{"_id":"themes/next_8.8/source/images/avatar_200.jpg","hash":"4a76d1527e511350c9112e899046a34be59ad278","modified":1642412591176},{"_id":"themes/next_8.8/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1642412591181},{"_id":"themes/next_8.8/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1642412591182},{"_id":"themes/next_8.8/source/images/avatar_80.jpg","hash":"465e53598964df3852f08794cb73c5e459855e92","modified":1642412591180},{"_id":"themes/next_8.8/source/images/logo-algolia-nebula-blue-full.svg","hash":"b85e274207b1392782476a0430feac98db1e7da0","modified":1642412591184},{"_id":"themes/next_8.8/source/images/logo.svg","hash":"2cb74fd3ea2635e015eabc58a8d488aed6cf6417","modified":1642412591185},{"_id":"themes/next_8.8/source/images/ye_16.ico","hash":"b8fb01b5361da89831d232a831a1532e9822bd72","modified":1642412591187},{"_id":"themes/next_8.8/source/images/ye_32.ico","hash":"375c99cd785d93dd989c36604ffbd10ada71322a","modified":1642412591190},{"_id":"themes/next_8.8/test/helpers/font.js","hash":"342ef3c6fd2dcca2a8802a516ed6d7f389fd2ca2","modified":1642412591221},{"_id":"themes/next_8.8/test/helpers/index.js","hash":"63ba28afed697f7b3574436b1133b8ecc9c0c357","modified":1642412591221},{"_id":"themes/next_8.8/test/helpers/next-url.js","hash":"a91d880cb75e0a8e65a7be4c7362b2c8ebfb7c4f","modified":1642412591222},{"_id":"themes/next_8.8/test/tags/caniuse.js","hash":"aa5e728445caeaf7c2ccd0f3fcb2cad0c93ca6d1","modified":1642412591224},{"_id":"themes/next_8.8/test/tags/button.js","hash":"48f2aa4c513e9e24bd6a811410520b74cd7ea88b","modified":1642412591223},{"_id":"themes/next_8.8/test/tags/center-quote.js","hash":"7667342fd1a1417eaf6a254012b84ae40e8d13dd","modified":1642412591224},{"_id":"themes/next_8.8/test/tags/group-pictures.js","hash":"5c68ae0184f9da6e00ba199f2554d503d8e6da71","modified":1642412591225},{"_id":"themes/next_8.8/test/tags/index.js","hash":"e8779e54f0979b221858f8bb74dd081bb503b910","modified":1642412591225},{"_id":"themes/next_8.8/test/tags/link-grid.js","hash":"43d298fafb7c45a874b766d443843bd26346e689","modified":1642412591226},{"_id":"themes/next_8.8/test/tags/label.js","hash":"4ebf3698c258ca978b997acbdd0dece44069c09d","modified":1642412591226},{"_id":"themes/next_8.8/test/tags/mermaid.js","hash":"ab77be5f3c6d9a57c7b9dda6decf1906a736fef9","modified":1642412591227},{"_id":"themes/next_8.8/test/tags/pdf.js","hash":"fd6ea5123560a90f7e7c1eface23dbe1455db25f","modified":1642412591229},{"_id":"themes/next_8.8/test/tags/note.js","hash":"3dcfcd65bf9f326972ea7571fdb1444200f5d07e","modified":1642412591228},{"_id":"themes/next_8.8/test/validate/index.js","hash":"5a95ccc8598667535bd022e988055c0e019f3670","modified":1642412591231},{"_id":"themes/next_8.8/test/tags/video.js","hash":"b796fc4dceb20a30e730c322bb5474c0162464cc","modified":1642412591230},{"_id":"themes/next_8.8/test/tags/tabs.js","hash":"d63722919f9da2e44d6b952801e10a2915ea9c12","modified":1642412591230},{"_id":"themes/next_8.8/layout/_partials/head/head-unique.njk","hash":"9167e429a459686c9fc140790124a46d677e6b15","modified":1642412591009},{"_id":"themes/next_8.8/layout/_partials/head/head.njk","hash":"d3c094aaef1431fbc9df333529a7b1789ccd134c","modified":1642412591009},{"_id":"themes/next_8.8/layout/_partials/header/brand.njk","hash":"aff4613756456be26415febc668860fdab8d33c5","modified":1642412591010},{"_id":"themes/next_8.8/layout/_partials/header/sub-menu.njk","hash":"75a158a5b54a3a76ee6590f5e0e2dd4a9f0be869","modified":1642412591013},{"_id":"themes/next_8.8/layout/_partials/header/menu-item.njk","hash":"b46f412c0b4f775fd329d50357f722f5d7c1a3ba","modified":1642412591012},{"_id":"themes/next_8.8/layout/_partials/header/index.njk","hash":"650de421a8ce4cf685428ffbe0087ff84cbd1356","modified":1642412591011},{"_id":"themes/next_8.8/layout/_partials/header/menu.njk","hash":"8561e4125b227e5134cb058e2a76fb2e5233ca29","modified":1642412591012},{"_id":"themes/next_8.8/layout/_partials/page/categories.njk","hash":"17156d99941f28a225951ffdcfa9a115e20dc2d2","modified":1642412591015},{"_id":"themes/next_8.8/layout/_partials/page/page-header.njk","hash":"7ed4f102a1825195cff8d7995bf9219f323a9034","modified":1642412591016},{"_id":"themes/next_8.8/layout/_partials/page/breadcrumb.njk","hash":"edb3bb6d644b7407673c5ef3a426a244e98fcf89","modified":1642412591015},{"_id":"themes/next_8.8/layout/_partials/post/post-meta.njk","hash":"9fa47e4fb342811da590ee4adc91cf81118c0a39","modified":1642412591022},{"_id":"themes/next_8.8/layout/_partials/page/tags.njk","hash":"a18d1598e36cc72f2b0b24c3cc3c5990dfaa3254","modified":1642412591018},{"_id":"themes/next_8.8/layout/_partials/page/schedule.njk","hash":"ca2ccf3cf1874c45712f192ad45dea96fbd9920d","modified":1642412591017},{"_id":"themes/next_8.8/layout/_partials/post/post-related.njk","hash":"7384e6390067ef2a84e7310d6adb3f6104ed62e2","modified":1642412591022},{"_id":"themes/next_8.8/layout/_partials/post/post-copyright.njk","hash":"133942922e34abae9e4de7ea5591d77c0caa4b37","modified":1642412591020},{"_id":"themes/next_8.8/layout/_partials/post/post-reward.njk","hash":"002b51d0cae3f2e2e008bdc58be90c728282de5b","modified":1642412591023},{"_id":"themes/next_8.8/layout/_partials/search/algolia-search.njk","hash":"efb2b6f19df02ba5ae623a1f274fff52aed21e6f","modified":1642412591024},{"_id":"themes/next_8.8/layout/_partials/post/post-footer.njk","hash":"bde2c7356d9362972bde41cc206d5816f8ed714d","modified":1642412591021},{"_id":"themes/next_8.8/layout/_partials/post/post-followme.njk","hash":"154df0bb323c332d8c25343f258ee865e5553423","modified":1642412591020},{"_id":"themes/next_8.8/layout/_partials/search/localsearch.njk","hash":"661f7acae43f0be694266323320f977d84119abe","modified":1642412591026},{"_id":"themes/next_8.8/layout/_partials/search/index.njk","hash":"8f6f256ab3b351ffc80f1f3f1d9834e9a7cfac31","modified":1642412591025},{"_id":"themes/next_8.8/layout/_third-party/analytics/baidu-analytics.njk","hash":"6215309aee028dcb734452beec448c5afb6c63fc","modified":1642412591030},{"_id":"themes/next_8.8/layout/_third-party/analytics/cloudflare.njk","hash":"c978e9efd472c4825f93b83524b11f1c4f7efaab","modified":1642412591030},{"_id":"themes/next_8.8/layout/_third-party/analytics/google-analytics.njk","hash":"d89066ff53879693f023e540d59c86137172c529","modified":1642412591031},{"_id":"themes/next_8.8/layout/_third-party/analytics/growingio.njk","hash":"8afaa772c390bd9d53a5cff9645ac3168334eb98","modified":1642412591032},{"_id":"themes/next_8.8/layout/_partials/sidebar/site-overview.njk","hash":"3d8591bb92df77ceb9d5b07bc76da1ca89e5bd76","modified":1642412591026},{"_id":"themes/next_8.8/layout/_third-party/analytics/index.njk","hash":"2d36a481a70d5f450f1f166dc556ac1218b18537","modified":1642412591032},{"_id":"themes/next_8.8/layout/_third-party/comments/disqusjs.njk","hash":"0749cb6902baecdfd01f779a2a2513f6d2f6a823","modified":1642412591038},{"_id":"themes/next_8.8/layout/_third-party/comments/disqus.njk","hash":"9375b19a89b7fa9474e558d085af5448d4c5c50c","modified":1642412591036},{"_id":"themes/next_8.8/layout/_third-party/comments/livere.njk","hash":"3b13b09fba84ec6000886890a6710736a2b8fafe","modified":1642412591040},{"_id":"themes/next_8.8/layout/_third-party/comments/changyan.njk","hash":"d1c950f8fbdf85e7a3eae5463767a89e858e8220","modified":1642412591035},{"_id":"themes/next_8.8/layout/_third-party/comments/isso.njk","hash":"64cc3bdaf644fd32c0d0a247f29f5b6904da9af3","modified":1642412591039},{"_id":"themes/next_8.8/layout/_third-party/comments/gitalk.njk","hash":"b63b7e2ede0d3e66e732fa1a06bda9b19e1e85d4","modified":1642412591039},{"_id":"themes/next_8.8/layout/_third-party/chat/chatra.njk","hash":"d7263fca16d0278ccf1f6aa1c6df6902a6344a09","modified":1642412591033},{"_id":"themes/next_8.8/layout/_third-party/comments/utterances.njk","hash":"5a94032bc3512a10ad4328fc19ec07b819a1d687","modified":1642412591041},{"_id":"themes/next_8.8/layout/_third-party/chat/gitter.njk","hash":"f8cc14b7aa949999a1faaeb7855e2f20b59a386d","modified":1642412591034},{"_id":"themes/next_8.8/layout/_third-party/chat/tidio.njk","hash":"02aab857c27fc103216029be991688b12a73a525","modified":1642412591034},{"_id":"themes/next_8.8/layout/_third-party/math/katex.njk","hash":"d82c24136bbd3443b85f07f5579845833b594684","modified":1642412591045},{"_id":"themes/next_8.8/layout/_third-party/math/index.njk","hash":"abf37fc55aa86702118e8fdf5bf2d389dd589aa0","modified":1642412591044},{"_id":"themes/next_8.8/layout/_third-party/statistics/busuanzi-counter.njk","hash":"a4bc501da0f22f7e420f0ca47e83988ce90b1368","modified":1642412591051},{"_id":"themes/next_8.8/layout/_third-party/math/mathjax.njk","hash":"3677017fd4572b158311f5f5d870590ab25184e0","modified":1642412591045},{"_id":"themes/next_8.8/layout/_third-party/search/algolia-search.njk","hash":"24ed76e0c72a25ac152820c750a05826a706b6f4","modified":1642412591049},{"_id":"themes/next_8.8/layout/_third-party/search/localsearch.njk","hash":"e45ea3542cdc9ed7ec8447b5e6f35df4c5e82758","modified":1642412591050},{"_id":"themes/next_8.8/layout/_third-party/statistics/firestore.njk","hash":"d32ebe94560fa95824478ebbff531bffc47b194d","modified":1642412591052},{"_id":"themes/next_8.8/layout/_third-party/statistics/lean-analytics.njk","hash":"2446e748cdc102c78492216319ac02148db7daf6","modified":1642412591054},{"_id":"themes/next_8.8/layout/_third-party/statistics/index.njk","hash":"568ddf7955d11d93fb5e842b403a7ac8b1b7fdb1","modified":1642412591053},{"_id":"themes/next_8.8/scripts/events/lib/config.js","hash":"b0ced2583fdd505da3ef27a9db9c55cc7b936732","modified":1642412591066},{"_id":"themes/next_8.8/layout/_third-party/tags/mermaid.njk","hash":"099e031f52fb8e47b3af5b2684737efc9e643ee7","modified":1642412591055},{"_id":"themes/next_8.8/layout/_third-party/tags/pdf.njk","hash":"2c81984cc4f5123103460442f6e046f5b6c97127","modified":1642412591056},{"_id":"themes/next_8.8/scripts/events/lib/highlight.js","hash":"6aec7b2c38c50989a23bfaa0d560e75c7f553e12","modified":1642412591067},{"_id":"themes/next_8.8/scripts/events/lib/injects.js","hash":"d987709267a1bc6e5014411e9983d7c49c102c16","modified":1642412591067},{"_id":"themes/next_8.8/scripts/events/lib/vendors.js","hash":"08dac57e15c9f06c7cf54884b045f2362595f9d2","modified":1642412591069},{"_id":"themes/next_8.8/scripts/filters/comment/changyan.js","hash":"aa05e6b3d613a756178b8ba06832ad27499d4c14","modified":1642412591071},{"_id":"themes/next_8.8/scripts/filters/comment/default-config.js","hash":"93ee5f9109dad885dc38c49bcee630c10f9dce6e","modified":1642412591072},{"_id":"themes/next_8.8/scripts/filters/comment/gitalk.js","hash":"7bb7dafdd7f6bca8464b54e17e552ce7f1714195","modified":1642412591075},{"_id":"themes/next_8.8/scripts/events/lib/utils.js","hash":"b281be775b693f9bf32766c8f6ef703c72ac9b00","modified":1642412591068},{"_id":"themes/next_8.8/scripts/filters/comment/disqus.js","hash":"7f71d6b271ba65ff333d5682e7575711d368c0d2","modified":1642412591073},{"_id":"themes/next_8.8/scripts/filters/comment/common.js","hash":"2486f3e0150c753e5f3af1a3665d074704b8ee2c","modified":1642412591071},{"_id":"themes/next_8.8/scripts/filters/comment/disqusjs.js","hash":"135b87d151055eefdbc711d9e704b112b3214a84","modified":1642412591074},{"_id":"themes/next_8.8/scripts/filters/comment/livere.js","hash":"5a07d8bb52bc1d51a624ca8db54be144566c306b","modified":1642412591076},{"_id":"themes/next_8.8/source/css/_variables/Gemini.styl","hash":"96e0a7c2a65ce68215e17e369085b2ea2f1334f2","modified":1642412591167},{"_id":"themes/next_8.8/source/css/_variables/Mist.styl","hash":"e1fbf169b9b6a194b518240cbd06ec3c48b83d61","modified":1642412591168},{"_id":"themes/next_8.8/source/css/_variables/Muse.styl","hash":"e3be898f5ebcf435a26542653a9297ff2c71aeb0","modified":1642412591169},{"_id":"themes/next_8.8/scripts/filters/comment/isso.js","hash":"ff8b5b5145220a17d0ecd9508ba9bd2d3b2da47d","modified":1642412591076},{"_id":"themes/next_8.8/scripts/filters/comment/utterances.js","hash":"d3bded697bc32dace689d2a6dfb6eb7514169d15","modified":1642412591078},{"_id":"themes/next_8.8/source/css/_variables/Pisces.styl","hash":"c65536a128b9bc9dbe2fbb1b235a3cded2891002","modified":1642412591170},{"_id":"themes/next_8.8/source/css/_variables/base.styl","hash":"163c7441d777bee87042d475e6ce0fde199add28","modified":1642412591170},{"_id":"themes/next_8.8/source/js/third-party/rating.js","hash":"4e92c2d107ba47b47826829f9668030d5ea9bfb8","modified":1642412591214},{"_id":"themes/next_8.8/source/js/third-party/pace.js","hash":"0ef04218b93561ba4d0ff420d556c3d90a756d32","modified":1642412591213},{"_id":"themes/next_8.8/source/js/schemes/muse.js","hash":"9a836d2bcc3defe4bd1ee51f5f4eb7006ebdd41b","modified":1642412591199},{"_id":"themes/next_8.8/source/js/third-party/fancybox.js","hash":"c098d14e65dd170537134358d4b8359ad0539c2c","modified":1642412591210},{"_id":"themes/next_8.8/source/js/third-party/quicklink.js","hash":"6f58cd7aa8f6f1ab92d5a96551add293f4e55312","modified":1642412591214},{"_id":"themes/next_8.8/source/css/_common/components/index.styl","hash":"fe1868f47681e00a33a96199302be85377282f63","modified":1642412591098},{"_id":"themes/next_8.8/source/css/_common/components/back-to-top.styl","hash":"bab653bcf226311381e8411a0492202f1bf1fce9","modified":1642412591097},{"_id":"themes/next_8.8/source/css/_common/outline/mobile.styl","hash":"64775c729512b30b144ab5ae9dc4a4dfd4e13f35","modified":1642412591126},{"_id":"themes/next_8.8/source/css/_common/components/reading-progress.styl","hash":"90a86045a33c1bae49fc2f6fa1e1b53170c7f77b","modified":1642412591111},{"_id":"themes/next_8.8/source/css/_common/outline/index.styl","hash":"8e34df131830d4fa3725e4590a672ba1cf1903e5","modified":1642412591125},{"_id":"themes/next_8.8/source/css/_common/scaffolding/base.styl","hash":"0805d7db96b6c83b31e8d023d1ae88f6d2969133","modified":1642412591135},{"_id":"themes/next_8.8/source/css/_common/scaffolding/base.styl_backup","hash":"d0a7c99095f490b0d2ed6b1be43d435960798cec","modified":1642412591136},{"_id":"themes/next_8.8/source/css/_common/scaffolding/buttons.styl","hash":"a042571d85ff7265f799004239a45f36b716b8a6","modified":1642412591138},{"_id":"themes/next_8.8/source/css/_common/scaffolding/index.styl","hash":"523fb7b653b87ae37fc91fc8813e4ffad87b0d7e","modified":1642412591142},{"_id":"themes/next_8.8/source/css/_common/scaffolding/comments.styl","hash":"e4fecc889ba3317a64e9abba5842c79dff9b7827","modified":1642412591139},{"_id":"themes/next_8.8/source/css/_common/scaffolding/normalize.styl","hash":"b56367ea676ea8e8783ea89cd4ab150c7da7a060","modified":1642412591143},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tables.styl","hash":"e840b23d33023e6d45e018f6e84b683dd56efd8d","modified":1642412591144},{"_id":"themes/next_8.8/source/css/_common/scaffolding/pagination.styl","hash":"b5c7782368889fa9fd93807d28ff2daf270e3703","modified":1642412591143},{"_id":"themes/next_8.8/source/css/_schemes/Gemini/index.styl","hash":"fd49b521d67eaccc629f77b4e095cb7310327565","modified":1642412591153},{"_id":"themes/next_8.8/source/css/_schemes/Mist/_header.styl","hash":"4817e77577896ab5c0da434549917ee703a3f4cf","modified":1642412591154},{"_id":"themes/next_8.8/source/css/_schemes/Mist/_layout.styl","hash":"5604ac1e161099a4d3e5657d53507268866dc717","modified":1642412591154},{"_id":"themes/next_8.8/source/css/_schemes/Mist/_menu.styl","hash":"357b899ac0f0dfbbbebf1ea972030c7cefa463ce","modified":1642412591156},{"_id":"themes/next_8.8/source/css/_common/scaffolding/toggles.styl","hash":"572a41499391677d84b16d8dbd6a996a3d5ce041","modified":1642412591150},{"_id":"themes/next_8.8/source/css/_schemes/Mist/_posts-expand.styl","hash":"b332868d76d9f1651efd65abfc0d3c9d699b1a45","modified":1642412591156},{"_id":"themes/next_8.8/source/css/_schemes/Muse/_header.styl","hash":"06080fd963c904d96c00eff098a284e337953013","modified":1642412591158},{"_id":"themes/next_8.8/source/css/_schemes/Mist/index.styl","hash":"ab16a3dcdc0393b9b582ef59dcc13db9320e917c","modified":1642412591157},{"_id":"themes/next_8.8/source/css/_schemes/Muse/_menu.styl","hash":"8a70d51d8f7cd113e5fbc9f0e70c46a072f282c8","modified":1642412591159},{"_id":"themes/next_8.8/source/css/_schemes/Muse/_sidebar.styl","hash":"944364893bd7160d954c10ba931af641c91515a4","modified":1642412591160},{"_id":"themes/next_8.8/source/css/_schemes/Muse/_layout.styl","hash":"82a29572dd90451f75358a2ee2522b87304a0bb8","modified":1642412591159},{"_id":"themes/next_8.8/source/css/_schemes/Muse/_sub-menu.styl","hash":"c48ccd8d6651fe1a01faff8f01179456d39ba9b1","modified":1642412591160},{"_id":"themes/next_8.8/source/css/_schemes/Pisces/_header.styl","hash":"b741ab96e73370711c63a6581159f2ea8b5bfa1b","modified":1642412591162},{"_id":"themes/next_8.8/source/css/_schemes/Muse/index.styl","hash":"6ad168288b213cec357e9b5a97674ff2ef3a910c","modified":1642412591161},{"_id":"themes/next_8.8/source/css/_schemes/Pisces/_layout.styl","hash":"6eee86c8f0175d6c09e434053516cd8556f78d44","modified":1642412591163},{"_id":"themes/next_8.8/source/css/_schemes/Pisces/_sidebar.styl","hash":"d9141e6e14a56b5952488101e9a8388c2170e270","modified":1642412591164},{"_id":"themes/next_8.8/source/css/_schemes/Pisces/index.styl","hash":"8000075b227749a7495eaf417cac6ccfbe441580","modified":1642412591166},{"_id":"themes/next_8.8/source/css/_schemes/Pisces/_sub-menu.styl","hash":"778ed2ad5643b93970c95626b325defeb586733f","modified":1642412591165},{"_id":"themes/next_8.8/source/js/third-party/analytics/baidu-analytics.js","hash":"f629acc46ff40c071ffd31b77d5c7616f0fdd778","modified":1642412591200},{"_id":"themes/next_8.8/source/js/third-party/analytics/growingio.js","hash":"78dd3cf04082b7dbe6246e404b2aa8e726922402","modified":1642412591202},{"_id":"themes/next_8.8/source/css/_schemes/Pisces/_menu.styl","hash":"72dc825c50357402c342d62ab60fc0c478ab6bc1","modified":1642412591164},{"_id":"themes/next_8.8/source/js/third-party/chat/gitter.js","hash":"cc38c94125f90dadde11b5ebac7d8bf99a1a08a2","modified":1642412591204},{"_id":"themes/next_8.8/source/js/third-party/chat/chatra.js","hash":"c32180522788c10e51df1803aa6842ef0432ddc9","modified":1642412591203},{"_id":"themes/next_8.8/source/js/third-party/analytics/google-analytics.js","hash":"59684383385059dc4f8a1ff85dbbeb703bcdbcb5","modified":1642412591201},{"_id":"themes/next_8.8/source/js/third-party/comments/disqus.js","hash":"e1cc671b0d524864fd445e3ab4ade9ee6d07e565","modified":1642412591206},{"_id":"themes/next_8.8/source/js/third-party/chat/tidio.js","hash":"b0079f6a4601e06ca6fe46e83a2f5af553e9bc3c","modified":1642412591204},{"_id":"themes/next_8.8/source/js/third-party/comments/gitalk.js","hash":"0ec038cf83e8ec067534f16a54041e47a3c1e59a","modified":1642412591207},{"_id":"themes/next_8.8/source/js/third-party/comments/changyan.js","hash":"8c8ebec444c727b704ea41ad88b0b96ed2e4b8d4","modified":1642412591205},{"_id":"themes/next_8.8/source/js/third-party/comments/isso.js","hash":"753a873b6f566aff5ba77ca23f91b78eb880ca64","modified":1642412591208},{"_id":"themes/next_8.8/source/js/third-party/math/mathjax.js","hash":"5c749b9c1c3bb738122d0516211ecff6496d4907","modified":1642412591212},{"_id":"themes/next_8.8/source/js/third-party/comments/livere.js","hash":"2247d88c934c765c43013337860774aaa99f0b31","modified":1642412591209},{"_id":"themes/next_8.8/source/js/third-party/comments/utterances.js","hash":"f67f90eb03e284c82da2b8cf2f1e31801813c16d","modified":1642412591210},{"_id":"themes/next_8.8/source/js/third-party/comments/disqusjs.js","hash":"b6c58f098473b526d6a3cd35655caf34b77f7cff","modified":1642412591207},{"_id":"themes/next_8.8/source/js/third-party/math/katex.js","hash":"83c54ee536e487a1031783443fe0cb63b1b4767e","modified":1642412591211},{"_id":"themes/next_8.8/source/js/third-party/search/algolia-search.js","hash":"ac401e3736d56a3c9cb85ab885744cce0b813c55","modified":1642412591215},{"_id":"themes/next_8.8/source/js/third-party/search/local-search.js","hash":"45c485f82258d246f37deb66884bd2643323ef3a","modified":1642412591216},{"_id":"themes/next_8.8/source/js/third-party/tags/mermaid.js","hash":"3dc4628efa6debd6490fc0ebddff2424a7b319d8","modified":1642412591218},{"_id":"themes/next_8.8/source/js/third-party/statistics/lean-analytics.js","hash":"5a928990856b8e456f0663cf3b6b406733672e39","modified":1642412591217},{"_id":"themes/next_8.8/source/css/_common/components/pages/breadcrumb.styl","hash":"8afdc311c6b8db121758371f95cf1c5e77354f42","modified":1642412591099},{"_id":"themes/next_8.8/source/js/third-party/tags/pdf.js","hash":"af78c22f0e61c8c8aa8794e585e0d632c6d4fcb8","modified":1642412591219},{"_id":"themes/next_8.8/source/js/third-party/statistics/firestore.js","hash":"0960f16107ed61452fb0dffc6ed22dc143de34ef","modified":1642412591217},{"_id":"themes/next_8.8/source/css/_common/components/pages/categories.styl","hash":"b6e2eb1550a7845cb2adf86081a4ab6c7bde1e68","modified":1642412591099},{"_id":"themes/next_8.8/source/css/_common/components/pages/schedule.styl","hash":"6b816c2511242ee503fb5f34cd3e4dcdafc06b85","modified":1642412591100},{"_id":"themes/next_8.8/source/css/_common/components/pages/index.styl","hash":"7504dbc5c70262b048143b2c37d2b5aa2809afa2","modified":1642412591100},{"_id":"themes/next_8.8/source/css/_common/components/pages/tag-cloud.styl","hash":"1a81d1a71fcf0699629ce6e72dfd0a15f3a2dd0a","modified":1642412591100},{"_id":"themes/next_8.8/source/css/_common/components/post/post-body.styl","hash":"ea351936d71e0b6259febac3d7d56d1be6927bf9","modified":1642412591102},{"_id":"themes/next_8.8/source/css/_common/components/post/post-followme.styl","hash":"fc1a7bac6493f24aa50665574f37f3dd954f210c","modified":1642412591106},{"_id":"themes/next_8.8/source/css/_common/components/post/post-collapse.styl","hash":"ec37a36e94ba791663607a5022f763915778578f","modified":1642412591104},{"_id":"themes/next_8.8/source/css/_common/components/post/post-footer.styl","hash":"1d284f3ea03ba9b4feb76b375e539a8e0bccf1c3","modified":1642412591106},{"_id":"themes/next_8.8/source/css/_common/components/post/index.styl","hash":"d0805a763176b3c0003967401644f41dfe3bc9e8","modified":1642412591101},{"_id":"themes/next_8.8/source/css/_common/components/post/post-gallery.styl","hash":"aa366d37389760c8595529b850f461569577a1c5","modified":1642412591107},{"_id":"themes/next_8.8/source/css/_common/components/post/post-nav.styl","hash":"9ac6f477177264c26a46e8333b8456720a0444dc","modified":1642412591109},{"_id":"themes/next_8.8/source/css/_common/components/third-party/disqusjs.styl","hash":"c2326ee3e8b724d99c24a818ddee32813ea5bf89","modified":1642412591113},{"_id":"themes/next_8.8/source/css/_common/components/post/post-header.styl","hash":"010c901e4ef49a606f8a350efbf09044e76d2ff3","modified":1642412591108},{"_id":"themes/next_8.8/source/css/_common/components/post/post-widgets.styl","hash":"b6677dc2a2368084ab82bb4f145ac79e5966c150","modified":1642412591111},{"_id":"themes/next_8.8/source/css/_common/components/third-party/index.styl","hash":"fb0b9eaca498be8af0bc430171a17becf87f8554","modified":1642412591114},{"_id":"themes/next_8.8/source/css/_common/components/post/post-reward.styl","hash":"07cff69f2d57e6321595f64c16d8b763dc88df6a","modified":1642412591110},{"_id":"themes/next_8.8/source/css/_common/components/third-party/gitalk.styl","hash":"070737d101e7cd58e997e8c7af09958268c43a21","modified":1642412591113},{"_id":"themes/next_8.8/source/css/_common/components/third-party/math.styl","hash":"9d995eb4871a6c273d9d51558676a1fdabf69e72","modified":1642412591115},{"_id":"themes/next_8.8/source/css/_common/components/third-party/related-posts.styl","hash":"41ed817e1eb64078074e245e771446ee041e5790","modified":1642412591116},{"_id":"themes/next_8.8/source/css/_common/components/third-party/search.styl","hash":"e72799ce3f9b79753e365b2f8c8ef6c310668d4a","modified":1642412591116},{"_id":"themes/next_8.8/source/css/_common/components/third-party/utterances.styl","hash":"56d90ae0559caa55b75f3c300ff2711f9ed65fc4","modified":1642412591117},{"_id":"themes/next_8.8/source/css/_common/outline/header/bookmark.styl","hash":"e74f4bb47a101b014ee2a1783c87f3b87323f9a0","modified":1642412591120},{"_id":"themes/next_8.8/source/css/_common/outline/header/github-banner.styl","hash":"38c64c2d04e46848382bfa246a0e9c508294767b","modified":1642412591121},{"_id":"themes/next_8.8/source/css/_common/outline/footer/index.styl","hash":"8b9407e5cfd0571ef8de7df19022b268f962fa2f","modified":1642412591119},{"_id":"themes/next_8.8/source/css/_common/outline/header/index.styl","hash":"650ed4ad6df1b6ff04647e7b6d568304e4d3ed2e","modified":1642412591122},{"_id":"themes/next_8.8/source/css/_common/outline/header/menu.styl","hash":"392fd53a8dd4e3f33a853ebb24290a622300e0ff","modified":1642412591123},{"_id":"themes/next_8.8/source/css/_common/outline/header/site-meta.styl","hash":"759e582d34d08e3386c55d87a835a9523608619f","modified":1642412591124},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/sidebar-author-links.styl","hash":"52fc98b1435129eb3edb9293ced9e507741f1350","modified":1642412591128},{"_id":"themes/next_8.8/source/css/_common/outline/header/site-nav.styl","hash":"bf3ad8b4268f763a1e26377681644887694bc009","modified":1642412591125},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/sidebar-author.styl","hash":"5b38ac4a0f1ade0e681aff0e3366c481d9cf3dcd","modified":1642412591130},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/sidebar-blogroll.styl","hash":"9950c3188a28e1c63b5498b7bdcd14b12ace3e28","modified":1642412591130},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/sidebar-button.styl","hash":"b926e368f702f8686aaa2eb98d3d2e533418958c","modified":1642412591131},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/index.styl","hash":"cee43480eba028c37d51cb620c2d81486aa24e01","modified":1642412591127},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/sidebar-dimmer.styl","hash":"fbdb63c6a8887d19b7137325ba7d6806f728139c","modified":1642412591132},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/sidebar-toc.styl","hash":"021a37cf178440cc341940a299d3bca359996c6b","modified":1642412591133},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/sidebar-nav.styl","hash":"ee94a1a27090ad24e3ed579093088d97ff96d77d","modified":1642412591132},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/sidebar-toggle.styl","hash":"3103b81fc76b59e1e2c161e2c484625c770ed66f","modified":1642412591134},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/site-state.styl","hash":"26dd0adfcb1db6df29c6090c8d7e9b5a43583fb0","modified":1642412591134},{"_id":"themes/next_8.8/source/css/_common/scaffolding/highlight/copy-code.styl","hash":"83ee4993710fc8daa1c8dbfccd5d5091fd244c30","modified":1642412591140},{"_id":"themes/next_8.8/source/css/_common/scaffolding/highlight/index.styl","hash":"0b3e2696eca39781c3524b2c5a2555ebc616e6e8","modified":1642412591141},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tags/label.styl","hash":"debee14539272fbe3835a7d3853af2230baa3501","modified":1642412591146},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tags/group-pictures.styl","hash":"393ff96234e4196b569d4b11496774eb78e147de","modified":1642412591146},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tags/blockquote-center.styl","hash":"d6418fd2bbfba7b73ddf11ec62db9637fdf5d8af","modified":1642412591145},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tags/index.styl","hash":"3f76c73a891bbc10679753e702feba9e8a5ffdd2","modified":1642412591146},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tags/mermaid.styl","hash":"48d35dba575a7c9e8845b16652e76b7d4a4646de","modified":1642412591148},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tags/link-grid.styl","hash":"7f8a7345e6537a62cd9e9a94c8f7065b541d9b04","modified":1642412591147},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tags/note.styl","hash":"d27fbf7799695295dd5860a161a13ac4d90c5ba4","modified":1642412591148},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tags/pdf.styl","hash":"b6654a1d7cf82577d8263faffee8af3ad4a5c0e8","modified":1642412591149},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tags/tabs.styl","hash":"9b34143aec49e390e18f380026a45096f7477722","modified":1642412591150},{"_id":"themes/next_8.8/source/images/avatar.jpg","hash":"ed5dde31684ebfd99e9965da1140fb919496f1d3","modified":1642412591175},{"_id":"themes/next_8.8/source/images/avatar_300.jpg","hash":"13b3ca4593cd3c96d4f973b26ec9dbe51e28ed29","modified":1642412591178},{"_id":"themes/next_8.8/source/images/avatar_500.jpg","hash":"fa083b4b1260c2ee43810fc4819e958196ff7d49","modified":1642412591179},{"_id":"public/sitemap.xml","hash":"04ebdc610868532d186bd23aefda111d70e2f002","modified":1644308078664},{"_id":"public/about/index.html","hash":"98115932dcf3db1cb5e2dd71d1a0e8d02c8f8866","modified":1643706079628},{"_id":"public/tags/index.html","hash":"bf9f1d553e5bbcb243bff0ebba5bebfae1874ac4","modified":1643706079628},{"_id":"public/categories/index.html","hash":"598577bea490f4c37fc125f7c45d1974e74909cd","modified":1643706079628},{"_id":"public/2022/01/10/How-to-Blance-Losses-in-Multi-Task-Training/index.html","hash":"eae95e47fe7b77ab75b2e476594cbe9aaacd9018","modified":1643706079628},{"_id":"public/2021/12/18/Transformer-and-BERT/index.html","hash":"f363c467218fd190535dc2d1f72264bd0afb0ec9","modified":1644308556153},{"_id":"public/2021/12/17/Algorithm/index.html","hash":"0875ac0a622c7270c76878439cd4ee9fd6ec12ac","modified":1643706079628},{"_id":"public/2021/12/10/An-Introduction-to-Git/index.html","hash":"b51899c70d7a675e0f4ae9ed2aaad9069ab706c9","modified":1643706079628},{"_id":"public/archives/index.html","hash":"a21d890b387460fda1355e2929e2da5be768f3af","modified":1643706079628},{"_id":"public/archives/2021/index.html","hash":"a8b087b80ed6263c752c1bcba87669b753db04e5","modified":1643706079628},{"_id":"public/archives/2021/12/index.html","hash":"80e5451e888ab37591a0ae40c03e67fd0b3b3f45","modified":1643706079628},{"_id":"public/archives/2022/index.html","hash":"585c1e5ba32d46c2aa2c84a93fb1aea57d051af5","modified":1643706079628},{"_id":"public/archives/2022/01/index.html","hash":"99b3b01d4383e5d4f620ad19748ffa02b8cec011","modified":1643706079628},{"_id":"public/categories/Programming/index.html","hash":"3ae98724e426d491e6ee709d77b548fe9d3e242f","modified":1643706079628},{"_id":"public/categories/Little-Things/index.html","hash":"44374d3164396589a43b4ce1eed9e064b974cb14","modified":1643706079628},{"_id":"public/categories/Experiments/index.html","hash":"c126985e317f1e87027b41fba6f58a4fe2466132","modified":1643706079628},{"_id":"public/categories/Topological-Data-Analysis/index.html","hash":"f8fa558a40e1282cc45a99a119317b55d4c18a7d","modified":1643706079628},{"_id":"public/categories/Little-Things/Git/index.html","hash":"2fbb764d59ca13ed0df63fc7586c84caa34e629b","modified":1643706079628},{"_id":"public/categories/Reinforcement-Learning/index.html","hash":"72c9c6e15e993d766a01f8b4a64597f2ca39a750","modified":1643706079628},{"_id":"public/categories/About-Papers/index.html","hash":"905541f32a9b2dcbd7aed371a054b910d604bab5","modified":1643706079628},{"_id":"public/categories/Natural-Language-Processing/index.html","hash":"68b181264d93b7d0636bbbf3cef94c389105e23c","modified":1643706079628},{"_id":"public/tags/Algorithm/index.html","hash":"63cd86d38498321684481527288bd5446b90afa2","modified":1643706079628},{"_id":"public/tags/Programming/index.html","hash":"2cef45b4e94f9df2571fc013da4c13af0c3320cb","modified":1643706079628},{"_id":"public/tags/Git/index.html","hash":"e0a83a2a225bf04512d112d8b03ebbd1e347c842","modified":1643706079628},{"_id":"public/tags/vim/index.html","hash":"9366ca9c8dddea30d57c130c22c340be4b11ce16","modified":1643706079628},{"_id":"public/tags/markdown/index.html","hash":"86bba9e9bba555df34a30daff1e1b312ad1ee9fc","modified":1643706079628},{"_id":"public/tags/vimtex/index.html","hash":"6d7431caac18f9a2adcab2287843a3f3613cd080","modified":1643706079628},{"_id":"public/tags/Personal-Thought/index.html","hash":"74f0fa4b4252283f3baa1a901a7fa13fe7c12145","modified":1643706079628},{"_id":"public/tags/Experiments/index.html","hash":"fa3355c93b2d3874f595ddb5ccc38dfded35fe7c","modified":1643706079628},{"_id":"public/tags/private/index.html","hash":"561c68d374a4ec832ff5954c5c3700ecffc17a54","modified":1643706079628},{"_id":"public/tags/Topological-Data-Analysis/index.html","hash":"7ec692aae768d2eb1c713a34dc9aef58cd8b6b78","modified":1643706079628},{"_id":"public/tags/Reinforcement-Learning/index.html","hash":"135284dde5cb6727d39898e6feb01b2a085a0e85","modified":1643706079628},{"_id":"public/tags/Multi-Task-Training/index.html","hash":"07d425c5448068b6c46f7260a200b30c5d3f1587","modified":1643706079628},{"_id":"public/tags/Papers/index.html","hash":"4576af8cb052bdb838fa9fb16970d758d27f9a84","modified":1643706079628},{"_id":"public/tags/Transformer/index.html","hash":"7c55d53ccab38de1ebcbab17a35414943d4047c1","modified":1643706079628},{"_id":"public/tags/Natural-Language-Processing/index.html","hash":"37ee2fbfda6f10a75b0857280d514b8676208988","modified":1643706079628},{"_id":"public/2022/01/20/Config-Vim/index.html","hash":"5da58fbae348e3eefe997c63350cfec61453e9bd","modified":1644308078664},{"_id":"public/2021/12/30/Foundation-for-Topological-Data-Analysis/index.html","hash":"15e36e237edcf8423234da0e818080e7e74099f8","modified":1643706079628},{"_id":"public/2021/12/17/Experiments/index.html","hash":"cdb655af676b35cd4b7100b067631f719a71f090","modified":1643706079628},{"_id":"public/2021/12/15/Personal-Thought/index.html","hash":"e9e58a7f61e35793a118060923a710dd8633d597","modified":1643706079628},{"_id":"public/2021/12/14/Tips-in-Papers/index.html","hash":"18653410c06c1c310df071879540cf66dd8b9884","modified":1643706079628},{"_id":"public/2021/12/03/First-Step-to-RL/index.html","hash":"51743dc278d05b38ff3558766c20f836898e9f06","modified":1643706079628},{"_id":"public/2021/11/24/Construct-Your-Blog-with-Hexo-and-Github/index.html","hash":"b733293940ac6cd813cf2de16f23c95e06f381c8","modified":1643706079628},{"_id":"public/index.html","hash":"df78f1ebe181f9b3ea2b74beb7203d2a36661192","modified":1644308556153},{"_id":"public/tags/BERT/index.html","hash":"af9afa9f1c9709475f187b46fa8bd4363231ced2","modified":1643706079628},{"_id":"public/CNAME","hash":"55c55444033af34e797e2d3cb1c1b4cf7889b3db","modified":1643706079628},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1643706079628},{"_id":"public/images/avatar_100.jpg","hash":"a0f0a00c9326c57a5cc181aeb80dfda8e947920a","modified":1643706079628},{"_id":"public/images/avatar.gif","hash":"2dbc3e2f2d624b2ca1afe6edc2ca17307f1950c8","modified":1643706079628},{"_id":"public/images/avatar_200.jpg","hash":"4a76d1527e511350c9112e899046a34be59ad278","modified":1643706079628},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1643706079628},{"_id":"public/images/avatar_80.jpg","hash":"465e53598964df3852f08794cb73c5e459855e92","modified":1643706079628},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1643706079628},{"_id":"public/images/logo.svg","hash":"2cb74fd3ea2635e015eabc58a8d488aed6cf6417","modified":1643706079628},{"_id":"public/images/logo-algolia-nebula-blue-full.svg","hash":"b85e274207b1392782476a0430feac98db1e7da0","modified":1643706079628},{"_id":"public/images/ye_16.ico","hash":"b8fb01b5361da89831d232a831a1532e9822bd72","modified":1643706079628},{"_id":"public/images/ye_32.ico","hash":"375c99cd785d93dd989c36604ffbd10ada71322a","modified":1643706079628},{"_id":"public/2021/12/14/Tips-in-Papers/ADCM.jpg","hash":"ee5b75f3a063e07624c5f03111f98902d885cd8e","modified":1643706079628},{"_id":"public/2021/12/03/First-Step-to-RL/q_learning.py","hash":"3a68ec1153d26f2c18c9d34ee5deb5e495b8dca9","modified":1643706079628},{"_id":"public/2021/12/03/First-Step-to-RL/policy_network.py","hash":"b3e8f06360cad0084a0656edaed4539f1b01e327","modified":1643706079628},{"_id":"public/2021/12/14/Tips-in-Papers/Saccader_eq1.jpg","hash":"2ea2a74f960377a6a9d0bc98ecfadd8ba09cd316","modified":1643706079628},{"_id":"public/2021/12/14/Tips-in-Papers/Saccader_eq2.jpg","hash":"cf5832e89b29e39275d2fa4a2e39592e97f9a39e","modified":1643706079628},{"_id":"public/2021/12/14/Tips-in-Papers/Saccader_eq3.jpg","hash":"8fa845242bb6f737736d91485ca237e7132e19b5","modified":1643706079628},{"_id":"public/2021/12/14/Tips-in-Papers/Saccader_gl.jpg","hash":"fb8050f8b71a7c667cfa85672b505b3340d2231b","modified":1643706079628},{"_id":"public/images/avatar.jpg","hash":"ed5dde31684ebfd99e9965da1140fb919496f1d3","modified":1643706079628},{"_id":"public/images/avatar_300.jpg","hash":"13b3ca4593cd3c96d4f973b26ec9dbe51e28ed29","modified":1643706079628},{"_id":"public/2021/12/14/Tips-in-Papers/Saccader_Cell.jpg","hash":"b17e79bf86bb3422527bf51d200ceceeb53ce936","modified":1643706079628},{"_id":"public/2021/12/14/Tips-in-Papers/Saccader_Over.jpg","hash":"deaccd942ffca9e163a9f2c9bdfb69251d1004cb","modified":1643706079628},{"_id":"public/css/hbe.style.css","hash":"b0a0077cb588c0941823905fcc383aa7509ade73","modified":1643706079628},{"_id":"public/lib/hbe.js","hash":"136dba00826bdd086153bf0acb5473aea7183ad1","modified":1643706079628},{"_id":"public/css/noscript.css","hash":"54d14cd43dc297950a4a8d39ec9644dd5fc3499f","modified":1643706079628},{"_id":"public/js/comments-buttons.js","hash":"1a7344440321713426a0b2ab17e276b5bdf85ade","modified":1643706079628},{"_id":"public/js/bookmark.js","hash":"0f563ffbf05fad30e854e413ab17ff7164ab5a53","modified":1643706079628},{"_id":"public/js/next-boot.js","hash":"48497e2156a10155dc42311633a110c9685692c9","modified":1643706079628},{"_id":"public/js/config.js","hash":"4c4ebbe3b3f3841a26f9d5af6d0ba8bc6da01c54","modified":1643706079628},{"_id":"public/js/comments.js","hash":"66ae2e26ea36a41b72c638ea8b220296638ae952","modified":1643706079628},{"_id":"public/js/motion.js","hash":"9c4c861dfb080b6244d4d9eba33ac686735754f3","modified":1643706079628},{"_id":"public/js/schemes/muse.js","hash":"9a836d2bcc3defe4bd1ee51f5f4eb7006ebdd41b","modified":1643706079628},{"_id":"public/js/utils.js","hash":"e447160d342b1f93df5214b6a733441039ced439","modified":1643706079628},{"_id":"public/js/pjax.js","hash":"919f5281c4a04d11cfd94573ecf57b3dbabd3cc8","modified":1643706079628},{"_id":"public/js/third-party/rating.js","hash":"4e92c2d107ba47b47826829f9668030d5ea9bfb8","modified":1643706079628},{"_id":"public/js/third-party/analytics/baidu-analytics.js","hash":"f629acc46ff40c071ffd31b77d5c7616f0fdd778","modified":1643706079628},{"_id":"public/js/third-party/pace.js","hash":"0ef04218b93561ba4d0ff420d556c3d90a756d32","modified":1643706079628},{"_id":"public/js/third-party/analytics/google-analytics.js","hash":"59684383385059dc4f8a1ff85dbbeb703bcdbcb5","modified":1643706079628},{"_id":"public/js/schedule.js","hash":"2b43e2d576a308289880befc27580dbb2aa34439","modified":1643706079628},{"_id":"public/js/third-party/quicklink.js","hash":"6f58cd7aa8f6f1ab92d5a96551add293f4e55312","modified":1643706079628},{"_id":"public/js/third-party/chat/chatra.js","hash":"c32180522788c10e51df1803aa6842ef0432ddc9","modified":1643706079628},{"_id":"public/js/third-party/chat/tidio.js","hash":"b0079f6a4601e06ca6fe46e83a2f5af553e9bc3c","modified":1643706079628},{"_id":"public/js/third-party/comments/changyan.js","hash":"8c8ebec444c727b704ea41ad88b0b96ed2e4b8d4","modified":1643706079628},{"_id":"public/js/third-party/comments/disqus.js","hash":"e1cc671b0d524864fd445e3ab4ade9ee6d07e565","modified":1643706079628},{"_id":"public/js/third-party/comments/disqusjs.js","hash":"b6c58f098473b526d6a3cd35655caf34b77f7cff","modified":1643706079628},{"_id":"public/js/third-party/comments/gitalk.js","hash":"0ec038cf83e8ec067534f16a54041e47a3c1e59a","modified":1643706079628},{"_id":"public/js/third-party/comments/isso.js","hash":"753a873b6f566aff5ba77ca23f91b78eb880ca64","modified":1643706079628},{"_id":"public/js/third-party/comments/livere.js","hash":"2247d88c934c765c43013337860774aaa99f0b31","modified":1643706079628},{"_id":"public/js/third-party/comments/utterances.js","hash":"f67f90eb03e284c82da2b8cf2f1e31801813c16d","modified":1643706079628},{"_id":"public/js/third-party/math/mathjax.js","hash":"5c749b9c1c3bb738122d0516211ecff6496d4907","modified":1643706079628},{"_id":"public/js/third-party/math/katex.js","hash":"83c54ee536e487a1031783443fe0cb63b1b4767e","modified":1643706079628},{"_id":"public/js/third-party/search/algolia-search.js","hash":"ac401e3736d56a3c9cb85ab885744cce0b813c55","modified":1643706079628},{"_id":"public/js/third-party/chat/gitter.js","hash":"cc38c94125f90dadde11b5ebac7d8bf99a1a08a2","modified":1643706079628},{"_id":"public/js/third-party/statistics/lean-analytics.js","hash":"5a928990856b8e456f0663cf3b6b406733672e39","modified":1643706079628},{"_id":"public/js/third-party/tags/mermaid.js","hash":"3dc4628efa6debd6490fc0ebddff2424a7b319d8","modified":1643706079628},{"_id":"public/js/third-party/statistics/firestore.js","hash":"0960f16107ed61452fb0dffc6ed22dc143de34ef","modified":1643706079628},{"_id":"public/js/third-party/tags/pdf.js","hash":"af78c22f0e61c8c8aa8794e585e0d632c6d4fcb8","modified":1643706079628},{"_id":"public/js/third-party/analytics/growingio.js","hash":"78dd3cf04082b7dbe6246e404b2aa8e726922402","modified":1643706079628},{"_id":"public/js/third-party/search/local-search.js","hash":"45c485f82258d246f37deb66884bd2643323ef3a","modified":1643706079628},{"_id":"public/js/third-party/fancybox.js","hash":"c098d14e65dd170537134358d4b8359ad0539c2c","modified":1643706079628},{"_id":"public/css/main.css","hash":"b4cd7ab94ab2fdff102e77147fe884d4b2553997","modified":1643706079628},{"_id":"public/images/avatar_500.jpg","hash":"fa083b4b1260c2ee43810fc4819e958196ff7d49","modified":1643706079628},{"_id":"public/2021/12/14/Tips-in-Papers/Tnet_over.jpg","hash":"ed04ca4049c6f909b0bf51ea57ffa9a2b445e1c8","modified":1643706079628},{"_id":"public/2021/12/10/An-Introduction-to-Git/git.jpg","hash":"db9ed8bb86df7e73d5be3bcae4cc8656e4a7a0ed","modified":1643706079628}],"Category":[{"name":"Programming","_id":"ckz3w5stn00040cvkfetd3p1f"},{"name":"Little Things","_id":"ckz3w5stz000a0cvk53j7dzkl"},{"name":"Experiments","_id":"ckz3w5suj000r0cvk238sfky5"},{"name":"Topological Data Analysis","_id":"ckz3w5sul000v0cvkglkp66ud"},{"name":"Git","parent":"ckz3w5stz000a0cvk53j7dzkl","_id":"ckz3w5sum000y0cvk7aqy1hqy"},{"name":"Reinforcement Learning","_id":"ckz3w5suo00110cvk8gsogxrj"},{"name":"Hexo","parent":"ckz3w5stz000a0cvk53j7dzkl","_id":"ckz3w5suo00150cvkbt2w3tp1"},{"name":"About Papers","_id":"ckz3w5suq001b0cvkczqu1wkz"},{"name":"Natural Language Processing","_id":"ckz3w5sux001i0cvkflbk875e"}],"Data":[],"Page":[{"title":"about","date":"2021-12-12T13:52:49.000Z","_content":"\n\n\n<!-- I received the B.S. degree in the School of Engineering and Technology, China University of Geosciences (Beijing) in 2015. I am currently working towards the Ph.D degree in the Beijing Key Laboratory of Work Safety Intelligent Monitoring, the Department of EE, BUPT. -->\n\n<!-- I major in deep learning, computer vision, natural language processing, and reinforcement learning. -->\n\n<!-- I am also interested in high performance computing. -->\n\n<br/>\n\n<br/>\n\n---\n\n<br/>\n\n<br/>\n\n>      *There is a pleasure in the pathless woods;*\n>      *there is a rapture on the lonely shore;*\n>      *there is society, where none intrudes,*\n>      *by the deep sea, and music in its roar;*\n>      *I love not man the less, but nature more...*\n>                           *by George Gordon Byron* \n\n","source":"about/index.md","raw":"---\ntitle: about\ndate: 2021-12-12 21:52:49\n---\n\n\n\n<!-- I received the B.S. degree in the School of Engineering and Technology, China University of Geosciences (Beijing) in 2015. I am currently working towards the Ph.D degree in the Beijing Key Laboratory of Work Safety Intelligent Monitoring, the Department of EE, BUPT. -->\n\n<!-- I major in deep learning, computer vision, natural language processing, and reinforcement learning. -->\n\n<!-- I am also interested in high performance computing. -->\n\n<br/>\n\n<br/>\n\n---\n\n<br/>\n\n<br/>\n\n>      *There is a pleasure in the pathless woods;*\n>      *there is a rapture on the lonely shore;*\n>      *there is society, where none intrudes,*\n>      *by the deep sea, and music in its roar;*\n>      *I love not man the less, but nature more...*\n>                           *by George Gordon Byron* \n\n","updated":"2022-01-17T09:43:07.988Z","path":"about/index.html","comments":1,"layout":"page","_id":"ckz3w5st700000cvk26gma5ln","content":"<!-- I received the B.S. degree in the School of Engineering and Technology, China University of Geosciences (Beijing) in 2015. I am currently working towards the Ph.D degree in the Beijing Key Laboratory of Work Safety Intelligent Monitoring, the Department of EE, BUPT. --><!-- I major in deep learning, computer vision, natural language processing, and reinforcement learning. --><!-- I am also interested in high performance computing. --><html><head></head><body><p><br></p>\n<p><br></p>\n<hr>\n<p><br></p>\n<p><br></p>\n<blockquote>\n<p> <em>There is a pleasure in the pathless woods;</em><br>\n <em>there is a rapture on the lonely shore;</em><br>\n <em>there is society, where none intrudes,</em><br>\n <em>by the deep sea, and music in its roar;</em><br>\n <em>I love not man the less, but nature more...</em><br>\n <em>by George Gordon Byron</em></p>\n</blockquote>\n</body></html>","site":{"data":{}},"excerpt":"<html><head></head><body></body></html>","more":"<!-- I received the B.S. degree in the School of Engineering and Technology, China University of Geosciences (Beijing) in 2015. I am currently working towards the Ph.D degree in the Beijing Key Laboratory of Work Safety Intelligent Monitoring, the Department of EE, BUPT. -->\n<!-- I major in deep learning, computer vision, natural language processing, and reinforcement learning. -->\n<!-- I am also interested in high performance computing. -->\n<p><br/></p>\n<p><br/></p>\n<hr />\n<p><br/></p>\n<p><br/></p>\n<blockquote>\n<p> <em>There is a pleasure in the pathless woods;</em><br />\n <em>there is a rapture on the lonely shore;</em><br />\n <em>there is society, where none intrudes,</em><br />\n <em>by the deep sea, and music in its roar;</em><br />\n <em>I love not man the less, but nature more...</em><br />\n <em>by George Gordon Byron</em></p>\n</blockquote>\n"},{"title":"tags","date":"2021-12-08T06:48:06.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2021-12-08 14:48:06\ntype: \"tags\"\n---\n","updated":"2022-01-17T09:43:07.990Z","path":"tags/index.html","comments":1,"layout":"page","_id":"ckz3w5stj00020cvk8o242ifb","content":"<html><head></head><body></body></html>","site":{"data":{}},"excerpt":"<html><head></head><body></body></html>","more":"\n"},{"title":"categories","date":"2021-11-24T11:46:36.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2021-11-24 19:46:36\ntype: \"categories\"\n---\n","updated":"2022-01-17T09:43:07.989Z","path":"categories/index.html","comments":1,"layout":"page","_id":"ckz3w5stq00060cvkczxg3deg","content":"<html><head></head><body></body></html>","site":{"data":{}},"excerpt":"<html><head></head><body></body></html>","more":"\n"}],"Post":[{"title":"Algorithm","top":false,"cover":false,"toc":true,"mathjax":true,"date":"2021-12-17T08:26:10.000Z","password":null,"summary":null,"description":"","_content":"","source":"_posts/Algorithm.md","raw":"---\ntitle: Algorithm\ntop: false\ncover: false\ntoc: true\nmathjax: true\ndate: 2021-12-17 16:26:10\npassword:\nsummary:\ndescription: \ncategories:\n- Programming\ntags:\n- Algorithm\n- Programming\n---\n","slug":"Algorithm","published":1,"updated":"2022-01-17T09:43:07.967Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckz3w5stc00010cvk1e6nfxqx","content":"<html><head></head><body></body></html>","site":{"data":{}},"excerpt":"<html><head></head><body></body></html>","more":"\n"},{"title":"An Introduction to Git","date":"2021-12-10T13:55:33.000Z","description":"gitgit","summary":null,"_content":"\n\n\n\n\n\n\n![The Structure of Git](git.jpg)\n\ngit checkout\n\ngit ls-files\n\n--cached (-c) \n\n--midified (-m)\n\n--delete (-d)\n\n--other (-o)git\n\n","source":"_posts/An-Introduction-to-Git.md","raw":"---\ntitle: An Introduction to Git\ndate: 2021-12-10 21:55:33\ndescription: gitgit\nsummary:\ncategories:\n- Little Things\n- Git\ntags:\n- Git\n---\n\n\n\n\n\n\n\n![The Structure of Git](git.jpg)\n\ngit checkout\n\ngit ls-files\n\n--cached (-c) \n\n--midified (-m)\n\n--delete (-d)\n\n--other (-o)git\n\n","slug":"An-Introduction-to-Git","published":1,"updated":"2022-01-17T09:43:07.967Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckz3w5stk00030cvkf6sogjin","content":"<html><head></head><body><p><img src=\"git.jpg\" alt=\"The Structure of Git\"></p>\n<p>git checkout</p>\n<p>git ls-files</p>\n<p>--cached (-c) </p>\n<p>--midified (-m)</p>\n<p>--delete (-d)</p>\n<p>--other (-o)git</p>\n</body></html>","site":{"data":{}},"excerpt":"<html><head></head><body></body></html>","more":"<p><img src=\"git.jpg\" alt=\"The Structure of Git\" /></p>\n<p>git checkout</p>\n<p>git ls-files</p>\n<p>--cached (-c) </p>\n<p>--midified (-m)</p>\n<p>--delete (-d)</p>\n<p>--other (-o)git</p>\n"},{"title":"Construct Your Blog with Hexo and Github","date":"2021-11-24T08:20:43.000Z","hidden":true,"description":"Hexo, Next","_content":"\n\n# \n\nhttps://segmentfault.com/a/1190000017986794\n\nhttps://godweiyang.com/2018/04/13/hexo-blog/\n\nhttps://blog.guaoxiaohei.com/posts/Hexo-Level/\n\nhttps://www.itfanr.cc/2021/04/16/hexo-blog-article-encryption/\n\nhttps://www.zhangxinxu.com/wordpress/2021/05/css-html-hr/\n\ntypora\n\nhttps://zhuanlan.zhihu.com/p/110257979\n\n# \n\n## hexo d GitHub\n\n_config.yml\n\n\n\nurl: github,  https://xyegithub.github.io/myBlog/\n\nroot: url/myBlog/\n\nhexo clean; hexo g; hexo d\n\n##  git\n\n\n\nhttps://juejin.cn/post/6844904193170341896\n\ndnscmd ipconfig /flushdns\n\n\n\n### \n\nhexo _config.ymlgit`https://github.com/xxx`ssh `git@github.com:xxx/xxx`\n\n## github page \n\n\n\nhttps://mizeri.github.io/2021/04/18/hexo-sitemap-google/\n\n## \n\n1. freenomyexiang.ml\n2. DNSpodAgithub page\n\n","source":"_posts/Construct-Your-Blog-with-Hexo-and-Github.md","raw":"---\ntitle: Construct Your Blog with Hexo and Github\ndate: 2021-11-24 16:20:43\nhidden: true\ndescription:  Hexo, Next\ntags: \n- Hexo\ncategories:\n- Little Things\n- Hexo\n---\n\n\n# \n\nhttps://segmentfault.com/a/1190000017986794\n\nhttps://godweiyang.com/2018/04/13/hexo-blog/\n\nhttps://blog.guaoxiaohei.com/posts/Hexo-Level/\n\nhttps://www.itfanr.cc/2021/04/16/hexo-blog-article-encryption/\n\nhttps://www.zhangxinxu.com/wordpress/2021/05/css-html-hr/\n\ntypora\n\nhttps://zhuanlan.zhihu.com/p/110257979\n\n# \n\n## hexo d GitHub\n\n_config.yml\n\n\n\nurl: github,  https://xyegithub.github.io/myBlog/\n\nroot: url/myBlog/\n\nhexo clean; hexo g; hexo d\n\n##  git\n\n\n\nhttps://juejin.cn/post/6844904193170341896\n\ndnscmd ipconfig /flushdns\n\n\n\n### \n\nhexo _config.ymlgit`https://github.com/xxx`ssh `git@github.com:xxx/xxx`\n\n## github page \n\n\n\nhttps://mizeri.github.io/2021/04/18/hexo-sitemap-google/\n\n## \n\n1. freenomyexiang.ml\n2. DNSpodAgithub page\n\n","slug":"Construct-Your-Blog-with-Hexo-and-Github","published":1,"updated":"2022-01-17T09:43:07.971Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckz3w5str00070cvkatt21uiq","content":"<html><head></head><body><h1 id=\"\"><span class=\"post-title-index\">1. </span></h1>\n<p>https://segmentfault.com/a/1190000017986794</p>\n<p>https://godweiyang.com/2018/04/13/hexo-blog/</p>\n<p>https://blog.guaoxiaohei.com/posts/Hexo-Level/</p>\n<p>https://www.itfanr.cc/2021/04/16/hexo-blog-article-encryption/</p>\n<p>https://www.zhangxinxu.com/wordpress/2021/05/css-html-hr/</p>\n<p>typora</p>\n<p>https://zhuanlan.zhihu.com/p/110257979</p>\n<h1 id=\"\"><span class=\"post-title-index\">2. </span></h1>\n<h2 id=\"hexo-d-github\"><span class=\"post-title-index\">2.1. </span>hexo d GitHub</h2>\n<p>_config.yml</p>\n<p>url: github,  https://xyegithub.github.io/myBlog/</p>\n<p>root: url/myBlog/</p>\n<p>hexo clean; hexo g; hexo d</p>\n<h2 id=\"git\"><span class=\"post-title-index\">2.2. </span>git</h2>\n<p></p>\n<p>https://juejin.cn/post/6844904193170341896</p>\n<p>dnscmd ipconfig /flushdns</p>\n<p></p>\n<h3 id=\"\"><span class=\"post-title-index\">2.2.1. </span></h3>\n<p>hexo _config.ymlgit<code>https://github.com/xxx</code>ssh <code>git@github.com:xxx/xxx</code></p>\n<h2 id=\"github-page-\"><span class=\"post-title-index\">2.3. </span>github page </h2>\n<p></p>\n<p>https://mizeri.github.io/2021/04/18/hexo-sitemap-google/</p>\n<h2 id=\"\"><span class=\"post-title-index\">2.4. </span></h2>\n<ol type=\"1\">\n<li>freenomyexiang.ml</li>\n<li>DNSpodAgithub page</li>\n</ol>\n<!-- flag of hidden posts --></body></html>","site":{"data":{}},"excerpt":"<html><head></head><body></body></html>","more":"<h1 id=\"\"></h1>\n<p>https://segmentfault.com/a/1190000017986794</p>\n<p>https://godweiyang.com/2018/04/13/hexo-blog/</p>\n<p>https://blog.guaoxiaohei.com/posts/Hexo-Level/</p>\n<p>https://www.itfanr.cc/2021/04/16/hexo-blog-article-encryption/</p>\n<p>https://www.zhangxinxu.com/wordpress/2021/05/css-html-hr/</p>\n<p>typora</p>\n<p>https://zhuanlan.zhihu.com/p/110257979</p>\n<h1 id=\"\"></h1>\n<h2 id=\"hexo-d-github\">hexo d GitHub</h2>\n<p>_config.yml</p>\n<p>url: github,  https://xyegithub.github.io/myBlog/</p>\n<p>root: url/myBlog/</p>\n<p>hexo clean; hexo g; hexo d</p>\n<h2 id=\"git\">git</h2>\n<p></p>\n<p>https://juejin.cn/post/6844904193170341896</p>\n<p>dnscmd ipconfig /flushdns</p>\n<p></p>\n<h3 id=\"\"></h3>\n<p>hexo _config.ymlgit<code>https://github.com/xxx</code>ssh <code>git@github.com:xxx/xxx</code></p>\n<h2 id=\"github-page-\">github page </h2>\n<p></p>\n<p>https://mizeri.github.io/2021/04/18/hexo-sitemap-google/</p>\n<h2 id=\"\"></h2>\n<ol type=\"1\">\n<li>freenomyexiang.ml</li>\n<li>DNSpodAgithub page</li>\n</ol>\n"},{"title":"Config Vim","top":false,"cover":false,"toc":true,"mathjax":true,"date":"2022-01-20T02:24:34.000Z","password":null,"summary":null,"description":"vimmarkdown, vimtex","_content":"\n# anaconda\nanaconda\n`./Anaconda3-2021.11-Linux-x86_64.sh: 489: [[: Exec format error`\n./Anaconda.shsh\nbash Anaconda.sh.\n\n# Linux\n\n## \napt install\n`dpkg -L xxx`\n\n## \n,log\nlog\n\n## linux \n`LIBRARY_PATH`\n`LD_LIBRARY_PATH`\n\n## \nokular: error while loading shared libraries: libQt5Core.so.5: cannot open shared object file: No such file or directory\nstrip\nsudo strip --remove-section=.note.ABI-tag /usr/lib/x86_64-linux-gnu/libQt5Core.so.5\n\n# vim\naptvimvimlatex+clietservice\napt-cache search libc-dev\nln -s libXtst.so.6 libXtst.so\n\nhttps://www.jianshu.com/p/aa5ea81bbc72\nhttps://toutiao.io/posts/runvgs/preview\nconfig \n\n```python\n./configure --with-features=huge \\\n    --enable-multibyte \\\n    --enable-rubyinterp=yes \\\n    --enable-python3interp=yes \\\n    --enable-perlinterp=yes \\\n    --enable-cscope \\\n    --enable-fontset \\\n    --enable-largefile \\\n    --enable-fail-if-missing \\\n    --prefix=/path-to-install\n```\n\n`--enable-fail-if-missing` \n\n## apt\n\nfeature hug+clientserver\nsrc/auto/cofig.log\n`sudo apt-get install vim-gtk`+clientservervim\n\n","source":"_posts/Config-Vim.md","raw":"---\ntitle: Config Vim\ntop: false\ncover: false\ntoc: true\nmathjax: true\ndate: 2022-01-20 10:24:34\npassword:\nsummary:\ndescription:  vimmarkdown, vimtex\ncategories:\n- Little Things\ntags:\n- vim\n- markdown\n- vimtex\n---\n\n# anaconda\nanaconda\n`./Anaconda3-2021.11-Linux-x86_64.sh: 489: [[: Exec format error`\n./Anaconda.shsh\nbash Anaconda.sh.\n\n# Linux\n\n## \napt install\n`dpkg -L xxx`\n\n## \n,log\nlog\n\n## linux \n`LIBRARY_PATH`\n`LD_LIBRARY_PATH`\n\n## \nokular: error while loading shared libraries: libQt5Core.so.5: cannot open shared object file: No such file or directory\nstrip\nsudo strip --remove-section=.note.ABI-tag /usr/lib/x86_64-linux-gnu/libQt5Core.so.5\n\n# vim\naptvimvimlatex+clietservice\napt-cache search libc-dev\nln -s libXtst.so.6 libXtst.so\n\nhttps://www.jianshu.com/p/aa5ea81bbc72\nhttps://toutiao.io/posts/runvgs/preview\nconfig \n\n```python\n./configure --with-features=huge \\\n    --enable-multibyte \\\n    --enable-rubyinterp=yes \\\n    --enable-python3interp=yes \\\n    --enable-perlinterp=yes \\\n    --enable-cscope \\\n    --enable-fontset \\\n    --enable-largefile \\\n    --enable-fail-if-missing \\\n    --prefix=/path-to-install\n```\n\n`--enable-fail-if-missing` \n\n## apt\n\nfeature hug+clientserver\nsrc/auto/cofig.log\n`sudo apt-get install vim-gtk`+clientservervim\n\n","slug":"Config-Vim","published":1,"updated":"2022-02-08T07:30:43.760Z","_id":"ckz3w5sts00080cvk5fyn3gcq","comments":1,"layout":"post","photos":[],"link":"","content":"<html><head></head><body><h1 id=\"anaconda\"><span class=\"post-title-index\">1. </span>anaconda</h1>\n<p>anaconda<br>\n<code>./Anaconda3-2021.11-Linux-x86_64.sh: 489: [[: Exec format error</code><br>\n./Anaconda.shsh<br>\nbash Anaconda.sh.</p>\n<h1 id=\"linux\"><span class=\"post-title-index\">2. </span>Linux</h1>\n<h2 id=\"\"><span class=\"post-title-index\">2.1. </span></h2>\n<p>apt install<br>\n<code>dpkg -L xxx</code></p>\n<h2 id=\"\"><span class=\"post-title-index\">2.2. </span></h2>\n<p>,log<br>\nlog</p>\n<h2 id=\"linux-\"><span class=\"post-title-index\">2.3. </span>linux </h2>\n<p><code>LIBRARY_PATH</code><br>\n<code>LD_LIBRARY_PATH</code></p>\n<h2 id=\"\"><span class=\"post-title-index\">2.4. </span></h2>\n<p>okular: error while loading shared libraries: libQt5Core.so.5: cannot open shared object file: No such file or directory<br>\nstrip<br>\nsudo strip --remove-section=.note.ABI-tag /usr/lib/x86_64-linux-gnu/libQt5Core.so.5</p>\n<h1 id=\"vim\"><span class=\"post-title-index\">3. </span>vim</h1>\n<p>aptvimvimlatex+clietservice<br>\napt-cache search libc-dev<br>\nln -s libXtst.so.6 libXtst.so<br>\n<br>\nhttps://www.jianshu.com/p/aa5ea81bbc72<br>\nhttps://toutiao.io/posts/runvgs/preview<br>\nconfig </p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./configure --<span class=\"keyword\">with</span>-features=huge \\</span><br><span class=\"line\">    --enable-multibyte \\</span><br><span class=\"line\">    --enable-rubyinterp=yes \\</span><br><span class=\"line\">    --enable-python3interp=yes \\</span><br><span class=\"line\">    --enable-perlinterp=yes \\</span><br><span class=\"line\">    --enable-cscope \\</span><br><span class=\"line\">    --enable-fontset \\</span><br><span class=\"line\">    --enable-largefile \\</span><br><span class=\"line\">    --enable-fail-<span class=\"keyword\">if</span>-missing \\</span><br><span class=\"line\">    --prefix=/path-to-install</span><br></pre></td></tr></tbody></table></figure>\n<p><code>--enable-fail-if-missing</code> </p>\n<h2 id=\"apt\"><span class=\"post-title-index\">3.1. </span>apt</h2>\n<p>feature hug+clientserver<br>\nsrc/auto/cofig.log<br>\n<code>sudo apt-get install vim-gtk</code>+clientservervim</p>\n</body></html>","site":{"data":{}},"excerpt":"<html><head></head><body></body></html>","more":"<h1 id=\"anaconda\">anaconda</h1>\n<p>anaconda<br />\n<code>./Anaconda3-2021.11-Linux-x86_64.sh: 489: [[: Exec format error</code><br />\n./Anaconda.shsh<br />\nbash Anaconda.sh.</p>\n<h1 id=\"linux\">Linux</h1>\n<h2 id=\"\"></h2>\n<p>apt install<br />\n<code>dpkg -L xxx</code></p>\n<h2 id=\"\"></h2>\n<p>,log<br />\nlog</p>\n<h2 id=\"linux-\">linux </h2>\n<p><code>LIBRARY_PATH</code><br />\n<code>LD_LIBRARY_PATH</code></p>\n<h2 id=\"\"></h2>\n<p>okular: error while loading shared libraries: libQt5Core.so.5: cannot open shared object file: No such file or directory<br />\nstrip<br />\nsudo strip --remove-section=.note.ABI-tag /usr/lib/x86_64-linux-gnu/libQt5Core.so.5</p>\n<h1 id=\"vim\">vim</h1>\n<p>aptvimvimlatex+clietservice<br />\napt-cache search libc-dev<br />\nln -s libXtst.so.6 libXtst.so<br />\n<br />\nhttps://www.jianshu.com/p/aa5ea81bbc72<br />\nhttps://toutiao.io/posts/runvgs/preview<br />\nconfig </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./configure --<span class=\"keyword\">with</span>-features=huge \\</span><br><span class=\"line\">    --enable-multibyte \\</span><br><span class=\"line\">    --enable-rubyinterp=yes \\</span><br><span class=\"line\">    --enable-python3interp=yes \\</span><br><span class=\"line\">    --enable-perlinterp=yes \\</span><br><span class=\"line\">    --enable-cscope \\</span><br><span class=\"line\">    --enable-fontset \\</span><br><span class=\"line\">    --enable-largefile \\</span><br><span class=\"line\">    --enable-fail-<span class=\"keyword\">if</span>-missing \\</span><br><span class=\"line\">    --prefix=/path-to-install</span><br></pre></td></tr></table></figure>\n<p><code>--enable-fail-if-missing</code> </p>\n<h2 id=\"apt\">apt</h2>\n<p>feature hug+clientserver<br />\nsrc/auto/cofig.log<br />\n<code>sudo apt-get install vim-gtk</code>+clientservervim</p>\n"},{"title":"Experiments","top":false,"cover":false,"toc":true,"mathjax":true,"date":"2021-12-17T02:00:56.000Z","password":null,"summary":null,"description":"diy","_content":"\n\n\n# Deep Learning\n\n## Feature Map Multiplication\n\n### dataset: Caltech101\n\n[source code](https://github.com/xyegithub/Featrue-map-multiplication)\n\n3\n\n/media/new_2t/yexiang/image_classification/multiply/from_#1/ffmnst/Caltech101\n\n#### bnsig\n\n|                                                          | accuracy |\n| ------------------------------------------------------------ | -------- |\n| `out = self.bn(out) + self.shortcut(x) `                     | 87.62    |\n| `out = (out.sigmoid() + 1)* self.bn_s(self.shortcut(x))`     | 78.23    |\n| `out = (self.bn(out).sigmoid() + 1) * self.bn_s(self.shortcut(x))` | 78.69    |\n| `(self.bn(out).sigmoid() + 0.5) * self.bn_s(self.shortcut(x)) ` | 78.57    |\n| `self.bn_s.bias.data[:]=1`<br>`out = (self.bn(out).sigmoid() + 1) * self.bn_s(self.shortcut(x))    ` | 82.26    |\n| `self.bn_s.bias.data[:]=1`  <br>`out = (self.bn(out).sigmoid() + 0.5) * self.bn_s(self.shortcut(x)) ` | 84.10    |\n| ` self.bn.bias.data[:]=0`<br>`self.bn_s.bias.data[:]=1`<br>` out = (self.bn(out).sigmoid() + 0.5) * self.bn_s(self.shortcut(x))             ` | 84.85    |\n| `self.bn.bias.data[:]=0`<br>`self.bn_s.bias.data[:]=1`<br>`out = (self.bn(out).sigmoid()) * self.bn_s(self.shortcut(x))` | 85.83    |\n| `self.bn.bias.data[:]=0`<br>`self.bn.weight.data[:]=1`<br>`self.bn_s.bias.data[:]=1`<br>`out = (self.bn(out).sigmoid()) * self.bn_s(self.shortcut(x))` | 81.57    |\n\n1. shortcut1\n2. Ressigmoid0.51sigmoidsigmoid0sigmoid0.5\n3. Ressigmoidoutbn0bn\n\n#### sig, bn\n\n|                                                          | accuracy |\n| ------------------------------------------------------------ | -------- |\n| `out = self.bn(out) + self.shortcut(x) `                     | 87.62    |\n| `out = (self.shortcut(x).sigmoid() + 1)* self.bn_s(out)`     | 83.93    |\n| `out = (self.bn(self.shortcut(x)).sigmoid() + 1) * self.bn_s(out)` | 85.54    |\n| `(self.bn(self.shortcut(x)).sigmoid() + 0.5) * self.bn_s(out) ` | 85.14    |\n| `self.bn_s.bias.data[:]=1`<br>`out = (self.bn(self.shortcut(x)).sigmoid() + 1) * self.bn_s(out)    ` | 85.43    |\n| `self.bn_s.bias.data[:]=1`  <br>`out = (self.bn(self.shortcut(x)).sigmoid() + 0.5) * self.bn_s(out) ` | 87.44    |\n| ` self.bn.bias.data[:]=0`<br>`self.bn_s.bias.data[:]=1`<br>` out = (self.bn(self.shortcut(x)).sigmoid() + 0.5) * self.bn_s(out)             ` | 84.39    |\n| `self.bn.bias.data[:]=0`<br>`self.bn_s.bias.data[:]=1`<br>`out = (self.bn(self.shortcut(x)).sigmoid()) * self.bn_s(out)` | 84.39    |\n| `self.bn.bias.data[:]=0`<br>`self.bn.weight.data[:]=1`<br>`self.bn_s.bias.data[:]=1`<br>`out = (self.bn(self.shortcut(x)).sigmoid()) * self.bn_s(out)` | 84.91    |\n\n**sig, bnbn, sigsigshortcutsigmoidshortcut**\n\n#### bn, bn\n\n|                                                          | Accuracy |\n| ------------------------------------------------------------ | -------- |\n| `out = self.bn(out)* self.bn_s(self.shortcut(x))`            | 77.13    |\n| `self.bn_s.bias.data[:]=1`<br>`out = self.bn(out)* self.bn_s(self.shortcut(x)) ` | 75.75    |\n| `self.bn.bias.data[:] = 1`<br>`out = self.bn(out)* self.bn_s(self.shortcut(x))` | 79.55    |\n| `self.bn.bias.data[:]=1`<br>`self.bn_s.bias.data[:]=1`<br>`out = self.bn(out)* self.bn_s(self.shortcut(x))` | 81.39    |\n| `self.bn.bias.data[:]=1`<br>`self.bn.weight.data[:]=1`<br>`self.bn_s.bias.data[:]=1`<br>`out = self.bn(out)* self.bn_s(self.shortcut(x))` | 81.05    |\n| `self.bn.bias.data[:]=1`<br>`self.bn_s.bias.data[:]=1`<br>`self.bn_s.weight.data[:]=1`<br>`out = self.bn(out)* self.bn_s(self.shortcut(x))` | 80.24    |\n| `self.bn.bias.data[:]=1`<br>`self.bn.weight.data[:]=1`<br>`self.bn_s.bias.data[:]=1`<br>`self.bn_s.weight.data[:]=1`<br>`out = self.bn(out)* self.bn_s(self.shortcut(x))` | 81.22    |\n\n1bn\n\n#### sig, sig\n\n|                                                          | Accuracy |\n| ------------------------------------------------------------ | -------- |\n| `out = (self.shortcut(x).sigmoid()) * out.sigmoid())`        | 79.44    |\n| `out = (self.shortcut(x).sigmoid()) * (out.sigmoid() + 0.5)` | 70.05    |\n| `out = (self.shortcut(x).sigmoid() + 0.5) * (out.sigmoid())` | 76.61    |\n| `out = (self.shortcut(x).sigmoid()) * (out.sigmoid() + 1)`   | 72.64    |\n| `out = (self.shortcut(x).sigmoid() + 1) * (out.sigmoid())`   | 71.77    |\n\nbnsigmoidbn\n\n|                                                          | Accuracy |\n| ------------------------------------------------------------ | -------- |\n| `out = (self.shortcut(x).sigmoid()) * self.bn(out).sigmoid()` | 86.69    |\n| `out = (self.bn_s(self.shortcut(x)).sigmoid()) * out.sigmoid()` | 79.90    |\n| `self.bn_s.bias.data[:]=0`<br>`out = (self.bn_s(self.shortcut(x)).sigmoid()) * out.sigmoid()` | 76.32    |\n| `self.bn.bias.data[:]=0`<br>`out = (self.shortcut(x).sigmoid()) * self.bn(out).sigmoid()` | 83.99    |\n| `self.bn.bias.data[:]=0 `<br>` out = (self.shortcut(x).sigmoid()) * (self.bn(out).sigmoid() + 0.5)` | 86.52    |\n| `self.bn.bias.data[:]=0 `<br>`out = (self.shortcut(x).sigmoid()) * (self.bn(out).sigmoid() + 1) ` | 82.83    |\n| `self.bn.weight.data[:]=1`<br>`out = (self.shortcut(x).sigmoid()) * self.bn(out).sigmoid()` | 78.74    |\n| `self.bn_s.bias.data[:]=0`<br>`out = (self.bn_s(self.shortcut(x)).sigmoid() + 0.5) * out.sigmoid()` | 73.39    |\n\nbn\n\n|                                                          | Accuracy |\n| ------------------------------------------------------------ | -------- |\n| ` out = (self.bn_s(self.shortcut(x)).sigmoid()) * self.bn(out).sigmoid()` | 86.23    |\n| `self.bn.bias.data[:]=0`<br>`out = (self.bn_s(self.shortcut(x)).sigmoid()) * self.bn(out).sigmoid()` | 86.41    |\n| `self.bn.bias.data[:]=0`<br>`out = (self.bn_s(self.shortcut(x)).sigmoid()) * (self.bn(out).sigmoid() + 1)` | 85.83    |\n| `self.bn.bias.data[:]=0`<br>`out = (self.bn_s(self.shortcut(x)).sigmoid()) * (self.bn(out).sigmoid() + 0.5)` | 86.23    |\n| `self.bn.bias.data[:]=0`<br>`self.bn_s.bias.data[:]=0`<br/>`out = (self.bn_s(self.shortcut(x)).sigmoid()) * self.bn(out).sigmoid()` | 86.52    |\n| `self.bn.bias.data[:]=0`<br/>`self.bn_s.bias.data[:]=0`<br/>`self.bn_s.weight.data[:]=1`<br>`out = (self.bn_s(self.shortcut(x)).sigmoid()) * self.bn(out).sigmoid()` | 82.49    |\n| `self.bn.bias.data[:]=0`<br/>`self.bn.weight.data[:]=1`<br/>`self.bn_s.bias.data[:]=0`<br>`out = (self.bn_s(self.shortcut(x)).sigmoid()) * self.bn(out).sigmoid()` | 83.47    |\n| `self.bn.bias.data[:]=0`<br>`self.bn_s.bias.data[:]=0`<br>`out = (self.bn_s(self.shortcut(x)).sigmoid()) * (self.bn(out).sigmoid() + 0.5)` | 84.91    |\n| `self.bn.bias.data[:]=0`<br/>`self.bn_s.bias.data[:]=0`<br/>`out = (self.bn_s(self.shortcut(x)).sigmoid() + 0.5) * self.bn(out).sigmoid())` | 86.92    |\n| `self.bn.bias.data[:]=0`<br/>`self.bn_s.bias.data[:]=0`<br/>`out = (self.bn_s(self.shortcut(x)).sigmoid() + 1) * self.bn(out).sigmoid())` | 86.75    |\n| `self.bn.bias.data[:]=0`<br/>`self.bn_s.bias.data[:]=0`<br/>`out = (self.bn_s(self.shortcut(x)).sigmoid() + 0.5) * (self.bn(out).sigmoid() + 0.5)` | 84.79    |\n| `self.bn.weight.data[:]=1`<br>`out = (self.bn_s(self.shortcut(x)).sigmoid()) * self.bn(out).sigmoid()` | 81.91    |\n| `self.bn_s.weight.data[:]=1`<br>`out = (self.bn_s(self.shortcut(x)).sigmoid()) * self.bn(out).sigmoid()` | 80.93    |\n\n#### Resdual \n\n|                                                          | Accuracy |\n| ------------------------------------------------------------ | -------- |\n| `out_1 = F.relu(self.bn2(out_1))`<br>`out *= self.adap(out_1)` | 86.06    |\n| `out_1 = F.relu(self.bn2(out_1))`<br>`out *= self.adap(out_1).sigmoid()` | 87.33    |\n| `out_1 = self.bn2(out_1)`<br>`out = self.conv2(out_1)`<br>`out *= self.adap(out_1).sigmoid()` | 85.71    |\n| `out_1 = self.bn2(out_1)`<br/>`out = self.conv2(out_1.relu())`<br/>`out *= self.adap(out_1).sigmoid()` | 87.85    |\n| `self.bn2.bias.data[:]=0`<br>`out_1 = self.bn2(out_1)`<br>`out = self.conv2(out_1)`<br>`out *= self.adap(out_1).sigmoid()` | 86.64    |\n| `self.bn2.bias.data[:]=0`<br/>`out_1 = self.bn2(out_1)`<br/>`out = self.conv2(out_1.relu())`<br/>`out *= self.adap(out_1).sigmoid()` | 81.51    |\n| `self.bn2.bias.data[:]=0`<br/>`out_1 = self.bn2(out_1)`<br/>`out = self.conv2(out_1.sigmoid())`<br/>`out *= self.adap(out_1).sigmoid()` | 82.66    |\n| `out_1 = F.relu(self.bn2(out_1))`<br>`out = self.conv2(out_1).sigmoid()`<br>`out = self.adap(out_1) * out` | 84.22    |\n|                                                              |          |\n\n#### NAM\n\n1. sigmoid\n2. sigmoidbn\n3. 1\n\n\n$$\n\\begin{align}\natt &= norm(x) \\\\\natt &= att \\times \\gamma + \\delta \\\\\natt &= att \\times \\frac\\gamma{sum(\\gamma)} \\\\\nout &= att.sigmoid() \\times x\n\\end{align}\n$$\n\n|                                  | Accuracy |\n| ------------------------------------ | -------- |\n| `out = self.nam(x) + self.bn_s(out)` | 86.41    |\n\n","source":"_posts/Experiments.md","raw":"---\ntitle: Experiments\ntop: false\ncover: false\ntoc: true\nmathjax: true\ndate: 2021-12-17 10:00:56\npassword:\nsummary:\ndescription: diy\ncategories:\n- Experiments\ntags:\n- Personal Thought\n- Experiments\n- private\n---\n\n\n\n# Deep Learning\n\n## Feature Map Multiplication\n\n### dataset: Caltech101\n\n[source code](https://github.com/xyegithub/Featrue-map-multiplication)\n\n3\n\n/media/new_2t/yexiang/image_classification/multiply/from_#1/ffmnst/Caltech101\n\n#### bnsig\n\n|                                                          | accuracy |\n| ------------------------------------------------------------ | -------- |\n| `out = self.bn(out) + self.shortcut(x) `                     | 87.62    |\n| `out = (out.sigmoid() + 1)* self.bn_s(self.shortcut(x))`     | 78.23    |\n| `out = (self.bn(out).sigmoid() + 1) * self.bn_s(self.shortcut(x))` | 78.69    |\n| `(self.bn(out).sigmoid() + 0.5) * self.bn_s(self.shortcut(x)) ` | 78.57    |\n| `self.bn_s.bias.data[:]=1`<br>`out = (self.bn(out).sigmoid() + 1) * self.bn_s(self.shortcut(x))    ` | 82.26    |\n| `self.bn_s.bias.data[:]=1`  <br>`out = (self.bn(out).sigmoid() + 0.5) * self.bn_s(self.shortcut(x)) ` | 84.10    |\n| ` self.bn.bias.data[:]=0`<br>`self.bn_s.bias.data[:]=1`<br>` out = (self.bn(out).sigmoid() + 0.5) * self.bn_s(self.shortcut(x))             ` | 84.85    |\n| `self.bn.bias.data[:]=0`<br>`self.bn_s.bias.data[:]=1`<br>`out = (self.bn(out).sigmoid()) * self.bn_s(self.shortcut(x))` | 85.83    |\n| `self.bn.bias.data[:]=0`<br>`self.bn.weight.data[:]=1`<br>`self.bn_s.bias.data[:]=1`<br>`out = (self.bn(out).sigmoid()) * self.bn_s(self.shortcut(x))` | 81.57    |\n\n1. shortcut1\n2. Ressigmoid0.51sigmoidsigmoid0sigmoid0.5\n3. Ressigmoidoutbn0bn\n\n#### sig, bn\n\n|                                                          | accuracy |\n| ------------------------------------------------------------ | -------- |\n| `out = self.bn(out) + self.shortcut(x) `                     | 87.62    |\n| `out = (self.shortcut(x).sigmoid() + 1)* self.bn_s(out)`     | 83.93    |\n| `out = (self.bn(self.shortcut(x)).sigmoid() + 1) * self.bn_s(out)` | 85.54    |\n| `(self.bn(self.shortcut(x)).sigmoid() + 0.5) * self.bn_s(out) ` | 85.14    |\n| `self.bn_s.bias.data[:]=1`<br>`out = (self.bn(self.shortcut(x)).sigmoid() + 1) * self.bn_s(out)    ` | 85.43    |\n| `self.bn_s.bias.data[:]=1`  <br>`out = (self.bn(self.shortcut(x)).sigmoid() + 0.5) * self.bn_s(out) ` | 87.44    |\n| ` self.bn.bias.data[:]=0`<br>`self.bn_s.bias.data[:]=1`<br>` out = (self.bn(self.shortcut(x)).sigmoid() + 0.5) * self.bn_s(out)             ` | 84.39    |\n| `self.bn.bias.data[:]=0`<br>`self.bn_s.bias.data[:]=1`<br>`out = (self.bn(self.shortcut(x)).sigmoid()) * self.bn_s(out)` | 84.39    |\n| `self.bn.bias.data[:]=0`<br>`self.bn.weight.data[:]=1`<br>`self.bn_s.bias.data[:]=1`<br>`out = (self.bn(self.shortcut(x)).sigmoid()) * self.bn_s(out)` | 84.91    |\n\n**sig, bnbn, sigsigshortcutsigmoidshortcut**\n\n#### bn, bn\n\n|                                                          | Accuracy |\n| ------------------------------------------------------------ | -------- |\n| `out = self.bn(out)* self.bn_s(self.shortcut(x))`            | 77.13    |\n| `self.bn_s.bias.data[:]=1`<br>`out = self.bn(out)* self.bn_s(self.shortcut(x)) ` | 75.75    |\n| `self.bn.bias.data[:] = 1`<br>`out = self.bn(out)* self.bn_s(self.shortcut(x))` | 79.55    |\n| `self.bn.bias.data[:]=1`<br>`self.bn_s.bias.data[:]=1`<br>`out = self.bn(out)* self.bn_s(self.shortcut(x))` | 81.39    |\n| `self.bn.bias.data[:]=1`<br>`self.bn.weight.data[:]=1`<br>`self.bn_s.bias.data[:]=1`<br>`out = self.bn(out)* self.bn_s(self.shortcut(x))` | 81.05    |\n| `self.bn.bias.data[:]=1`<br>`self.bn_s.bias.data[:]=1`<br>`self.bn_s.weight.data[:]=1`<br>`out = self.bn(out)* self.bn_s(self.shortcut(x))` | 80.24    |\n| `self.bn.bias.data[:]=1`<br>`self.bn.weight.data[:]=1`<br>`self.bn_s.bias.data[:]=1`<br>`self.bn_s.weight.data[:]=1`<br>`out = self.bn(out)* self.bn_s(self.shortcut(x))` | 81.22    |\n\n1bn\n\n#### sig, sig\n\n|                                                          | Accuracy |\n| ------------------------------------------------------------ | -------- |\n| `out = (self.shortcut(x).sigmoid()) * out.sigmoid())`        | 79.44    |\n| `out = (self.shortcut(x).sigmoid()) * (out.sigmoid() + 0.5)` | 70.05    |\n| `out = (self.shortcut(x).sigmoid() + 0.5) * (out.sigmoid())` | 76.61    |\n| `out = (self.shortcut(x).sigmoid()) * (out.sigmoid() + 1)`   | 72.64    |\n| `out = (self.shortcut(x).sigmoid() + 1) * (out.sigmoid())`   | 71.77    |\n\nbnsigmoidbn\n\n|                                                          | Accuracy |\n| ------------------------------------------------------------ | -------- |\n| `out = (self.shortcut(x).sigmoid()) * self.bn(out).sigmoid()` | 86.69    |\n| `out = (self.bn_s(self.shortcut(x)).sigmoid()) * out.sigmoid()` | 79.90    |\n| `self.bn_s.bias.data[:]=0`<br>`out = (self.bn_s(self.shortcut(x)).sigmoid()) * out.sigmoid()` | 76.32    |\n| `self.bn.bias.data[:]=0`<br>`out = (self.shortcut(x).sigmoid()) * self.bn(out).sigmoid()` | 83.99    |\n| `self.bn.bias.data[:]=0 `<br>` out = (self.shortcut(x).sigmoid()) * (self.bn(out).sigmoid() + 0.5)` | 86.52    |\n| `self.bn.bias.data[:]=0 `<br>`out = (self.shortcut(x).sigmoid()) * (self.bn(out).sigmoid() + 1) ` | 82.83    |\n| `self.bn.weight.data[:]=1`<br>`out = (self.shortcut(x).sigmoid()) * self.bn(out).sigmoid()` | 78.74    |\n| `self.bn_s.bias.data[:]=0`<br>`out = (self.bn_s(self.shortcut(x)).sigmoid() + 0.5) * out.sigmoid()` | 73.39    |\n\nbn\n\n|                                                          | Accuracy |\n| ------------------------------------------------------------ | -------- |\n| ` out = (self.bn_s(self.shortcut(x)).sigmoid()) * self.bn(out).sigmoid()` | 86.23    |\n| `self.bn.bias.data[:]=0`<br>`out = (self.bn_s(self.shortcut(x)).sigmoid()) * self.bn(out).sigmoid()` | 86.41    |\n| `self.bn.bias.data[:]=0`<br>`out = (self.bn_s(self.shortcut(x)).sigmoid()) * (self.bn(out).sigmoid() + 1)` | 85.83    |\n| `self.bn.bias.data[:]=0`<br>`out = (self.bn_s(self.shortcut(x)).sigmoid()) * (self.bn(out).sigmoid() + 0.5)` | 86.23    |\n| `self.bn.bias.data[:]=0`<br>`self.bn_s.bias.data[:]=0`<br/>`out = (self.bn_s(self.shortcut(x)).sigmoid()) * self.bn(out).sigmoid()` | 86.52    |\n| `self.bn.bias.data[:]=0`<br/>`self.bn_s.bias.data[:]=0`<br/>`self.bn_s.weight.data[:]=1`<br>`out = (self.bn_s(self.shortcut(x)).sigmoid()) * self.bn(out).sigmoid()` | 82.49    |\n| `self.bn.bias.data[:]=0`<br/>`self.bn.weight.data[:]=1`<br/>`self.bn_s.bias.data[:]=0`<br>`out = (self.bn_s(self.shortcut(x)).sigmoid()) * self.bn(out).sigmoid()` | 83.47    |\n| `self.bn.bias.data[:]=0`<br>`self.bn_s.bias.data[:]=0`<br>`out = (self.bn_s(self.shortcut(x)).sigmoid()) * (self.bn(out).sigmoid() + 0.5)` | 84.91    |\n| `self.bn.bias.data[:]=0`<br/>`self.bn_s.bias.data[:]=0`<br/>`out = (self.bn_s(self.shortcut(x)).sigmoid() + 0.5) * self.bn(out).sigmoid())` | 86.92    |\n| `self.bn.bias.data[:]=0`<br/>`self.bn_s.bias.data[:]=0`<br/>`out = (self.bn_s(self.shortcut(x)).sigmoid() + 1) * self.bn(out).sigmoid())` | 86.75    |\n| `self.bn.bias.data[:]=0`<br/>`self.bn_s.bias.data[:]=0`<br/>`out = (self.bn_s(self.shortcut(x)).sigmoid() + 0.5) * (self.bn(out).sigmoid() + 0.5)` | 84.79    |\n| `self.bn.weight.data[:]=1`<br>`out = (self.bn_s(self.shortcut(x)).sigmoid()) * self.bn(out).sigmoid()` | 81.91    |\n| `self.bn_s.weight.data[:]=1`<br>`out = (self.bn_s(self.shortcut(x)).sigmoid()) * self.bn(out).sigmoid()` | 80.93    |\n\n#### Resdual \n\n|                                                          | Accuracy |\n| ------------------------------------------------------------ | -------- |\n| `out_1 = F.relu(self.bn2(out_1))`<br>`out *= self.adap(out_1)` | 86.06    |\n| `out_1 = F.relu(self.bn2(out_1))`<br>`out *= self.adap(out_1).sigmoid()` | 87.33    |\n| `out_1 = self.bn2(out_1)`<br>`out = self.conv2(out_1)`<br>`out *= self.adap(out_1).sigmoid()` | 85.71    |\n| `out_1 = self.bn2(out_1)`<br/>`out = self.conv2(out_1.relu())`<br/>`out *= self.adap(out_1).sigmoid()` | 87.85    |\n| `self.bn2.bias.data[:]=0`<br>`out_1 = self.bn2(out_1)`<br>`out = self.conv2(out_1)`<br>`out *= self.adap(out_1).sigmoid()` | 86.64    |\n| `self.bn2.bias.data[:]=0`<br/>`out_1 = self.bn2(out_1)`<br/>`out = self.conv2(out_1.relu())`<br/>`out *= self.adap(out_1).sigmoid()` | 81.51    |\n| `self.bn2.bias.data[:]=0`<br/>`out_1 = self.bn2(out_1)`<br/>`out = self.conv2(out_1.sigmoid())`<br/>`out *= self.adap(out_1).sigmoid()` | 82.66    |\n| `out_1 = F.relu(self.bn2(out_1))`<br>`out = self.conv2(out_1).sigmoid()`<br>`out = self.adap(out_1) * out` | 84.22    |\n|                                                              |          |\n\n#### NAM\n\n1. sigmoid\n2. sigmoidbn\n3. 1\n\n\n$$\n\\begin{align}\natt &= norm(x) \\\\\natt &= att \\times \\gamma + \\delta \\\\\natt &= att \\times \\frac\\gamma{sum(\\gamma)} \\\\\nout &= att.sigmoid() \\times x\n\\end{align}\n$$\n\n|                                  | Accuracy |\n| ------------------------------------ | -------- |\n| `out = self.nam(x) + self.bn_s(out)` | 86.41    |\n\n","slug":"Experiments","published":1,"updated":"2022-01-17T09:43:07.972Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckz3w5stu00090cvk1v727gfc","content":"<div class=\"hbe hbe-container\" id=\"hexo-blog-encrypt\" data-wpm=\"Oh, this is an invalid password. Check and try again, please.\" data-whm=\"OOPS, these decrypted content may changed, but you can still have a look.\">\n  <script id=\"hbeData\" type=\"hbeData\" data-hmacdigest=\"1a2a06f76c5e35a7b02c756a4c8bf0208b37fdbf676c3d1c85ab07dc3d55a0d1\">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</script>\n  <div class=\"hbe hbe-content\">\n    <div class=\"hbe hbe-input hbe-input-default\">\n      <input class=\"hbe hbe-input-field hbe-input-field-default\" type=\"password\" id=\"hbePass\">\n      <label class=\"hbe hbe-input-label hbe-input-label-default\" for=\"hbePass\">\n        <span class=\"hbe hbe-input-label-content hbe-input-label-content-default\">Hey, password is required here.</span>\n      </label>\n    </div>\n  </div>\n</div>\n<script data-pjax src=\"/lib/hbe.js\"></script><link href=\"/css/hbe.style.css\" rel=\"stylesheet\" type=\"text/css\">","site":{"data":{}},"excerpt":"","more":"","origin":"<html><head></head><body><h1 id=\"deep-learning\"><span class=\"post-title-index\">1. </span>Deep Learning</h1>\n<h2 id=\"feature-map-multiplication\"><span class=\"post-title-index\">1.1. </span>Feature Map Multiplication</h2>\n<h3 id=\"dataset-caltech101\"><span class=\"post-title-index\">1.1.1. </span>dataset: Caltech101</h3>\n<p><a href=\"https://github.com/xyegithub/Featrue-map-multiplication\">source code</a></p>\n<p>3</p>\n<p>/media/new_2t/yexiang/image_classification/multiply/from_#1/ffmnst/Caltech101</p>\n<h4 id=\"bnsig\"><span class=\"post-title-index\">1.1.1.1. </span>bnsig</h4>\n<table>\n<colgroup>\n<col style=\"width: 88%\">\n<col style=\"width: 11%\">\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th></th>\n<th>accuracy</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td><code>out = self.bn(out) + self.shortcut(x)</code></td>\n<td>87.62</td>\n</tr>\n<tr class=\"even\">\n<td><code>out = (out.sigmoid() + 1)* self.bn_s(self.shortcut(x))</code></td>\n<td>78.23</td>\n</tr>\n<tr class=\"odd\">\n<td><code>out = (self.bn(out).sigmoid() + 1) * self.bn_s(self.shortcut(x))</code></td>\n<td>78.69</td>\n</tr>\n<tr class=\"even\">\n<td><code>(self.bn(out).sigmoid() + 0.5) * self.bn_s(self.shortcut(x))</code></td>\n<td>78.57</td>\n</tr>\n<tr class=\"odd\">\n<td><code>self.bn_s.bias.data[:]=1</code><br><code>out = (self.bn(out).sigmoid() + 1) * self.bn_s(self.shortcut(x))</code></td>\n<td>82.26</td>\n</tr>\n<tr class=\"even\">\n<td><code>self.bn_s.bias.data[:]=1</code> <br><code>out = (self.bn(out).sigmoid() + 0.5) * self.bn_s(self.shortcut(x))</code></td>\n<td>84.10</td>\n</tr>\n<tr class=\"odd\">\n<td><code>self.bn.bias.data[:]=0</code><br><code>self.bn_s.bias.data[:]=1</code><br><code>out = (self.bn(out).sigmoid() + 0.5) * self.bn_s(self.shortcut(x))</code></td>\n<td>84.85</td>\n</tr>\n<tr class=\"even\">\n<td><code>self.bn.bias.data[:]=0</code><br><code>self.bn_s.bias.data[:]=1</code><br><code>out = (self.bn(out).sigmoid()) * self.bn_s(self.shortcut(x))</code></td>\n<td>85.83</td>\n</tr>\n<tr class=\"odd\">\n<td><code>self.bn.bias.data[:]=0</code><br><code>self.bn.weight.data[:]=1</code><br><code>self.bn_s.bias.data[:]=1</code><br><code>out = (self.bn(out).sigmoid()) * self.bn_s(self.shortcut(x))</code></td>\n<td>81.57</td>\n</tr>\n</tbody>\n</table>\n<ol type=\"1\">\n<li>shortcut1</li>\n<li>Ressigmoid0.51sigmoidsigmoid0sigmoid0.5</li>\n<li>Ressigmoidoutbn0bn</li>\n</ol>\n<h4 id=\"sig-bn\"><span class=\"post-title-index\">1.1.1.2. </span>sig, bn</h4>\n<table>\n<colgroup>\n<col style=\"width: 88%\">\n<col style=\"width: 11%\">\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th></th>\n<th>accuracy</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td><code>out = self.bn(out) + self.shortcut(x)</code></td>\n<td>87.62</td>\n</tr>\n<tr class=\"even\">\n<td><code>out = (self.shortcut(x).sigmoid() + 1)* self.bn_s(out)</code></td>\n<td>83.93</td>\n</tr>\n<tr class=\"odd\">\n<td><code>out = (self.bn(self.shortcut(x)).sigmoid() + 1) * self.bn_s(out)</code></td>\n<td>85.54</td>\n</tr>\n<tr class=\"even\">\n<td><code>(self.bn(self.shortcut(x)).sigmoid() + 0.5) * self.bn_s(out)</code></td>\n<td>85.14</td>\n</tr>\n<tr class=\"odd\">\n<td><code>self.bn_s.bias.data[:]=1</code><br><code>out = (self.bn(self.shortcut(x)).sigmoid() + 1) * self.bn_s(out)</code></td>\n<td>85.43</td>\n</tr>\n<tr class=\"even\">\n<td><code>self.bn_s.bias.data[:]=1</code> <br><code>out = (self.bn(self.shortcut(x)).sigmoid() + 0.5) * self.bn_s(out)</code></td>\n<td>87.44</td>\n</tr>\n<tr class=\"odd\">\n<td><code>self.bn.bias.data[:]=0</code><br><code>self.bn_s.bias.data[:]=1</code><br><code>out = (self.bn(self.shortcut(x)).sigmoid() + 0.5) * self.bn_s(out)</code></td>\n<td>84.39</td>\n</tr>\n<tr class=\"even\">\n<td><code>self.bn.bias.data[:]=0</code><br><code>self.bn_s.bias.data[:]=1</code><br><code>out = (self.bn(self.shortcut(x)).sigmoid()) * self.bn_s(out)</code></td>\n<td>84.39</td>\n</tr>\n<tr class=\"odd\">\n<td><code>self.bn.bias.data[:]=0</code><br><code>self.bn.weight.data[:]=1</code><br><code>self.bn_s.bias.data[:]=1</code><br><code>out = (self.bn(self.shortcut(x)).sigmoid()) * self.bn_s(out)</code></td>\n<td>84.91</td>\n</tr>\n</tbody>\n</table>\n<p><strong>sig, bnbn, sigsigshortcutsigmoidshortcut</strong></p>\n<h4 id=\"bn-bn\"><span class=\"post-title-index\">1.1.1.3. </span>bn, bn</h4>\n<table>\n<colgroup>\n<col style=\"width: 88%\">\n<col style=\"width: 11%\">\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th></th>\n<th>Accuracy</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td><code>out = self.bn(out)* self.bn_s(self.shortcut(x))</code></td>\n<td>77.13</td>\n</tr>\n<tr class=\"even\">\n<td><code>self.bn_s.bias.data[:]=1</code><br><code>out = self.bn(out)* self.bn_s(self.shortcut(x))</code></td>\n<td>75.75</td>\n</tr>\n<tr class=\"odd\">\n<td><code>self.bn.bias.data[:] = 1</code><br><code>out = self.bn(out)* self.bn_s(self.shortcut(x))</code></td>\n<td>79.55</td>\n</tr>\n<tr class=\"even\">\n<td><code>self.bn.bias.data[:]=1</code><br><code>self.bn_s.bias.data[:]=1</code><br><code>out = self.bn(out)* self.bn_s(self.shortcut(x))</code></td>\n<td>81.39</td>\n</tr>\n<tr class=\"odd\">\n<td><code>self.bn.bias.data[:]=1</code><br><code>self.bn.weight.data[:]=1</code><br><code>self.bn_s.bias.data[:]=1</code><br><code>out = self.bn(out)* self.bn_s(self.shortcut(x))</code></td>\n<td>81.05</td>\n</tr>\n<tr class=\"even\">\n<td><code>self.bn.bias.data[:]=1</code><br><code>self.bn_s.bias.data[:]=1</code><br><code>self.bn_s.weight.data[:]=1</code><br><code>out = self.bn(out)* self.bn_s(self.shortcut(x))</code></td>\n<td>80.24</td>\n</tr>\n<tr class=\"odd\">\n<td><code>self.bn.bias.data[:]=1</code><br><code>self.bn.weight.data[:]=1</code><br><code>self.bn_s.bias.data[:]=1</code><br><code>self.bn_s.weight.data[:]=1</code><br><code>out = self.bn(out)* self.bn_s(self.shortcut(x))</code></td>\n<td>81.22</td>\n</tr>\n</tbody>\n</table>\n<p>1bn</p>\n<h4 id=\"sig-sig\"><span class=\"post-title-index\">1.1.1.4. </span>sig, sig</h4>\n<table>\n<thead>\n<tr class=\"header\">\n<th></th>\n<th>Accuracy</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td><code>out = (self.shortcut(x).sigmoid()) * out.sigmoid())</code></td>\n<td>79.44</td>\n</tr>\n<tr class=\"even\">\n<td><code>out = (self.shortcut(x).sigmoid()) * (out.sigmoid() + 0.5)</code></td>\n<td>70.05</td>\n</tr>\n<tr class=\"odd\">\n<td><code>out = (self.shortcut(x).sigmoid() + 0.5) * (out.sigmoid())</code></td>\n<td>76.61</td>\n</tr>\n<tr class=\"even\">\n<td><code>out = (self.shortcut(x).sigmoid()) * (out.sigmoid() + 1)</code></td>\n<td>72.64</td>\n</tr>\n<tr class=\"odd\">\n<td><code>out = (self.shortcut(x).sigmoid() + 1) * (out.sigmoid())</code></td>\n<td>71.77</td>\n</tr>\n</tbody>\n</table>\n<p>bnsigmoidbn</p>\n<table>\n<colgroup>\n<col style=\"width: 88%\">\n<col style=\"width: 11%\">\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th></th>\n<th>Accuracy</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td><code>out = (self.shortcut(x).sigmoid()) * self.bn(out).sigmoid()</code></td>\n<td>86.69</td>\n</tr>\n<tr class=\"even\">\n<td><code>out = (self.bn_s(self.shortcut(x)).sigmoid()) * out.sigmoid()</code></td>\n<td>79.90</td>\n</tr>\n<tr class=\"odd\">\n<td><code>self.bn_s.bias.data[:]=0</code><br><code>out = (self.bn_s(self.shortcut(x)).sigmoid()) * out.sigmoid()</code></td>\n<td>76.32</td>\n</tr>\n<tr class=\"even\">\n<td><code>self.bn.bias.data[:]=0</code><br><code>out = (self.shortcut(x).sigmoid()) * self.bn(out).sigmoid()</code></td>\n<td>83.99</td>\n</tr>\n<tr class=\"odd\">\n<td><code>self.bn.bias.data[:]=0</code><br><code>out = (self.shortcut(x).sigmoid()) * (self.bn(out).sigmoid() + 0.5)</code></td>\n<td>86.52</td>\n</tr>\n<tr class=\"even\">\n<td><code>self.bn.bias.data[:]=0</code><br><code>out = (self.shortcut(x).sigmoid()) * (self.bn(out).sigmoid() + 1)</code></td>\n<td>82.83</td>\n</tr>\n<tr class=\"odd\">\n<td><code>self.bn.weight.data[:]=1</code><br><code>out = (self.shortcut(x).sigmoid()) * self.bn(out).sigmoid()</code></td>\n<td>78.74</td>\n</tr>\n<tr class=\"even\">\n<td><code>self.bn_s.bias.data[:]=0</code><br><code>out = (self.bn_s(self.shortcut(x)).sigmoid() + 0.5) * out.sigmoid()</code></td>\n<td>73.39</td>\n</tr>\n</tbody>\n</table>\n<p>bn</p>\n<table>\n<colgroup>\n<col style=\"width: 88%\">\n<col style=\"width: 11%\">\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th></th>\n<th>Accuracy</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td><code>out = (self.bn_s(self.shortcut(x)).sigmoid()) * self.bn(out).sigmoid()</code></td>\n<td>86.23</td>\n</tr>\n<tr class=\"even\">\n<td><code>self.bn.bias.data[:]=0</code><br><code>out = (self.bn_s(self.shortcut(x)).sigmoid()) * self.bn(out).sigmoid()</code></td>\n<td>86.41</td>\n</tr>\n<tr class=\"odd\">\n<td><code>self.bn.bias.data[:]=0</code><br><code>out = (self.bn_s(self.shortcut(x)).sigmoid()) * (self.bn(out).sigmoid() + 1)</code></td>\n<td>85.83</td>\n</tr>\n<tr class=\"even\">\n<td><code>self.bn.bias.data[:]=0</code><br><code>out = (self.bn_s(self.shortcut(x)).sigmoid()) * (self.bn(out).sigmoid() + 0.5)</code></td>\n<td>86.23</td>\n</tr>\n<tr class=\"odd\">\n<td><code>self.bn.bias.data[:]=0</code><br><code>self.bn_s.bias.data[:]=0</code><br><code>out = (self.bn_s(self.shortcut(x)).sigmoid()) * self.bn(out).sigmoid()</code></td>\n<td>86.52</td>\n</tr>\n<tr class=\"even\">\n<td><code>self.bn.bias.data[:]=0</code><br><code>self.bn_s.bias.data[:]=0</code><br><code>self.bn_s.weight.data[:]=1</code><br><code>out = (self.bn_s(self.shortcut(x)).sigmoid()) * self.bn(out).sigmoid()</code></td>\n<td>82.49</td>\n</tr>\n<tr class=\"odd\">\n<td><code>self.bn.bias.data[:]=0</code><br><code>self.bn.weight.data[:]=1</code><br><code>self.bn_s.bias.data[:]=0</code><br><code>out = (self.bn_s(self.shortcut(x)).sigmoid()) * self.bn(out).sigmoid()</code></td>\n<td>83.47</td>\n</tr>\n<tr class=\"even\">\n<td><code>self.bn.bias.data[:]=0</code><br><code>self.bn_s.bias.data[:]=0</code><br><code>out = (self.bn_s(self.shortcut(x)).sigmoid()) * (self.bn(out).sigmoid() + 0.5)</code></td>\n<td>84.91</td>\n</tr>\n<tr class=\"odd\">\n<td><code>self.bn.bias.data[:]=0</code><br><code>self.bn_s.bias.data[:]=0</code><br><code>out = (self.bn_s(self.shortcut(x)).sigmoid() + 0.5) * self.bn(out).sigmoid())</code></td>\n<td>86.92</td>\n</tr>\n<tr class=\"even\">\n<td><code>self.bn.bias.data[:]=0</code><br><code>self.bn_s.bias.data[:]=0</code><br><code>out = (self.bn_s(self.shortcut(x)).sigmoid() + 1) * self.bn(out).sigmoid())</code></td>\n<td>86.75</td>\n</tr>\n<tr class=\"odd\">\n<td><code>self.bn.bias.data[:]=0</code><br><code>self.bn_s.bias.data[:]=0</code><br><code>out = (self.bn_s(self.shortcut(x)).sigmoid() + 0.5) * (self.bn(out).sigmoid() + 0.5)</code></td>\n<td>84.79</td>\n</tr>\n<tr class=\"even\">\n<td><code>self.bn.weight.data[:]=1</code><br><code>out = (self.bn_s(self.shortcut(x)).sigmoid()) * self.bn(out).sigmoid()</code></td>\n<td>81.91</td>\n</tr>\n<tr class=\"odd\">\n<td><code>self.bn_s.weight.data[:]=1</code><br><code>out = (self.bn_s(self.shortcut(x)).sigmoid()) * self.bn(out).sigmoid()</code></td>\n<td>80.93</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"resdual-\"><span class=\"post-title-index\">1.1.1.5. </span>Resdual </h4>\n<table>\n<colgroup>\n<col style=\"width: 88%\">\n<col style=\"width: 11%\">\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th></th>\n<th>Accuracy</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td><code>out_1 = F.relu(self.bn2(out_1))</code><br><code>out *= self.adap(out_1)</code></td>\n<td>86.06</td>\n</tr>\n<tr class=\"even\">\n<td><code>out_1 = F.relu(self.bn2(out_1))</code><br><code>out *= self.adap(out_1).sigmoid()</code></td>\n<td>87.33</td>\n</tr>\n<tr class=\"odd\">\n<td><code>out_1 = self.bn2(out_1)</code><br><code>out = self.conv2(out_1)</code><br><code>out *= self.adap(out_1).sigmoid()</code></td>\n<td>85.71</td>\n</tr>\n<tr class=\"even\">\n<td><code>out_1 = self.bn2(out_1)</code><br><code>out = self.conv2(out_1.relu())</code><br><code>out *= self.adap(out_1).sigmoid()</code></td>\n<td>87.85</td>\n</tr>\n<tr class=\"odd\">\n<td><code>self.bn2.bias.data[:]=0</code><br><code>out_1 = self.bn2(out_1)</code><br><code>out = self.conv2(out_1)</code><br><code>out *= self.adap(out_1).sigmoid()</code></td>\n<td>86.64</td>\n</tr>\n<tr class=\"even\">\n<td><code>self.bn2.bias.data[:]=0</code><br><code>out_1 = self.bn2(out_1)</code><br><code>out = self.conv2(out_1.relu())</code><br><code>out *= self.adap(out_1).sigmoid()</code></td>\n<td>81.51</td>\n</tr>\n<tr class=\"odd\">\n<td><code>self.bn2.bias.data[:]=0</code><br><code>out_1 = self.bn2(out_1)</code><br><code>out = self.conv2(out_1.sigmoid())</code><br><code>out *= self.adap(out_1).sigmoid()</code></td>\n<td>82.66</td>\n</tr>\n<tr class=\"even\">\n<td><code>out_1 = F.relu(self.bn2(out_1))</code><br><code>out = self.conv2(out_1).sigmoid()</code><br><code>out = self.adap(out_1) * out</code></td>\n<td>84.22</td>\n</tr>\n<tr class=\"odd\">\n<td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"nam\"><span class=\"post-title-index\">1.1.1.6. </span>NAM</h4>\n<ol type=\"1\">\n<li>sigmoid</li>\n<li>sigmoidbn</li>\n<li>1</li>\n</ol>\n<p><span class=\"math display\">\\[\n\\begin{align}\natt &amp;= norm(x) \\\\\natt &amp;= att \\times \\gamma + \\delta \\\\\natt &amp;= att \\times \\frac\\gamma{sum(\\gamma)} \\\\\nout &amp;= att.sigmoid() \\times x\n\\end{align}\n\\]</span></p>\n<table>\n<thead>\n<tr class=\"header\">\n<th></th>\n<th>Accuracy</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td><code>out = self.nam(x) + self.bn_s(out)</code></td>\n<td>86.41</td>\n</tr>\n</tbody>\n</table>\n</body></html>","encrypt":true},{"title":"Foundation for Topological Data Analysis","top":false,"cover":false,"toc":true,"mathjax":true,"date":"2021-12-30T01:07:39.000Z","password":null,"summary":null,"description":"","_content":"\n# \n\n## \n\n33\n\n1. qq\n2. qq, i.e., qq-1\n3. \n\n23\n\n## \n\n\n\n## \n\n\n\n4. 34\n\n4\n\n## \n\n03\n\n\n\n3qq+1\n\nqq+1q+1\n\nqq+1\n","source":"_posts/Foundation-for-Topological-Data-Analysis.md","raw":"---\ntitle: Foundation for Topological Data Analysis\ntop: false\ncover: false\ntoc: true\nmathjax: true\ndate: 2021-12-30 09:07:39\npassword:\nsummary:\ndescription: \ncategories:\n- Topological Data Analysis\ntags:\n- Topological Data Analysis \n---\n\n# \n\n## \n\n33\n\n1. qq\n2. qq, i.e., qq-1\n3. \n\n23\n\n## \n\n\n\n## \n\n\n\n4. 34\n\n4\n\n## \n\n03\n\n\n\n3qq+1\n\nqq+1q+1\n\nqq+1\n","slug":"Foundation-for-Topological-Data-Analysis","published":1,"updated":"2022-01-17T09:43:07.975Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckz3w5su1000c0cvkctsy38ac","content":"<html><head></head><body><h1 id=\"\"><span class=\"post-title-index\">1. </span></h1>\n<h2 id=\"\"><span class=\"post-title-index\">1.1. </span></h2>\n<p>33</p>\n<ol type=\"1\">\n<li>qq</li>\n<li>qq, i.e., qq-1</li>\n<li></li>\n</ol>\n<p>23</p>\n<h2 id=\"\"><span class=\"post-title-index\">1.2. </span></h2>\n<p></p>\n<h2 id=\"\"><span class=\"post-title-index\">1.3. </span></h2>\n<p></p>\n<ol start=\"4\" type=\"1\">\n<li>34</li>\n</ol>\n<p>4</p>\n<h2 id=\"\"><span class=\"post-title-index\">1.4. </span></h2>\n<p>03</p>\n<p></p>\n<p>3qq+1</p>\n<p>qq+1q+1</p>\n<p>qq+1</p>\n</body></html>","site":{"data":{}},"excerpt":"<html><head></head><body></body></html>","more":"<h1 id=\"\"></h1>\n<h2 id=\"\"></h2>\n<p>33</p>\n<ol type=\"1\">\n<li>qq</li>\n<li>qq, i.e., qq-1</li>\n<li></li>\n</ol>\n<p>23</p>\n<h2 id=\"\"></h2>\n<p></p>\n<h2 id=\"\"></h2>\n<p></p>\n<ol start=\"4\" type=\"1\">\n<li>34</li>\n</ol>\n<p>4</p>\n<h2 id=\"\"></h2>\n<p>03</p>\n<p></p>\n<p>3qq+1</p>\n<p>qq+1q+1</p>\n<p>qq+1</p>\n"},{"title":"First Step to Reinforcement Learning","date":"2021-12-03T08:48:41.000Z","description":"(Policy Network)(Value Network)","_content":"\n\n\n# \n\n<font color=green size=3></font>\n\n<font color=green size=3></font>\n\n<font color=green size=3></font>\n\n\n\n* S\n* A\n* \n* \n* \n* /\n\n## \n\n****\n\n1\n\n2 agent\n\n<font color=deeppink> agent</font>\n\n\n\n\n\n<font color=deeppink>label</font>\n\n\n\n \n\n## \n\n\n\n## \n\n\n\n\n\n\n\n## \n\n\n\n\n\n\n\n<font color=green></font>\n\n## \n\n1. targetActionPS. targetAction\n\n2. lossloss\n\n   0.1\n\n3. 2\n\n4. ActionActionrewardreward\n\n   ActionrewardActionreward\n\n   <font color=green>Actionrewardreward</font>\n\n5. \n\n   \n\n\n\n\n\n**<font color=green>Actionadvatagereward</font>**\n\n## \n\n\n\nActionscore = 0.1 score\n\n\n\nreward\n\n**<font color=green>actionaction ActionrewardActionactionrewardActionActionAction(reward)</font>**\n\nrewardrewardActionrewardAction\n\n\n\n**<font color=green>  rewardAction</font>**\n\n**<font color=green>rewardrewardrewardAction</font>**\n\n\n\n**<font color=green></font>**\n\n# (Policy Network)(Value Network)\n\nAlphaGo \n\n<font color=green></font>ActionActionAction.\n\n\n\n**Policy-based,Value-based**Actionq\n\n\n\n<font color=deeppink>Action+Action</font>ActionActionAction\n\n<font color=green></font>\n\n<font color=green>Value BasedActionPolicy BasedActionActionPolicy BasedValue Based</font>\n\n## (Policy Network)\n\n[Policy_Network.py](https://github.com/xyegithub/myBlog/blob/main/2021/12/03/First-Step-to-RL/policy_network.py) [Policy_Network.py](policy_network.py)\n\n\n\n ```python\n score = tf.matmul(layer1,W2)\n probability = tf.nn.sigmoid(score)#Action 1\n input_y = tf.placeholder(tf.float32,[None,1], \\\\\n                          name=\"input_y\")# \n advantages = tf.placeholder(tf.float32,name=\"reward_signal\") \n # \n loglik = tf.log(input_y*(input_y - probability) + \\\\\n                 (1 - input_y)*(input_y + probability)) \n # 1\n \n # 00\n #\n loss = -tf.reduce_mean(loglik * advantages) # \n #advantages\n #advantages \n #\n \n #\n #\n \n ##  \n #Policy_Network.py ,\n #0.501\n #\n \n ## action\n # + \n \n ## n\n #actionadvantages\n \n ## \n #\n #\n ```\n\n---\n\n\n\n0.5Action\n\n### \n\n\n\n```python\n#xAction 1\ntfprob = sess.run(probability,feed_dict={observations: x})\n# \naction = 1 if np.random.uniform() < tfprob else 0\n```\n\n\n\n```python\n# \n# tfprob = sess.run(probability,feed_dict={observations: x})\n# 0.5\ntfprob = 0.5\naction = 1 if np.random.uniform() < tfprob else 0\n```\n\n\n\n\n\n actionactionaction\n\n\n\n\n\n## (Value NetworkQ-learning)\n\nQ-Learing\n\n[Value_Network.py](https://github.com/xyegithub/myBlog/blob/main/2021/12/03/First-Step-to-RL/policy_network.py)\n\nActionreward\n\n\n\n```python\n#Save the experience to our episode buffer.\nepisodeBuffer.add(np.reshape(np.array([s,a,r,s1,d]),[1,5]))\n# saActionrActionreward\n# s1Actiond  \n```\n\n1. Q(s<sub>t</sub>, a<sub>t</sub>)areward\n\n2. Q<sub>desird</sub>rewardQ<sub>desired</sub>(s<sub>t</sub>, a<sub>t</sub>) = r + $\\lambda$ Max<sub>a</sub> Q<sub>desired</sub>(s<sub>t+1</sub>, a)\n\n3.   Q<sub>desired</sub>\n\n   \n\n\n\n```python\n#Choose an action by greedily (with e chance of random action)\n# from the Q-network\nif np.random.rand(1) < e or total_steps < pre_train_steps:\n    a = np.random.randint(0,4)\nelse:\n    a = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:[s]})[0]\n# total_steps < pre_train_steps\n# total_steps >= pre_train_steps\n# e1-eAction \n# \n# epre_train_steps\n# Action\n# \n# Action\n# \n```\n\n**\"by greedily (with e chance of random action) from the Q-network\"**\n\n**<font color=green></font>**\n\n\n\n```python\nif total_steps > pre_train_steps:\n    if e > endE:\n        e -= stepDrop\n# \n# \n# \n# pass\n# endE=0.1 \n#\n```\n\n\n\n\n\n```python \nif total_steps % (update_freq) == 0:\n    trainBatch = myBuffer.sample(batch_size) #Get a random batch of experiences.\n    #Below we perform the Double-DQN update to the target Q-values\n    # ActiontrainBatch[:,3]\n    # Q(st, at) = r + $\\lambda$ Max Q(s_t+1, a)\n    # Actiont+1Action\n    Q1 = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,3])})\n    # targetreward\n    Q2 = sess.run(targetQN.Qout,feed_dict={targetQN.scalarInput:np.vstack(trainBatch[:,3])})\n    end_multiplier = -(trainBatch[:,4] - 1)\n    # Actiontargetreward\n    # reward, Max Q(s_t+1, a)\n    doubleQ = Q2[range(batch_size),Q1]\n    # Q(st, at) = r + $\\lambda$ Max Q(s_t+1, a)\n    # rewardreward\n    targetQ = trainBatch[:,2] + (y*doubleQ * end_multiplier)\n    #Update the network with our target values.\n    # \n    # \n    _ = sess.run(mainQN.updateModel, \\\n                 feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,0]),mainQN.targetQ:targetQ, mainQN.actions:trainBatch[:,1]})\n    # target\n    updateTarget(targetOps,sess) #Update the target network toward the primary network.\n```\n\n\n\n\ntarget\n\n```python \ndef updateTargetGraph(tfVars,tau):\n    total_vars = len(tfVars)\n    op_holder = [] \n    # target\n    for idx,var in enumerate(tfVars[0:total_vars//2]):\n        # idx+total_varstarget\n        # target =  * tau + 1- tau*target\n        # target\n        # target\n        # target\n        # targetActionrewardAction\n        # target\n        # target\n        # \n        op_holder.append(tfVars[idx+total_vars//2].assign\\\\\n                         ((var.value()*tau) \\\\\n                      + ((1-tau)*tfVars[idx+total_vars//2].value())))\n        return op_holder\n\n    def updateTarget(op_holder,sess):\n        for op in op_holder:\n            sess.run(op)\n\n```\n\nstate of the arttrick\n\n1. CNN\n2. Experience replayNN batchsizeN=1\n3. target**targetQ-Learing********DQNQQ**targettargettargetQ\n4. Double DQNtricktargetActionactiontargetreward\n5. Dueling DQN\n\n```python \nself.AW = tf.Variable(xavier_init([h_size//2,env.actions]))\nself.VW = tf.Variable(xavier_init([h_size//2,1]))\nself.Advantage = tf.matmul(self.streamA,self.AW)\nself.Value = tf.matmul(self.streamV,self.VW)\n\n#Then combine them together to get our final Q-values.\n# QoutActionrewardValueAdvantage\n# Valueadvantage#action\n# Dueling DQNrewardValue\n# ActionAction\n# \n#out\n# Action\n# \nself.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keep_dims=True))\n\n```\n\n## \n\nreward\n\n1. Action\n2. Actionreward\n3. \n4. ActionAction1ActionrewardAction**rewardreward**\n\n\n\n# \n\n\n\n\n\nsgd\n\n\n\n\n\n\n\n\n\nreward \n\nQ<sub>desired</sub>(s<sub>t</sub>, a<sub>t</sub>) = r + $\\lambda$ Max<sub>a</sub> Q<sub>desired</sub>(s<sub>t+1</sub>, a)\n\n\n\n\n\n```python \nif done:\n    episode_number += 1\n    epx = np.vstack(xs)\n    epy = np.vstack(ys)\n    epr = np.vstack(drs)\n    tfp = tfps\n    xs,hs,dlogps,drs,ys,tfps = [],[],[],[],[],[]\n\n    discounted_epr = discount_rewards(epr)\n    discounted_epr -= np.mean(discounted_epr)\n    discounted_epr //= np.std(discounted_epr)\n\n    tGrad = sess.run(newGrads,feed_dict={observations: epx, input_y: epy, advantages: discounted_epr})\n    for ix,grad in enumerate(tGrad):\n        gradBuffer[ix] += grad\n\n        if episode_number % batch_size == 0:\n            sess.run(updateGrads,feed_dict={W1Grad: gradBuffer[0],W1_1Grad:gradBuffer[1],W2Grad:gradBuffer[2]})\n```\n\n\n\n\n\n","source":"_posts/First-Step-to-RL.md","raw":"---\ntitle: First Step to Reinforcement Learning\ndate: 2021-12-03 16:48:41\ndescription: (Policy Network)(Value Network)\ntags:\n- Reinforcement Learning\ncategories:\n- Reinforcement Learning\n\n\n---\n\n\n\n# \n\n<font color=green size=3></font>\n\n<font color=green size=3></font>\n\n<font color=green size=3></font>\n\n\n\n* S\n* A\n* \n* \n* \n* /\n\n## \n\n****\n\n1\n\n2 agent\n\n<font color=deeppink> agent</font>\n\n\n\n\n\n<font color=deeppink>label</font>\n\n\n\n \n\n## \n\n\n\n## \n\n\n\n\n\n\n\n## \n\n\n\n\n\n\n\n<font color=green></font>\n\n## \n\n1. targetActionPS. targetAction\n\n2. lossloss\n\n   0.1\n\n3. 2\n\n4. ActionActionrewardreward\n\n   ActionrewardActionreward\n\n   <font color=green>Actionrewardreward</font>\n\n5. \n\n   \n\n\n\n\n\n**<font color=green>Actionadvatagereward</font>**\n\n## \n\n\n\nActionscore = 0.1 score\n\n\n\nreward\n\n**<font color=green>actionaction ActionrewardActionactionrewardActionActionAction(reward)</font>**\n\nrewardrewardActionrewardAction\n\n\n\n**<font color=green>  rewardAction</font>**\n\n**<font color=green>rewardrewardrewardAction</font>**\n\n\n\n**<font color=green></font>**\n\n# (Policy Network)(Value Network)\n\nAlphaGo \n\n<font color=green></font>ActionActionAction.\n\n\n\n**Policy-based,Value-based**Actionq\n\n\n\n<font color=deeppink>Action+Action</font>ActionActionAction\n\n<font color=green></font>\n\n<font color=green>Value BasedActionPolicy BasedActionActionPolicy BasedValue Based</font>\n\n## (Policy Network)\n\n[Policy_Network.py](https://github.com/xyegithub/myBlog/blob/main/2021/12/03/First-Step-to-RL/policy_network.py) [Policy_Network.py](policy_network.py)\n\n\n\n ```python\n score = tf.matmul(layer1,W2)\n probability = tf.nn.sigmoid(score)#Action 1\n input_y = tf.placeholder(tf.float32,[None,1], \\\\\n                          name=\"input_y\")# \n advantages = tf.placeholder(tf.float32,name=\"reward_signal\") \n # \n loglik = tf.log(input_y*(input_y - probability) + \\\\\n                 (1 - input_y)*(input_y + probability)) \n # 1\n \n # 00\n #\n loss = -tf.reduce_mean(loglik * advantages) # \n #advantages\n #advantages \n #\n \n #\n #\n \n ##  \n #Policy_Network.py ,\n #0.501\n #\n \n ## action\n # + \n \n ## n\n #actionadvantages\n \n ## \n #\n #\n ```\n\n---\n\n\n\n0.5Action\n\n### \n\n\n\n```python\n#xAction 1\ntfprob = sess.run(probability,feed_dict={observations: x})\n# \naction = 1 if np.random.uniform() < tfprob else 0\n```\n\n\n\n```python\n# \n# tfprob = sess.run(probability,feed_dict={observations: x})\n# 0.5\ntfprob = 0.5\naction = 1 if np.random.uniform() < tfprob else 0\n```\n\n\n\n\n\n actionactionaction\n\n\n\n\n\n## (Value NetworkQ-learning)\n\nQ-Learing\n\n[Value_Network.py](https://github.com/xyegithub/myBlog/blob/main/2021/12/03/First-Step-to-RL/policy_network.py)\n\nActionreward\n\n\n\n```python\n#Save the experience to our episode buffer.\nepisodeBuffer.add(np.reshape(np.array([s,a,r,s1,d]),[1,5]))\n# saActionrActionreward\n# s1Actiond  \n```\n\n1. Q(s<sub>t</sub>, a<sub>t</sub>)areward\n\n2. Q<sub>desird</sub>rewardQ<sub>desired</sub>(s<sub>t</sub>, a<sub>t</sub>) = r + $\\lambda$ Max<sub>a</sub> Q<sub>desired</sub>(s<sub>t+1</sub>, a)\n\n3.   Q<sub>desired</sub>\n\n   \n\n\n\n```python\n#Choose an action by greedily (with e chance of random action)\n# from the Q-network\nif np.random.rand(1) < e or total_steps < pre_train_steps:\n    a = np.random.randint(0,4)\nelse:\n    a = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:[s]})[0]\n# total_steps < pre_train_steps\n# total_steps >= pre_train_steps\n# e1-eAction \n# \n# epre_train_steps\n# Action\n# \n# Action\n# \n```\n\n**\"by greedily (with e chance of random action) from the Q-network\"**\n\n**<font color=green></font>**\n\n\n\n```python\nif total_steps > pre_train_steps:\n    if e > endE:\n        e -= stepDrop\n# \n# \n# \n# pass\n# endE=0.1 \n#\n```\n\n\n\n\n\n```python \nif total_steps % (update_freq) == 0:\n    trainBatch = myBuffer.sample(batch_size) #Get a random batch of experiences.\n    #Below we perform the Double-DQN update to the target Q-values\n    # ActiontrainBatch[:,3]\n    # Q(st, at) = r + $\\lambda$ Max Q(s_t+1, a)\n    # Actiont+1Action\n    Q1 = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,3])})\n    # targetreward\n    Q2 = sess.run(targetQN.Qout,feed_dict={targetQN.scalarInput:np.vstack(trainBatch[:,3])})\n    end_multiplier = -(trainBatch[:,4] - 1)\n    # Actiontargetreward\n    # reward, Max Q(s_t+1, a)\n    doubleQ = Q2[range(batch_size),Q1]\n    # Q(st, at) = r + $\\lambda$ Max Q(s_t+1, a)\n    # rewardreward\n    targetQ = trainBatch[:,2] + (y*doubleQ * end_multiplier)\n    #Update the network with our target values.\n    # \n    # \n    _ = sess.run(mainQN.updateModel, \\\n                 feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,0]),mainQN.targetQ:targetQ, mainQN.actions:trainBatch[:,1]})\n    # target\n    updateTarget(targetOps,sess) #Update the target network toward the primary network.\n```\n\n\n\n\ntarget\n\n```python \ndef updateTargetGraph(tfVars,tau):\n    total_vars = len(tfVars)\n    op_holder = [] \n    # target\n    for idx,var in enumerate(tfVars[0:total_vars//2]):\n        # idx+total_varstarget\n        # target =  * tau + 1- tau*target\n        # target\n        # target\n        # target\n        # targetActionrewardAction\n        # target\n        # target\n        # \n        op_holder.append(tfVars[idx+total_vars//2].assign\\\\\n                         ((var.value()*tau) \\\\\n                      + ((1-tau)*tfVars[idx+total_vars//2].value())))\n        return op_holder\n\n    def updateTarget(op_holder,sess):\n        for op in op_holder:\n            sess.run(op)\n\n```\n\nstate of the arttrick\n\n1. CNN\n2. Experience replayNN batchsizeN=1\n3. target**targetQ-Learing********DQNQQ**targettargettargetQ\n4. Double DQNtricktargetActionactiontargetreward\n5. Dueling DQN\n\n```python \nself.AW = tf.Variable(xavier_init([h_size//2,env.actions]))\nself.VW = tf.Variable(xavier_init([h_size//2,1]))\nself.Advantage = tf.matmul(self.streamA,self.AW)\nself.Value = tf.matmul(self.streamV,self.VW)\n\n#Then combine them together to get our final Q-values.\n# QoutActionrewardValueAdvantage\n# Valueadvantage#action\n# Dueling DQNrewardValue\n# ActionAction\n# \n#out\n# Action\n# \nself.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keep_dims=True))\n\n```\n\n## \n\nreward\n\n1. Action\n2. Actionreward\n3. \n4. ActionAction1ActionrewardAction**rewardreward**\n\n\n\n# \n\n\n\n\n\nsgd\n\n\n\n\n\n\n\n\n\nreward \n\nQ<sub>desired</sub>(s<sub>t</sub>, a<sub>t</sub>) = r + $\\lambda$ Max<sub>a</sub> Q<sub>desired</sub>(s<sub>t+1</sub>, a)\n\n\n\n\n\n```python \nif done:\n    episode_number += 1\n    epx = np.vstack(xs)\n    epy = np.vstack(ys)\n    epr = np.vstack(drs)\n    tfp = tfps\n    xs,hs,dlogps,drs,ys,tfps = [],[],[],[],[],[]\n\n    discounted_epr = discount_rewards(epr)\n    discounted_epr -= np.mean(discounted_epr)\n    discounted_epr //= np.std(discounted_epr)\n\n    tGrad = sess.run(newGrads,feed_dict={observations: epx, input_y: epy, advantages: discounted_epr})\n    for ix,grad in enumerate(tGrad):\n        gradBuffer[ix] += grad\n\n        if episode_number % batch_size == 0:\n            sess.run(updateGrads,feed_dict={W1Grad: gradBuffer[0],W1_1Grad:gradBuffer[1],W2Grad:gradBuffer[2]})\n```\n\n\n\n\n\n","slug":"First-Step-to-RL","published":1,"updated":"2022-01-17T09:43:07.972Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckz3w5su3000d0cvk9sww0q4g","content":"<html><head></head><body><h1 id=\"\"><span class=\"post-title-index\">1. </span></h1>\n<p><font color=\"green\" size=\"3\"></font></p>\n<p><font color=\"green\" size=\"3\"></font></p>\n<p><font color=\"green\" size=\"3\"></font></p>\n<p></p>\n<ul>\n<li>S</li>\n<li>A</li>\n<li></li>\n<li></li>\n<li></li>\n<li>/</li>\n</ul>\n<h2 id=\"\"><span class=\"post-title-index\">1.1. </span></h2>\n<p><strong></strong></p>\n<p>1</p>\n<p>2 agent</p>\n<p><font color=\"deeppink\"> agent</font></p>\n<p></p>\n<p></p>\n<p><font color=\"deeppink\">label</font></p>\n<p></p>\n<p></p>\n<h2 id=\"\"><span class=\"post-title-index\">1.2. </span></h2>\n<p></p>\n<h2 id=\"\"><span class=\"post-title-index\">1.3. </span></h2>\n<p></p>\n<p></p>\n<p></p>\n<h2 id=\"\"><span class=\"post-title-index\">1.4. </span></h2>\n<p></p>\n<p></p>\n<p></p>\n<p><font color=\"green\"></font></p>\n<h2 id=\"\"><span class=\"post-title-index\">1.5. </span></h2>\n<ol type=\"1\">\n<li><p>targetActionPS. targetAction</p></li>\n<li><p>lossloss</p>\n<p>0.1</p></li>\n<li><p>2</p></li>\n<li><p>ActionActionrewardreward</p>\n<p>ActionrewardActionreward</p>\n<p><font color=\"green\">Actionrewardreward</font></p></li>\n<li><p></p></li>\n</ol>\n<p></p>\n<p></p>\n<p><strong><font color=\"green\">Actionadvatagereward</font></strong></p>\n<h2 id=\"\"><span class=\"post-title-index\">1.6. </span></h2>\n<p></p>\n<p>Actionscore = 0.1 score</p>\n<p></p>\n<p>reward</p>\n<p><strong><font color=\"green\">actionaction ActionrewardActionactionrewardActionActionAction(reward)</font></strong></p>\n<p>rewardrewardActionrewardAction</p>\n<p></p>\n<p><strong><font color=\"green\">  rewardAction</font></strong></p>\n<p><strong><font color=\"green\">rewardrewardrewardAction</font></strong></p>\n<p></p>\n<p><strong><font color=\"green\"></font></strong></p>\n<h1 id=\"policy-networkvalue-network\"><span class=\"post-title-index\">2. </span>(Policy Network)(Value Network)</h1>\n<p>AlphaGo </p>\n<p><font color=\"green\"></font>ActionActionAction.</p>\n<p></p>\n<p><strong>Policy-based,Value-based</strong>Actionq</p>\n<p></p>\n<p><font color=\"deeppink\">Action+Action</font>ActionActionAction</p>\n<p><font color=\"green\"></font></p>\n<p><font color=\"green\">Value BasedActionPolicy BasedActionActionPolicy BasedValue Based</font></p>\n<h2 id=\"policy-network\"><span class=\"post-title-index\">2.1. </span>(Policy Network)</h2>\n<p><a href=\"https://github.com/xyegithub/myBlog/blob/main/2021/12/03/First-Step-to-RL/policy_network.py\">Policy_Network.py</a> <a href=\"policy_network.py\">Policy_Network.py</a></p>\n<p></p>\n<p></p><figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">score = tf.matmul(layer1,W2)</span><br><span class=\"line\">probability = tf.nn.sigmoid(score)<span class=\"comment\">#Action 1</span></span><br><span class=\"line\">input_y = tf.placeholder(tf.float32,[<span class=\"literal\">None</span>,<span class=\"number\">1</span>], \\\\</span><br><span class=\"line\">                         name=<span class=\"string\">\"input_y\"</span>)<span class=\"comment\"># </span></span><br><span class=\"line\">advantages = tf.placeholder(tf.float32,name=<span class=\"string\">\"reward_signal\"</span>) </span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">loglik = tf.log(input_y*(input_y - probability) + \\\\</span><br><span class=\"line\">                (<span class=\"number\">1</span> - input_y)*(input_y + probability)) </span><br><span class=\"line\"><span class=\"comment\"># 1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 00</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\">loss = -tf.reduce_mean(loglik * advantages) <span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\">#advantages</span></span><br><span class=\"line\"><span class=\"comment\">#advantages </span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">##  </span></span><br><span class=\"line\"><span class=\"comment\">#Policy_Network.py ,</span></span><br><span class=\"line\"><span class=\"comment\">#0.501</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## action</span></span><br><span class=\"line\"><span class=\"comment\"># + </span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## n</span></span><br><span class=\"line\"><span class=\"comment\">#actionadvantages</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## </span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br></pre></td></tr></tbody></table></figure><p></p>\n<p>---</p>\n<p></p>\n<p>0.5Action</p>\n<h3 id=\"\"><span class=\"post-title-index\">2.1.1. </span></h3>\n<p></p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#xAction 1</span></span><br><span class=\"line\">tfprob = sess.run(probability,feed_dict={observations: x})</span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">action = <span class=\"number\">1</span> <span class=\"keyword\">if</span> np.random.uniform() &lt; tfprob <span class=\"keyword\">else</span> <span class=\"number\">0</span></span><br></pre></td></tr></tbody></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\"># tfprob = sess.run(probability,feed_dict={observations: x})</span></span><br><span class=\"line\"><span class=\"comment\"># 0.5</span></span><br><span class=\"line\">tfprob = <span class=\"number\">0.5</span></span><br><span class=\"line\">action = <span class=\"number\">1</span> <span class=\"keyword\">if</span> np.random.uniform() &lt; tfprob <span class=\"keyword\">else</span> <span class=\"number\">0</span></span><br></pre></td></tr></tbody></table></figure>\n<p></p>\n<p></p>\n<p> actionactionaction</p>\n<p></p>\n<p></p>\n<h2 id=\"value-networkq-learning\"><span class=\"post-title-index\">2.2. </span>(Value NetworkQ-learning)</h2>\n<p>Q-Learing</p>\n<p><a href=\"https://github.com/xyegithub/myBlog/blob/main/2021/12/03/First-Step-to-RL/policy_network.py\">Value_Network.py</a></p>\n<p>Actionreward</p>\n<p></p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#Save the experience to our episode buffer.</span></span><br><span class=\"line\">episodeBuffer.add(np.reshape(np.array([s,a,r,s1,d]),[<span class=\"number\">1</span>,<span class=\"number\">5</span>]))</span><br><span class=\"line\"><span class=\"comment\"># saActionrActionreward</span></span><br><span class=\"line\"><span class=\"comment\"># s1Actiond  </span></span><br></pre></td></tr></tbody></table></figure>\n<ol type=\"1\">\n<li><p>Q(s<sub>t</sub>, a<sub>t</sub>)areward</p></li>\n<li><p>Q<sub>desird</sub>rewardQ<sub>desired</sub>(s<sub>t</sub>, a<sub>t</sub>) = r + <span class=\"math inline\">\\(\\lambda\\)</span> Max<sub>a</sub> Q<sub>desired</sub>(s<sub>t+1</sub>, a)</p></li>\n<li><p>  Q<sub>desired</sub></p></li>\n</ol>\n<p></p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#Choose an action by greedily (with e chance of random action)</span></span><br><span class=\"line\"><span class=\"comment\"># from the Q-network</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> np.random.rand(<span class=\"number\">1</span>) &lt; e <span class=\"keyword\">or</span> total_steps &lt; pre_train_steps:</span><br><span class=\"line\">    a = np.random.randint(<span class=\"number\">0</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\"><span class=\"keyword\">else</span>:</span><br><span class=\"line\">    a = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:[s]})[<span class=\"number\">0</span>]</span><br><span class=\"line\"><span class=\"comment\"># total_steps &lt; pre_train_steps</span></span><br><span class=\"line\"><span class=\"comment\"># total_steps &gt;= pre_train_steps</span></span><br><span class=\"line\"><span class=\"comment\"># e1-eAction </span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\"># epre_train_steps</span></span><br><span class=\"line\"><span class=\"comment\"># Action</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\"># Action</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br></pre></td></tr></tbody></table></figure>\n<p><strong>\"by greedily (with e chance of random action) from the Q-network\"</strong></p>\n<p><strong><font color=\"green\"></font></strong></p>\n<p></p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> total_steps &gt; pre_train_steps:</span><br><span class=\"line\">    <span class=\"keyword\">if</span> e &gt; endE:</span><br><span class=\"line\">        e -= stepDrop</span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\"># pass</span></span><br><span class=\"line\"><span class=\"comment\"># endE=0.1 </span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br></pre></td></tr></tbody></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> total_steps % (update_freq) == <span class=\"number\">0</span>:</span><br><span class=\"line\">    trainBatch = myBuffer.sample(batch_size) <span class=\"comment\">#Get a random batch of experiences.</span></span><br><span class=\"line\">    <span class=\"comment\">#Below we perform the Double-DQN update to the target Q-values</span></span><br><span class=\"line\">    <span class=\"comment\"># ActiontrainBatch[:,3]</span></span><br><span class=\"line\">    <span class=\"comment\"># Q(st, at) = r + $\\lambda$ Max Q(s_t+1, a)</span></span><br><span class=\"line\">    <span class=\"comment\"># Actiont+1Action</span></span><br><span class=\"line\">    Q1 = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,<span class=\"number\">3</span>])})</span><br><span class=\"line\">    <span class=\"comment\"># targetreward</span></span><br><span class=\"line\">    Q2 = sess.run(targetQN.Qout,feed_dict={targetQN.scalarInput:np.vstack(trainBatch[:,<span class=\"number\">3</span>])})</span><br><span class=\"line\">    end_multiplier = -(trainBatch[:,<span class=\"number\">4</span>] - <span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"comment\"># Actiontargetreward</span></span><br><span class=\"line\">    <span class=\"comment\"># reward, Max Q(s_t+1, a)</span></span><br><span class=\"line\">    doubleQ = Q2[<span class=\"built_in\">range</span>(batch_size),Q1]</span><br><span class=\"line\">    <span class=\"comment\"># Q(st, at) = r + $\\lambda$ Max Q(s_t+1, a)</span></span><br><span class=\"line\">    <span class=\"comment\"># rewardreward</span></span><br><span class=\"line\">    targetQ = trainBatch[:,<span class=\"number\">2</span>] + (y*doubleQ * end_multiplier)</span><br><span class=\"line\">    <span class=\"comment\">#Update the network with our target values.</span></span><br><span class=\"line\">    <span class=\"comment\"># </span></span><br><span class=\"line\">    <span class=\"comment\"># </span></span><br><span class=\"line\">    _ = sess.run(mainQN.updateModel, \\</span><br><span class=\"line\">                 feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,<span class=\"number\">0</span>]),mainQN.targetQ:targetQ, mainQN.actions:trainBatch[:,<span class=\"number\">1</span>]})</span><br><span class=\"line\">    <span class=\"comment\"># target</span></span><br><span class=\"line\">    updateTarget(targetOps,sess) <span class=\"comment\">#Update the target network toward the primary network.</span></span><br></pre></td></tr></tbody></table></figure>\n<p>target</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">updateTargetGraph</span>(<span class=\"params\">tfVars,tau</span>):</span></span><br><span class=\"line\">    total_vars = <span class=\"built_in\">len</span>(tfVars)</span><br><span class=\"line\">    op_holder = [] </span><br><span class=\"line\">    <span class=\"comment\"># target</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> idx,var <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(tfVars[<span class=\"number\">0</span>:total_vars//<span class=\"number\">2</span>]):</span><br><span class=\"line\">        <span class=\"comment\"># idx+total_varstarget</span></span><br><span class=\"line\">        <span class=\"comment\"># target =  * tau + 1- tau*target</span></span><br><span class=\"line\">        <span class=\"comment\"># target</span></span><br><span class=\"line\">        <span class=\"comment\"># target</span></span><br><span class=\"line\">        <span class=\"comment\"># target</span></span><br><span class=\"line\">        <span class=\"comment\"># targetActionrewardAction</span></span><br><span class=\"line\">        <span class=\"comment\"># target</span></span><br><span class=\"line\">        <span class=\"comment\"># target</span></span><br><span class=\"line\">        <span class=\"comment\"># </span></span><br><span class=\"line\">        op_holder.append(tfVars[idx+total_vars//<span class=\"number\">2</span>].assign\\\\</span><br><span class=\"line\">                         ((var.value()*tau) \\\\</span><br><span class=\"line\">                      + ((<span class=\"number\">1</span>-tau)*tfVars[idx+total_vars//<span class=\"number\">2</span>].value())))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> op_holder</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">updateTarget</span>(<span class=\"params\">op_holder,sess</span>):</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> op <span class=\"keyword\">in</span> op_holder:</span><br><span class=\"line\">            sess.run(op)</span><br><span class=\"line\"></span><br></pre></td></tr></tbody></table></figure>\n<p>state of the arttrick</p>\n<ol type=\"1\">\n<li>CNN</li>\n<li>Experience replayNN batchsizeN=1</li>\n<li>target<strong>targetQ-Learing</strong><strong></strong><strong>DQNQQ</strong>targettargettargetQ</li>\n<li>Double DQNtricktargetActionactiontargetreward</li>\n<li>Dueling DQN</li>\n</ol>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">self.AW = tf.Variable(xavier_init([h_size//<span class=\"number\">2</span>,env.actions]))</span><br><span class=\"line\">self.VW = tf.Variable(xavier_init([h_size//<span class=\"number\">2</span>,<span class=\"number\">1</span>]))</span><br><span class=\"line\">self.Advantage = tf.matmul(self.streamA,self.AW)</span><br><span class=\"line\">self.Value = tf.matmul(self.streamV,self.VW)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#Then combine them together to get our final Q-values.</span></span><br><span class=\"line\"><span class=\"comment\"># QoutActionrewardValueAdvantage</span></span><br><span class=\"line\"><span class=\"comment\"># Valueadvantage#action</span></span><br><span class=\"line\"><span class=\"comment\"># Dueling DQNrewardValue</span></span><br><span class=\"line\"><span class=\"comment\"># ActionAction</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\">#out</span></span><br><span class=\"line\"><span class=\"comment\"># Action</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">self.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=<span class=\"number\">1</span>,keep_dims=<span class=\"literal\">True</span>))</span><br><span class=\"line\"></span><br></pre></td></tr></tbody></table></figure>\n<h2 id=\"\"><span class=\"post-title-index\">2.3. </span></h2>\n<p>reward</p>\n<ol type=\"1\">\n<li>Action</li>\n<li>Actionreward</li>\n<li></li>\n<li>ActionAction1ActionrewardAction<strong>rewardreward</strong></li>\n</ol>\n<h1 id=\"\"><span class=\"post-title-index\">3. </span></h1>\n<p></p>\n<p></p>\n<p>sgd</p>\n<p></p>\n<p></p>\n<p>reward </p>\n<p>Q<sub>desired</sub>(s<sub>t</sub>, a<sub>t</sub>) = r + <span class=\"math inline\">\\(\\lambda\\)</span> Max<sub>a</sub> Q<sub>desired</sub>(s<sub>t+1</sub>, a)</p>\n<p></p>\n<p></p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> done:</span><br><span class=\"line\">    episode_number += <span class=\"number\">1</span></span><br><span class=\"line\">    epx = np.vstack(xs)</span><br><span class=\"line\">    epy = np.vstack(ys)</span><br><span class=\"line\">    epr = np.vstack(drs)</span><br><span class=\"line\">    tfp = tfps</span><br><span class=\"line\">    xs,hs,dlogps,drs,ys,tfps = [],[],[],[],[],[]</span><br><span class=\"line\"></span><br><span class=\"line\">    discounted_epr = discount_rewards(epr)</span><br><span class=\"line\">    discounted_epr -= np.mean(discounted_epr)</span><br><span class=\"line\">    discounted_epr //= np.std(discounted_epr)</span><br><span class=\"line\"></span><br><span class=\"line\">    tGrad = sess.run(newGrads,feed_dict={observations: epx, input_y: epy, advantages: discounted_epr})</span><br><span class=\"line\">    <span class=\"keyword\">for</span> ix,grad <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(tGrad):</span><br><span class=\"line\">        gradBuffer[ix] += grad</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> episode_number % batch_size == <span class=\"number\">0</span>:</span><br><span class=\"line\">            sess.run(updateGrads,feed_dict={W1Grad: gradBuffer[<span class=\"number\">0</span>],W1_1Grad:gradBuffer[<span class=\"number\">1</span>],W2Grad:gradBuffer[<span class=\"number\">2</span>]})</span><br></pre></td></tr></tbody></table></figure>\n<p></p>\n</body></html>","site":{"data":{}},"excerpt":"<html><head></head><body></body></html>","more":"<h1 id=\"\"></h1>\n<p><font color=green size=3></font></p>\n<p><font color=green size=3></font></p>\n<p><font color=green size=3></font></p>\n<p></p>\n<ul>\n<li>S</li>\n<li>A</li>\n<li></li>\n<li></li>\n<li></li>\n<li>/</li>\n</ul>\n<h2 id=\"\"></h2>\n<p><strong></strong></p>\n<p>1</p>\n<p>2 agent</p>\n<p><font color=deeppink> agent</font></p>\n<p></p>\n<p></p>\n<p><font color=deeppink>label</font></p>\n<p></p>\n<p></p>\n<h2 id=\"\"></h2>\n<p></p>\n<h2 id=\"\"></h2>\n<p></p>\n<p></p>\n<p></p>\n<h2 id=\"\"></h2>\n<p></p>\n<p></p>\n<p></p>\n<p><font color=green></font></p>\n<h2 id=\"\"></h2>\n<ol type=\"1\">\n<li><p>targetActionPS. targetAction</p></li>\n<li><p>lossloss</p>\n<p>0.1</p></li>\n<li><p>2</p></li>\n<li><p>ActionActionrewardreward</p>\n<p>ActionrewardActionreward</p>\n<p><font color=green>Actionrewardreward</font></p></li>\n<li><p></p></li>\n</ol>\n<p></p>\n<p></p>\n<p><strong><font color=green>Actionadvatagereward</font></strong></p>\n<h2 id=\"\"></h2>\n<p></p>\n<p>Actionscore = 0.1 score</p>\n<p></p>\n<p>reward</p>\n<p><strong><font color=green>actionaction ActionrewardActionactionrewardActionActionAction(reward)</font></strong></p>\n<p>rewardrewardActionrewardAction</p>\n<p></p>\n<p><strong><font color=green>  rewardAction</font></strong></p>\n<p><strong><font color=green>rewardrewardrewardAction</font></strong></p>\n<p></p>\n<p><strong><font color=green></font></strong></p>\n<h1 id=\"policy-networkvalue-network\">(Policy Network)(Value Network)</h1>\n<p>AlphaGo </p>\n<p><font color=green></font>ActionActionAction.</p>\n<p></p>\n<p><strong>Policy-based,Value-based</strong>Actionq</p>\n<p></p>\n<p><font color=deeppink>Action+Action</font>ActionActionAction</p>\n<p><font color=green></font></p>\n<p><font color=green>Value BasedActionPolicy BasedActionActionPolicy BasedValue Based</font></p>\n<h2 id=\"policy-network\">(Policy Network)</h2>\n<p><a href=\"https://github.com/xyegithub/myBlog/blob/main/2021/12/03/First-Step-to-RL/policy_network.py\">Policy_Network.py</a> <a href=\"policy_network.py\">Policy_Network.py</a></p>\n<p></p>\n<p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">score = tf.matmul(layer1,W2)</span><br><span class=\"line\">probability = tf.nn.sigmoid(score)<span class=\"comment\">#Action 1</span></span><br><span class=\"line\">input_y = tf.placeholder(tf.float32,[<span class=\"literal\">None</span>,<span class=\"number\">1</span>], \\\\</span><br><span class=\"line\">                         name=<span class=\"string\">&quot;input_y&quot;</span>)<span class=\"comment\"># </span></span><br><span class=\"line\">advantages = tf.placeholder(tf.float32,name=<span class=\"string\">&quot;reward_signal&quot;</span>) </span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">loglik = tf.log(input_y*(input_y - probability) + \\\\</span><br><span class=\"line\">                (<span class=\"number\">1</span> - input_y)*(input_y + probability)) </span><br><span class=\"line\"><span class=\"comment\"># 1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 00</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\">loss = -tf.reduce_mean(loglik * advantages) <span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\">#advantages</span></span><br><span class=\"line\"><span class=\"comment\">#advantages </span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">##  </span></span><br><span class=\"line\"><span class=\"comment\">#Policy_Network.py ,</span></span><br><span class=\"line\"><span class=\"comment\">#0.501</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## action</span></span><br><span class=\"line\"><span class=\"comment\"># + </span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## n</span></span><br><span class=\"line\"><span class=\"comment\">#actionadvantages</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## </span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br></pre></td></tr></table></figure></p>\n<p>---</p>\n<p></p>\n<p>0.5Action</p>\n<h3 id=\"\"></h3>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#xAction 1</span></span><br><span class=\"line\">tfprob = sess.run(probability,feed_dict=&#123;observations: x&#125;)</span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">action = <span class=\"number\">1</span> <span class=\"keyword\">if</span> np.random.uniform() &lt; tfprob <span class=\"keyword\">else</span> <span class=\"number\">0</span></span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\"># tfprob = sess.run(probability,feed_dict=&#123;observations: x&#125;)</span></span><br><span class=\"line\"><span class=\"comment\"># 0.5</span></span><br><span class=\"line\">tfprob = <span class=\"number\">0.5</span></span><br><span class=\"line\">action = <span class=\"number\">1</span> <span class=\"keyword\">if</span> np.random.uniform() &lt; tfprob <span class=\"keyword\">else</span> <span class=\"number\">0</span></span><br></pre></td></tr></table></figure>\n<p></p>\n<p></p>\n<p> actionactionaction</p>\n<p></p>\n<p></p>\n<h2 id=\"value-networkq-learning\">(Value NetworkQ-learning)</h2>\n<p>Q-Learing</p>\n<p><a href=\"https://github.com/xyegithub/myBlog/blob/main/2021/12/03/First-Step-to-RL/policy_network.py\">Value_Network.py</a></p>\n<p>Actionreward</p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#Save the experience to our episode buffer.</span></span><br><span class=\"line\">episodeBuffer.add(np.reshape(np.array([s,a,r,s1,d]),[<span class=\"number\">1</span>,<span class=\"number\">5</span>]))</span><br><span class=\"line\"><span class=\"comment\"># saActionrActionreward</span></span><br><span class=\"line\"><span class=\"comment\"># s1Actiond  </span></span><br></pre></td></tr></table></figure>\n<ol type=\"1\">\n<li><p>Q(s<sub>t</sub>, a<sub>t</sub>)areward</p></li>\n<li><p>Q<sub>desird</sub>rewardQ<sub>desired</sub>(s<sub>t</sub>, a<sub>t</sub>) = r + <span class=\"math inline\">\\(\\lambda\\)</span> Max<sub>a</sub> Q<sub>desired</sub>(s<sub>t+1</sub>, a)</p></li>\n<li><p>  Q<sub>desired</sub></p></li>\n</ol>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#Choose an action by greedily (with e chance of random action)</span></span><br><span class=\"line\"><span class=\"comment\"># from the Q-network</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> np.random.rand(<span class=\"number\">1</span>) &lt; e <span class=\"keyword\">or</span> total_steps &lt; pre_train_steps:</span><br><span class=\"line\">    a = np.random.randint(<span class=\"number\">0</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\"><span class=\"keyword\">else</span>:</span><br><span class=\"line\">    a = sess.run(mainQN.predict,feed_dict=&#123;mainQN.scalarInput:[s]&#125;)[<span class=\"number\">0</span>]</span><br><span class=\"line\"><span class=\"comment\"># total_steps &lt; pre_train_steps</span></span><br><span class=\"line\"><span class=\"comment\"># total_steps &gt;= pre_train_steps</span></span><br><span class=\"line\"><span class=\"comment\"># e1-eAction </span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\"># epre_train_steps</span></span><br><span class=\"line\"><span class=\"comment\"># Action</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\"># Action</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br></pre></td></tr></table></figure>\n<p><strong>\"by greedily (with e chance of random action) from the Q-network\"</strong></p>\n<p><strong><font color=green></font></strong></p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> total_steps &gt; pre_train_steps:</span><br><span class=\"line\">    <span class=\"keyword\">if</span> e &gt; endE:</span><br><span class=\"line\">        e -= stepDrop</span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\"># pass</span></span><br><span class=\"line\"><span class=\"comment\"># endE=0.1 </span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> total_steps % (update_freq) == <span class=\"number\">0</span>:</span><br><span class=\"line\">    trainBatch = myBuffer.sample(batch_size) <span class=\"comment\">#Get a random batch of experiences.</span></span><br><span class=\"line\">    <span class=\"comment\">#Below we perform the Double-DQN update to the target Q-values</span></span><br><span class=\"line\">    <span class=\"comment\"># ActiontrainBatch[:,3]</span></span><br><span class=\"line\">    <span class=\"comment\"># Q(st, at) = r + $\\lambda$ Max Q(s_t+1, a)</span></span><br><span class=\"line\">    <span class=\"comment\"># Actiont+1Action</span></span><br><span class=\"line\">    Q1 = sess.run(mainQN.predict,feed_dict=&#123;mainQN.scalarInput:np.vstack(trainBatch[:,<span class=\"number\">3</span>])&#125;)</span><br><span class=\"line\">    <span class=\"comment\"># targetreward</span></span><br><span class=\"line\">    Q2 = sess.run(targetQN.Qout,feed_dict=&#123;targetQN.scalarInput:np.vstack(trainBatch[:,<span class=\"number\">3</span>])&#125;)</span><br><span class=\"line\">    end_multiplier = -(trainBatch[:,<span class=\"number\">4</span>] - <span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"comment\"># Actiontargetreward</span></span><br><span class=\"line\">    <span class=\"comment\"># reward, Max Q(s_t+1, a)</span></span><br><span class=\"line\">    doubleQ = Q2[<span class=\"built_in\">range</span>(batch_size),Q1]</span><br><span class=\"line\">    <span class=\"comment\"># Q(st, at) = r + $\\lambda$ Max Q(s_t+1, a)</span></span><br><span class=\"line\">    <span class=\"comment\"># rewardreward</span></span><br><span class=\"line\">    targetQ = trainBatch[:,<span class=\"number\">2</span>] + (y*doubleQ * end_multiplier)</span><br><span class=\"line\">    <span class=\"comment\">#Update the network with our target values.</span></span><br><span class=\"line\">    <span class=\"comment\"># </span></span><br><span class=\"line\">    <span class=\"comment\"># </span></span><br><span class=\"line\">    _ = sess.run(mainQN.updateModel, \\</span><br><span class=\"line\">                 feed_dict=&#123;mainQN.scalarInput:np.vstack(trainBatch[:,<span class=\"number\">0</span>]),mainQN.targetQ:targetQ, mainQN.actions:trainBatch[:,<span class=\"number\">1</span>]&#125;)</span><br><span class=\"line\">    <span class=\"comment\"># target</span></span><br><span class=\"line\">    updateTarget(targetOps,sess) <span class=\"comment\">#Update the target network toward the primary network.</span></span><br></pre></td></tr></table></figure>\n<p>target</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">updateTargetGraph</span>(<span class=\"params\">tfVars,tau</span>):</span></span><br><span class=\"line\">    total_vars = <span class=\"built_in\">len</span>(tfVars)</span><br><span class=\"line\">    op_holder = [] </span><br><span class=\"line\">    <span class=\"comment\"># target</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> idx,var <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(tfVars[<span class=\"number\">0</span>:total_vars//<span class=\"number\">2</span>]):</span><br><span class=\"line\">        <span class=\"comment\"># idx+total_varstarget</span></span><br><span class=\"line\">        <span class=\"comment\"># target =  * tau + 1- tau*target</span></span><br><span class=\"line\">        <span class=\"comment\"># target</span></span><br><span class=\"line\">        <span class=\"comment\"># target</span></span><br><span class=\"line\">        <span class=\"comment\"># target</span></span><br><span class=\"line\">        <span class=\"comment\"># targetActionrewardAction</span></span><br><span class=\"line\">        <span class=\"comment\"># target</span></span><br><span class=\"line\">        <span class=\"comment\"># target</span></span><br><span class=\"line\">        <span class=\"comment\"># </span></span><br><span class=\"line\">        op_holder.append(tfVars[idx+total_vars//<span class=\"number\">2</span>].assign\\\\</span><br><span class=\"line\">                         ((var.value()*tau) \\\\</span><br><span class=\"line\">                      + ((<span class=\"number\">1</span>-tau)*tfVars[idx+total_vars//<span class=\"number\">2</span>].value())))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> op_holder</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">updateTarget</span>(<span class=\"params\">op_holder,sess</span>):</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> op <span class=\"keyword\">in</span> op_holder:</span><br><span class=\"line\">            sess.run(op)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>state of the arttrick</p>\n<ol type=\"1\">\n<li>CNN</li>\n<li>Experience replayNN batchsizeN=1</li>\n<li>target<strong>targetQ-Learing</strong><strong></strong><strong>DQNQQ</strong>targettargettargetQ</li>\n<li>Double DQNtricktargetActionactiontargetreward</li>\n<li>Dueling DQN</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">self.AW = tf.Variable(xavier_init([h_size//<span class=\"number\">2</span>,env.actions]))</span><br><span class=\"line\">self.VW = tf.Variable(xavier_init([h_size//<span class=\"number\">2</span>,<span class=\"number\">1</span>]))</span><br><span class=\"line\">self.Advantage = tf.matmul(self.streamA,self.AW)</span><br><span class=\"line\">self.Value = tf.matmul(self.streamV,self.VW)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#Then combine them together to get our final Q-values.</span></span><br><span class=\"line\"><span class=\"comment\"># QoutActionrewardValueAdvantage</span></span><br><span class=\"line\"><span class=\"comment\"># Valueadvantage#action</span></span><br><span class=\"line\"><span class=\"comment\"># Dueling DQNrewardValue</span></span><br><span class=\"line\"><span class=\"comment\"># ActionAction</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\">#out</span></span><br><span class=\"line\"><span class=\"comment\"># Action</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">self.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=<span class=\"number\">1</span>,keep_dims=<span class=\"literal\">True</span>))</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h2 id=\"\"></h2>\n<p>reward</p>\n<ol type=\"1\">\n<li>Action</li>\n<li>Actionreward</li>\n<li></li>\n<li>ActionAction1ActionrewardAction<strong>rewardreward</strong></li>\n</ol>\n<h1 id=\"\"></h1>\n<p></p>\n<p></p>\n<p>sgd</p>\n<p></p>\n<p></p>\n<p>reward </p>\n<p>Q<sub>desired</sub>(s<sub>t</sub>, a<sub>t</sub>) = r + <span class=\"math inline\">\\(\\lambda\\)</span> Max<sub>a</sub> Q<sub>desired</sub>(s<sub>t+1</sub>, a)</p>\n<p></p>\n<p></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> done:</span><br><span class=\"line\">    episode_number += <span class=\"number\">1</span></span><br><span class=\"line\">    epx = np.vstack(xs)</span><br><span class=\"line\">    epy = np.vstack(ys)</span><br><span class=\"line\">    epr = np.vstack(drs)</span><br><span class=\"line\">    tfp = tfps</span><br><span class=\"line\">    xs,hs,dlogps,drs,ys,tfps = [],[],[],[],[],[]</span><br><span class=\"line\"></span><br><span class=\"line\">    discounted_epr = discount_rewards(epr)</span><br><span class=\"line\">    discounted_epr -= np.mean(discounted_epr)</span><br><span class=\"line\">    discounted_epr //= np.std(discounted_epr)</span><br><span class=\"line\"></span><br><span class=\"line\">    tGrad = sess.run(newGrads,feed_dict=&#123;observations: epx, input_y: epy, advantages: discounted_epr&#125;)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> ix,grad <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(tGrad):</span><br><span class=\"line\">        gradBuffer[ix] += grad</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> episode_number % batch_size == <span class=\"number\">0</span>:</span><br><span class=\"line\">            sess.run(updateGrads,feed_dict=&#123;W1Grad: gradBuffer[<span class=\"number\">0</span>],W1_1Grad:gradBuffer[<span class=\"number\">1</span>],W2Grad:gradBuffer[<span class=\"number\">2</span>]&#125;)</span><br></pre></td></tr></table></figure>\n<p></p>\n"},{"title":"How to Blance Losses in Multi Task Training","top":false,"cover":false,"toc":true,"mathjax":true,"date":"2022-01-10T06:34:19.000Z","password":null,"summary":null,"description":"loss","_content":"\n# \n\nTask Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics\n\nMulti-Task Learning as Multi-Objective Optimization\n\nMulti-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics\n\nBounding Box Regression with Uncertainty for Accurate Object Detection\n\nBayesianloss\n\n# Focal loss\n\nDynamic Task Prioritization for Multitask Learning\n\nfocal loss\n\ntasklossKPI (key performance indicator)KPIkpi\n\n batchTask_i  loss_iTask i  KPI: k_i Focal loss FL(k_i, gamma_i) = -((1 - k_i)^gamma_i) * log(k_i)gamma  2\n\nloss = sum(FL(k_i, gamma_i) * loss_i)lossFLloss \n","source":"_posts/How-to-Blance-Losses-in-Multi-Task-Training.md","raw":"---\ntitle: How to Blance Losses in Multi Task Training\ntop: false\ncover: false\ntoc: true\nmathjax: true\ndate: 2022-01-10 14:34:19\npassword:\nsummary:\ndescription: loss\ncategories:\n- Little Things\ntags:\n- Multi Task Training\n---\n\n# \n\nTask Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics\n\nMulti-Task Learning as Multi-Objective Optimization\n\nMulti-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics\n\nBounding Box Regression with Uncertainty for Accurate Object Detection\n\nBayesianloss\n\n# Focal loss\n\nDynamic Task Prioritization for Multitask Learning\n\nfocal loss\n\ntasklossKPI (key performance indicator)KPIkpi\n\n batchTask_i  loss_iTask i  KPI: k_i Focal loss FL(k_i, gamma_i) = -((1 - k_i)^gamma_i) * log(k_i)gamma  2\n\nloss = sum(FL(k_i, gamma_i) * loss_i)lossFLloss \n","slug":"How-to-Blance-Losses-in-Multi-Task-Training","published":1,"updated":"2022-01-17T09:43:07.977Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckz3w5su9000h0cvk1ae98s85","content":"<html><head></head><body><h1 id=\"\"><span class=\"post-title-index\">1. </span></h1>\n<p>Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics</p>\n<p>Multi-Task Learning as Multi-Objective Optimization</p>\n<p>Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics</p>\n<p>Bounding Box Regression with Uncertainty for Accurate Object Detection</p>\n<p>Bayesianloss</p>\n<h1 id=\"focal-loss\"><span class=\"post-title-index\">2. </span>Focal loss</h1>\n<p>Dynamic Task Prioritization for Multitask Learning</p>\n<p>focal loss</p>\n<p>tasklossKPI (key performance indicator)KPIkpi</p>\n<p> batchTask_i  loss_iTask i  KPI: k_i Focal loss FL(k_i, gamma_i) = -((1 - k_i)^gamma_i) * log(k_i)gamma  2</p>\n<p>loss = sum(FL(k_i, gamma_i) * loss_i)lossFLloss</p>\n</body></html>","site":{"data":{}},"excerpt":"<html><head></head><body></body></html>","more":"<h1 id=\"\"></h1>\n<p>Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics</p>\n<p>Multi-Task Learning as Multi-Objective Optimization</p>\n<p>Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics</p>\n<p>Bounding Box Regression with Uncertainty for Accurate Object Detection</p>\n<p>Bayesianloss</p>\n<h1 id=\"focal-loss\">Focal loss</h1>\n<p>Dynamic Task Prioritization for Multitask Learning</p>\n<p>focal loss</p>\n<p>tasklossKPI (key performance indicator)KPIkpi</p>\n<p> batchTask_i  loss_iTask i  KPI: k_i Focal loss FL(k_i, gamma_i) = -((1 - k_i)^gamma_i) * log(k_i)gamma  2</p>\n<p>loss = sum(FL(k_i, gamma_i) * loss_i)lossFLloss</p>\n"},{"title":"Personal Thought","top":false,"cover":false,"toc":true,"mathjax":true,"date":"2021-12-15T08:01:28.000Z","password":null,"summary":null,"description":"","_content":"\n\n\n# Deep Learning\n\n## \n\n### \n\n\n\ni.e.\n\n\n\nA1A1\n\nmixup\n\n\n\n\n\n**MNIST, CIFAR-10100%**\n\n\n\n### neural tangent kernel\n\n**[^1]**\n\n[^1]:NEURAL TANGENT KERNEL EIGENVALUES ACCURATELY PREDICT GENERALIZATION\n\n\n\n## \n\n### \n\nResNet\n\n```python\nshortcut(x) + out\n# \nshortcut(x) * out\n```\n\n****\n\n1.  CNN\n2. ResNet3Dmaskattention module\n\n### \n\nCifar-100Cifar-10\n\n### \n\n\n\n1. \n2. \n\n\n\n\n\n1. bn``bn(shortcut(x)) * bn(out)``\n\n2. bnbias1bn(shortcut(x))bn(out)1 \n\n    **1. shortcut(x) + outshortcut(x)1out1**\n\n   **2. out 0shorcut**\n\n### sigmoid\n\n3. attentionsigmoid``bn(shortcut(x)) * out.sigmoid()``sigmoid sigmoid\n4. soft attentionsigmoid1.  2. 2d1dSE-Netattentionattention\n\n\n\n","source":"_posts/Personal-Thought.md","raw":"---\ntitle: Personal Thought\ntop: false\ncover: false\ntoc: true\nmathjax: true\ndate: 2021-12-15 16:01:28\npassword:\nsummary:\ndescription: \ncategories:\n- About Papers\ntags:\n- Personal Thought\n- Papers\n- private\n---\n\n\n\n# Deep Learning\n\n## \n\n### \n\n\n\ni.e.\n\n\n\nA1A1\n\nmixup\n\n\n\n\n\n**MNIST, CIFAR-10100%**\n\n\n\n### neural tangent kernel\n\n**[^1]**\n\n[^1]:NEURAL TANGENT KERNEL EIGENVALUES ACCURATELY PREDICT GENERALIZATION\n\n\n\n## \n\n### \n\nResNet\n\n```python\nshortcut(x) + out\n# \nshortcut(x) * out\n```\n\n****\n\n1.  CNN\n2. ResNet3Dmaskattention module\n\n### \n\nCifar-100Cifar-10\n\n### \n\n\n\n1. \n2. \n\n\n\n\n\n1. bn``bn(shortcut(x)) * bn(out)``\n\n2. bnbias1bn(shortcut(x))bn(out)1 \n\n    **1. shortcut(x) + outshortcut(x)1out1**\n\n   **2. out 0shorcut**\n\n### sigmoid\n\n3. attentionsigmoid``bn(shortcut(x)) * out.sigmoid()``sigmoid sigmoid\n4. soft attentionsigmoid1.  2. 2d1dSE-Netattentionattention\n\n\n\n","slug":"Personal-Thought","published":1,"updated":"2022-01-17T09:43:07.977Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckz3w5sua000j0cvkdf3n4g2t","content":"<div class=\"hbe hbe-container\" id=\"hexo-blog-encrypt\" data-wpm=\"Oh, this is an invalid password. Check and try again, please.\" data-whm=\"OOPS, these decrypted content may changed, but you can still have a look.\">\n  <script id=\"hbeData\" type=\"hbeData\" data-hmacdigest=\"40e9a36583999d5f0304eec5d2bfefe7ad645846109b356755ac1a1667faa5e8\">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</script>\n  <div class=\"hbe hbe-content\">\n    <div class=\"hbe hbe-input hbe-input-default\">\n      <input class=\"hbe hbe-input-field hbe-input-field-default\" type=\"password\" id=\"hbePass\">\n      <label class=\"hbe hbe-input-label hbe-input-label-default\" for=\"hbePass\">\n        <span class=\"hbe hbe-input-label-content hbe-input-label-content-default\">Hey, password is required here.</span>\n      </label>\n    </div>\n  </div>\n</div>\n<script data-pjax src=\"/lib/hbe.js\"></script><link href=\"/css/hbe.style.css\" rel=\"stylesheet\" type=\"text/css\">","site":{"data":{}},"excerpt":"","more":"","origin":"<html><head></head><body><h1 id=\"deep-learning\"><span class=\"post-title-index\">1. </span>Deep Learning</h1>\n<h2 id=\"\"><span class=\"post-title-index\">1.1. </span></h2>\n<h3 id=\"\"><span class=\"post-title-index\">1.1.1. </span></h3>\n<p></p>\n<p>i.e.</p>\n<p></p>\n<p>A1A1</p>\n<p>mixup</p>\n<p></p>\n<p></p>\n<p><strong>MNIST, CIFAR-10100%</strong></p>\n<p></p>\n<h3 id=\"neural-tangent-kernel\"><span class=\"post-title-index\">1.1.2. </span>neural tangent kernel</h3>\n<p><strong><sup id=\"fnref:1\"><a href=\"#fn:1\" rel=\"footnote\">1</a></sup></strong></p>\n<h2 id=\"\"><span class=\"post-title-index\">1.2. </span></h2>\n<h3 id=\"\"><span class=\"post-title-index\">1.2.1. </span></h3>\n<p>ResNet</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">shortcut(x) + out</span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">shortcut(x) * out</span><br></pre></td></tr></tbody></table></figure>\n<p><strong></strong></p>\n<ol type=\"1\">\n<li> CNN</li>\n<li>ResNet3Dmaskattention module</li>\n</ol>\n<h3 id=\"\"><span class=\"post-title-index\">1.2.2. </span></h3>\n<p>Cifar-100Cifar-10</p>\n<h3 id=\"\"><span class=\"post-title-index\">1.2.3. </span></h3>\n<p></p>\n<ol type=\"1\">\n<li></li>\n<li></li>\n</ol>\n<p></p>\n<p></p>\n<ol type=\"1\">\n<li><p>bn<code>bn(shortcut(x)) * bn(out)</code></p></li>\n<li><p>bnbias1bn(shortcut(x))bn(out)1</p>\n<p><strong>1. shortcut(x) + outshortcut(x)1out1</strong></p>\n<p><strong>2. out 0shorcut</strong></p></li>\n</ol>\n<h3 id=\"sigmoid\"><span class=\"post-title-index\">1.2.4. </span>sigmoid</h3>\n<ol start=\"3\" type=\"1\">\n<li>attentionsigmoid<code>bn(shortcut(x)) * out.sigmoid()</code>sigmoid sigmoid</li>\n<li>soft attentionsigmoid1.  2. 2d1dSE-Netattentionattention</li>\n</ol>\n<div id=\"footnotes\">\n<hr>\n<div id=\"footnotelist\">\n<ol>\n<li id=\"fn:1\">\nNEURAL TANGENT KERNEL EIGENVALUES ACCURATELY PREDICT GENERALIZATION<a href=\"#fnref:1\" rev=\"footnote\"> </a>\n</li>\n</ol>\n</div>\n</div>\n</body></html>","encrypt":true},{"title":"Tips in Papers","top":false,"cover":false,"toc":true,"mathjax":true,"date":"2021-12-14T08:33:03.000Z","password":null,"summary":null,"description":"","_content":"\n# Attention\n\n## Hard and Soft Attention\n\n### Attention Mechanisms in CV: A Survey\n\n## Gumbel-Softmax Hard Attention\n\n### Categorical Reparameterization with Gumbel-Softmax\n\n#### Stochastic Neural Networks with discrete random variables\n\n\n\n#### Stochastic Gradient Estimation\n\ndumbel-softmax, score function estimator, biased path derivative estimator\n\n> However, no existing gradient estimator has been formulated specifically for categorical variables.\n\n## Reinforce Learnning Hard Attention\n\n### 2019 Scacader: Improving Accuracy of Hard Attention Models for Vision\n\n#### Hard Attention\n\nhard attentionSoft Attention<div id=\"ap\"></div> \n\n> Our best models narrow the gap to common ImageNet baselines, achieving 75% top-1 and 91% top-5 while attending to less than one-third of the image.We further demonstrate that occluding the image patches prposed by the Saccader model highly impairs classification, thus confirming these patches strong relevance to the classification task.\n\n> Typical soft attention mechanisms rescale features at one or more stages of the network. The soft mask used for rescaling often to provide some insight into the model's decision-making process, but the model's final decision may nonetheless rely on information provided by features with small weights [^1]\n\n[^1]:2018,Learn to pay attention.\n\n#### glimpsepatchglimpse\n\n> Models that employ hard attention make decisions based on only a subset of pixel in the input image, typically in the form of a series of glimpses.\n\n#### Hard AttentionHard Attention\n\n> Altough our aim in this work is to perform classification with only image-level class labels, out approach bears some resembalance to two-stage object detection models. \n\n> These models operate by generating many region proposals and then applying a classification model to each proposal. \n\n> Unlike our work, these approaches use ground-truth bounding boxes to train the classification model, and modern architectures also use bounding boxes to supervise the proposal generator.\n\n**Hard AttentionHard Attention, i.e., **\n\n#### \n\n<img src=Saccader_Over.jpg width=80% height = 80%>\n\nrep. netlogits per locationrepresentation networkBagNet[^2]\n\natten. netattentionSacadder cellwhatwhereconcatmixed\n\nSaccader cell\n\n**coordinate at time tslicetprediction**\n\n[^2]:2019, Approximating CNNs with bag-of-local-features models works surprisingly well on ImageNet.  \n\n#### Saccader cell\n\n<img src=Saccader_Cell.jpg width=80% height = 80%>\n\n\n\n1. Cell statestatestate2d softmaxlogitlogitstate\n\n   > This cell produces a sequence of locations. Elements in the sequence correctpond to target locations.\n\n2. cell state$C^t$t1$C^{t - 1}$$C^t$$-10^5$ 2d softmax\n\n   > The cell includes a 2D state ($C^t$) that keeps memory of the visited locations until time t by placing 1 in the corresponding location in the cell state. We use this state to prevent the network from returning to previously seen locations.\n\n   **statesequencestatestate011sequencelogicstatelogic**\n\n3. $C^t$mixed feature$C^{t - 1}$$C^t$1mixed featurechannel attentionchannel attentionSE-Netmask mask$C^{t - 1}$\n\n4. > At test time, the model extracts the logits at time $t$ from the representation network at location $argmax_{i,j}(\\hat{R}^t_{i,j})$.The final prediction is obtained by averaging the extracted logits across all times.\n\n**Saccader Cellsequencesequencefeature**\n\n#### \n\n> The goal of our training is to learn a policy that predicts a sequence of visual attentnion locations that is useful to the downsteam task (here image classification) in absence of location labels.\n>\n> We performed a three step training procedure using only the training class lables as supervision.\n\n![](Saccader_eq1.jpg)\n\n1. representation network\n\n   $y_{target}$$y_{target}$region of interestposition\n\n   > Key to Saccader is a pretraining step that require only class lables and provides initial attention locations for policy gradient optimization.\n\n   > Our pretraining procedure overcomes the sparse-reward problem that makes hard attention models difficult to optimize. It requires access to only class lables and prvides initial attention locations.These initial locations provide better rewards for the policy gradient learning.\n\n![](Saccader_eq2.jpg)\n\n2. location network (attention network, $1 \\times 1$ mixing convolution and Sacader cell)\n\n   T = 12\n\n   <font color =deeppink> </font>\n\n![](Saccader_eq3.jpg)\n\n3. > we trained the whole model to maximize the expected reward, where the reward ($r \\in \\{0, 1\\}$) represents whether the model final prediction after 6 glimpses (T = 6) is correct. \n\n<font color=deeppink></font>\n\n\n$l^t_s$saccader cellrT$y_{target}$<div id=\"\"></div> \n\n**13loss$y_{target}$**\n\nsaccader cellsaccader cellpatch \n\n**[](#ap)**\n\n#### ordered logits policySaccader\n\n> An ordered logits policy uses the BagNet model to pick the top K locations based on the largest class logits.\n\nOrdered logits policy\n\n> The ordered logits policy strats off with accuracy much higher than a random policy, suggesting that the patches it initially picks are meaningful to classification.\n>\n> However, accuracy is still lower than the learned Saccader model, and **performacne improves only slowly with additional glimpese. The ordered logits policy is able to capture some of the features relevant to classification, but it is a greedy policy that produces glimpses that cluster around a few top features (i.e., with low image coverage)**\n\nordered logits policyglimpeseperformanceSaccader\n\n#### \n\n[](#)Sccaderglimplese\n\n> In fact, increasing the number of glimpses beyond the number used for DRAM policy training leads to drop in performane, ulike the Saccader model that generalizes to greater umbers of glimpses.\n\n<img src=Saccader_gl.jpg width=50% height = 50%>\n\nglimpsesSaccader[](#)<font color=deeppink></font>\n\n### Hard Attention for Scalable Image Classification\n\n#### \n\n<img src=Tnet_over.jpg width=50% height = 50%>\n\n> Muti-scale processing in the proposed TNet architecture. Starting from level 1, the image is processed in low resolution to get a coarse description of its content (red cube). Extracted features are used for (hard) selection of image regions worth processing in higher resolution. The process is repeated recursively (here to 2 additional levels). Features from all levels are combined (arrows on the right) to create the final image representation used for classificaiton (blue cube).\n\nSaccader\n\n## Soft Attention\n\n### NAM, Normalization-based Attention Module\n\n#### \n\n> Those methods successfully exploit the mutual information from different dimensions of feawture. However, they lack consideration on the contributing factors of weights, which is capable of furthr suppressing the insignificant channels or pixels.\n\n****\n\nbnbnattentionattentionbn**bn**\n$$\n\\begin{align}\natt &= norm(x) \\\\\natt &= att \\times \\gamma + \\delta \\\\\natt &= att \\times \\frac\\gamma{sum(\\gamma)} \\\\\nout &= att.sigmoid() \\times x\n\\end{align}\n$$\n\n```python\nclass Channel_Att(nn.Module):\n    def __init__(self, channels, t=16):\n        super(Channel_Att, self).__init__()\n        self.channels = channels   \n        self.bn2 = nn.BatchNorm2d(self.channels, affine=True)\n    def forward(self, x):\n        residual = x\n        x = self.bn2(x)\n        weight_bn = self.bn2.weight.data.abs() / torch.sum(self.bn2.weight.data.abs())\n        x = x.permute(0, 2, 3, 1).contiguous()\n        x = torch.mul(weight_bn, x)\n        x = x.permute(0, 3, 1, 2).contiguous()\n        x = torch.sigmoid(x) * residual  \n        return x\n```\n\n#### \n\n> It applies a weightsparsity penalty to the attention modules, thus, making them more computational efficient while retaining similar performance.\n>\n> To suppress the less salient weights, we add a regularization term into the loss function.\n\n$$\nLoss = \\sum_{(x,y)}l(f(x, W), y) + p\\sum g(\\gamma) + p \\sum g(\\lambda)\n$$\n\n$g$$\\gamma$bn$\\lambda$pix normalization\n\n<font color=deeppink>loss attentioncomputational efficient? computational efficientparameterFLOPS</font>\n\n#### \n\n<font color=deeppink></font>\n\n<font color=deeppink></font>\n\n\n\n# Regularization \n\n## ADCM: Attentnion Dropout Convolutional Module\n\n![ADCM](ADCM.jpg)\n\nCBAMCBAMattention weightsDropfeature mapdropattentionhard attention\n","source":"_posts/Tips-in-Papers.md","raw":"---\ntitle: Tips in Papers\ntop: false\ncover: false\ntoc: true\nmathjax: true\ndate: 2021-12-14 16:33:03\npassword:\nsummary:\ndescription: \ncategories:\n- About Papers\ntags:\n- Papers\n- Personal Thought\n---\n\n# Attention\n\n## Hard and Soft Attention\n\n### Attention Mechanisms in CV: A Survey\n\n## Gumbel-Softmax Hard Attention\n\n### Categorical Reparameterization with Gumbel-Softmax\n\n#### Stochastic Neural Networks with discrete random variables\n\n\n\n#### Stochastic Gradient Estimation\n\ndumbel-softmax, score function estimator, biased path derivative estimator\n\n> However, no existing gradient estimator has been formulated specifically for categorical variables.\n\n## Reinforce Learnning Hard Attention\n\n### 2019 Scacader: Improving Accuracy of Hard Attention Models for Vision\n\n#### Hard Attention\n\nhard attentionSoft Attention<div id=\"ap\"></div> \n\n> Our best models narrow the gap to common ImageNet baselines, achieving 75% top-1 and 91% top-5 while attending to less than one-third of the image.We further demonstrate that occluding the image patches prposed by the Saccader model highly impairs classification, thus confirming these patches strong relevance to the classification task.\n\n> Typical soft attention mechanisms rescale features at one or more stages of the network. The soft mask used for rescaling often to provide some insight into the model's decision-making process, but the model's final decision may nonetheless rely on information provided by features with small weights [^1]\n\n[^1]:2018,Learn to pay attention.\n\n#### glimpsepatchglimpse\n\n> Models that employ hard attention make decisions based on only a subset of pixel in the input image, typically in the form of a series of glimpses.\n\n#### Hard AttentionHard Attention\n\n> Altough our aim in this work is to perform classification with only image-level class labels, out approach bears some resembalance to two-stage object detection models. \n\n> These models operate by generating many region proposals and then applying a classification model to each proposal. \n\n> Unlike our work, these approaches use ground-truth bounding boxes to train the classification model, and modern architectures also use bounding boxes to supervise the proposal generator.\n\n**Hard AttentionHard Attention, i.e., **\n\n#### \n\n<img src=Saccader_Over.jpg width=80% height = 80%>\n\nrep. netlogits per locationrepresentation networkBagNet[^2]\n\natten. netattentionSacadder cellwhatwhereconcatmixed\n\nSaccader cell\n\n**coordinate at time tslicetprediction**\n\n[^2]:2019, Approximating CNNs with bag-of-local-features models works surprisingly well on ImageNet.  \n\n#### Saccader cell\n\n<img src=Saccader_Cell.jpg width=80% height = 80%>\n\n\n\n1. Cell statestatestate2d softmaxlogitlogitstate\n\n   > This cell produces a sequence of locations. Elements in the sequence correctpond to target locations.\n\n2. cell state$C^t$t1$C^{t - 1}$$C^t$$-10^5$ 2d softmax\n\n   > The cell includes a 2D state ($C^t$) that keeps memory of the visited locations until time t by placing 1 in the corresponding location in the cell state. We use this state to prevent the network from returning to previously seen locations.\n\n   **statesequencestatestate011sequencelogicstatelogic**\n\n3. $C^t$mixed feature$C^{t - 1}$$C^t$1mixed featurechannel attentionchannel attentionSE-Netmask mask$C^{t - 1}$\n\n4. > At test time, the model extracts the logits at time $t$ from the representation network at location $argmax_{i,j}(\\hat{R}^t_{i,j})$.The final prediction is obtained by averaging the extracted logits across all times.\n\n**Saccader Cellsequencesequencefeature**\n\n#### \n\n> The goal of our training is to learn a policy that predicts a sequence of visual attentnion locations that is useful to the downsteam task (here image classification) in absence of location labels.\n>\n> We performed a three step training procedure using only the training class lables as supervision.\n\n![](Saccader_eq1.jpg)\n\n1. representation network\n\n   $y_{target}$$y_{target}$region of interestposition\n\n   > Key to Saccader is a pretraining step that require only class lables and provides initial attention locations for policy gradient optimization.\n\n   > Our pretraining procedure overcomes the sparse-reward problem that makes hard attention models difficult to optimize. It requires access to only class lables and prvides initial attention locations.These initial locations provide better rewards for the policy gradient learning.\n\n![](Saccader_eq2.jpg)\n\n2. location network (attention network, $1 \\times 1$ mixing convolution and Sacader cell)\n\n   T = 12\n\n   <font color =deeppink> </font>\n\n![](Saccader_eq3.jpg)\n\n3. > we trained the whole model to maximize the expected reward, where the reward ($r \\in \\{0, 1\\}$) represents whether the model final prediction after 6 glimpses (T = 6) is correct. \n\n<font color=deeppink></font>\n\n\n$l^t_s$saccader cellrT$y_{target}$<div id=\"\"></div> \n\n**13loss$y_{target}$**\n\nsaccader cellsaccader cellpatch \n\n**[](#ap)**\n\n#### ordered logits policySaccader\n\n> An ordered logits policy uses the BagNet model to pick the top K locations based on the largest class logits.\n\nOrdered logits policy\n\n> The ordered logits policy strats off with accuracy much higher than a random policy, suggesting that the patches it initially picks are meaningful to classification.\n>\n> However, accuracy is still lower than the learned Saccader model, and **performacne improves only slowly with additional glimpese. The ordered logits policy is able to capture some of the features relevant to classification, but it is a greedy policy that produces glimpses that cluster around a few top features (i.e., with low image coverage)**\n\nordered logits policyglimpeseperformanceSaccader\n\n#### \n\n[](#)Sccaderglimplese\n\n> In fact, increasing the number of glimpses beyond the number used for DRAM policy training leads to drop in performane, ulike the Saccader model that generalizes to greater umbers of glimpses.\n\n<img src=Saccader_gl.jpg width=50% height = 50%>\n\nglimpsesSaccader[](#)<font color=deeppink></font>\n\n### Hard Attention for Scalable Image Classification\n\n#### \n\n<img src=Tnet_over.jpg width=50% height = 50%>\n\n> Muti-scale processing in the proposed TNet architecture. Starting from level 1, the image is processed in low resolution to get a coarse description of its content (red cube). Extracted features are used for (hard) selection of image regions worth processing in higher resolution. The process is repeated recursively (here to 2 additional levels). Features from all levels are combined (arrows on the right) to create the final image representation used for classificaiton (blue cube).\n\nSaccader\n\n## Soft Attention\n\n### NAM, Normalization-based Attention Module\n\n#### \n\n> Those methods successfully exploit the mutual information from different dimensions of feawture. However, they lack consideration on the contributing factors of weights, which is capable of furthr suppressing the insignificant channels or pixels.\n\n****\n\nbnbnattentionattentionbn**bn**\n$$\n\\begin{align}\natt &= norm(x) \\\\\natt &= att \\times \\gamma + \\delta \\\\\natt &= att \\times \\frac\\gamma{sum(\\gamma)} \\\\\nout &= att.sigmoid() \\times x\n\\end{align}\n$$\n\n```python\nclass Channel_Att(nn.Module):\n    def __init__(self, channels, t=16):\n        super(Channel_Att, self).__init__()\n        self.channels = channels   \n        self.bn2 = nn.BatchNorm2d(self.channels, affine=True)\n    def forward(self, x):\n        residual = x\n        x = self.bn2(x)\n        weight_bn = self.bn2.weight.data.abs() / torch.sum(self.bn2.weight.data.abs())\n        x = x.permute(0, 2, 3, 1).contiguous()\n        x = torch.mul(weight_bn, x)\n        x = x.permute(0, 3, 1, 2).contiguous()\n        x = torch.sigmoid(x) * residual  \n        return x\n```\n\n#### \n\n> It applies a weightsparsity penalty to the attention modules, thus, making them more computational efficient while retaining similar performance.\n>\n> To suppress the less salient weights, we add a regularization term into the loss function.\n\n$$\nLoss = \\sum_{(x,y)}l(f(x, W), y) + p\\sum g(\\gamma) + p \\sum g(\\lambda)\n$$\n\n$g$$\\gamma$bn$\\lambda$pix normalization\n\n<font color=deeppink>loss attentioncomputational efficient? computational efficientparameterFLOPS</font>\n\n#### \n\n<font color=deeppink></font>\n\n<font color=deeppink></font>\n\n\n\n# Regularization \n\n## ADCM: Attentnion Dropout Convolutional Module\n\n![ADCM](ADCM.jpg)\n\nCBAMCBAMattention weightsDropfeature mapdropattentionhard attention\n","slug":"Tips-in-Papers","published":1,"updated":"2022-01-17T09:43:07.978Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckz3w5sug000n0cvkckglevj9","content":"<html><head></head><body><h1 id=\"attention\"><span class=\"post-title-index\">1. </span>Attention</h1>\n<h2 id=\"hard-and-soft-attention\"><span class=\"post-title-index\">1.1. </span>Hard and Soft Attention</h2>\n<h3 id=\"attention-mechanisms-in-cv-a-survey\"><span class=\"post-title-index\">1.1.1. </span>Attention Mechanisms in CV: A Survey</h3>\n<h2 id=\"gumbel-softmax-hard-attention\"><span class=\"post-title-index\">1.2. </span>Gumbel-Softmax Hard Attention</h2>\n<h3 id=\"categorical-reparameterization-with-gumbel-softmax\"><span class=\"post-title-index\">1.2.1. </span>Categorical Reparameterization with Gumbel-Softmax</h3>\n<h4 id=\"stochastic-neural-networks-with-discrete-random-variables\"><span class=\"post-title-index\">1.2.1.1. </span>Stochastic Neural Networks with discrete random variables</h4>\n<p></p>\n<h4 id=\"stochastic-gradient-estimation\"><span class=\"post-title-index\">1.2.1.2. </span>Stochastic Gradient Estimation</h4>\n<p>dumbel-softmax, score function estimator, biased path derivative estimator</p>\n<blockquote>\n<p>However, no existing gradient estimator has been formulated specifically for categorical variables.</p>\n</blockquote>\n<h2 id=\"reinforce-learnning-hard-attention\"><span class=\"post-title-index\">1.3. </span>Reinforce Learnning Hard Attention</h2>\n<h3 id=\"scacader-improving-accuracy-of-hard-attention-models-for-vision\"><span class=\"post-title-index\">1.3.1. </span>2019 Scacader: Improving Accuracy of Hard Attention Models for Vision</h3>\n<h4 id=\"hard-attention\"><span class=\"post-title-index\">1.3.1.1. </span>Hard Attention</h4>\nhard attentionSoft Attention\n<div id=\"ap\">\n\n</div>\n<blockquote>\n<p>Our best models narrow the gap to common ImageNet baselines, achieving 75% top-1 and 91% top-5 while attending to less than one-third of the image.We further demonstrate that occluding the image patches prposed by the Saccader model highly impairs classification, thus confirming these patches strong relevance to the classification task.</p>\n</blockquote>\n<blockquote>\n<p>Typical soft attention mechanisms rescale features at one or more stages of the network. The soft mask used for rescaling often to provide some insight into the model's decision-making process, but the model's final decision may nonetheless rely on information provided by features with small weights <sup id=\"fnref:1\"><a href=\"#fn:1\" rel=\"footnote\">1</a></sup></p>\n</blockquote>\n<h4 id=\"glimpsepatchglimpse\"><span class=\"post-title-index\">1.3.1.2. </span>glimpsepatchglimpse</h4>\n<blockquote>\n<p>Models that employ hard attention make decisions based on only a subset of pixel in the input image, typically in the form of a series of glimpses.</p>\n</blockquote>\n<h4 id=\"hard-attentionhard-attention\"><span class=\"post-title-index\">1.3.1.3. </span>Hard AttentionHard Attention</h4>\n<blockquote>\n<p>Altough our aim in this work is to perform classification with only image-level class labels, out approach bears some resembalance to two-stage object detection models.</p>\n</blockquote>\n<blockquote>\n<p>These models operate by generating many region proposals and then applying a classification model to each proposal.</p>\n</blockquote>\n<blockquote>\n<p>Unlike our work, these approaches use ground-truth bounding boxes to train the classification model, and modern architectures also use bounding boxes to supervise the proposal generator.</p>\n</blockquote>\n<p><strong>Hard AttentionHard Attention, i.e., </strong></p>\n<h4 id=\"\"><span class=\"post-title-index\">1.3.1.4. </span></h4>\n<p><img src=\"Saccader_Over.jpg\" width=\"80%\" height=\"80%\"></p>\n<p>rep. netlogits per locationrepresentation networkBagNet<sup id=\"fnref:2\"><a href=\"#fn:2\" rel=\"footnote\">2</a></sup></p>\n<p>atten. netattentionSacadder cellwhatwhereconcatmixed</p>\n<p>Saccader cell</p>\n<p><strong>coordinate at time tslicetprediction</strong></p>\n<h4 id=\"saccader-cell\"><span class=\"post-title-index\">1.3.1.5. </span>Saccader cell</h4>\n<p><img src=\"Saccader_Cell.jpg\" width=\"80%\" height=\"80%\"></p>\n<p></p>\n<ol type=\"1\">\n<li><p>Cell statestatestate2d softmaxlogitlogitstate</p>\n<blockquote>\n<p>This cell produces a sequence of locations. Elements in the sequence correctpond to target locations.</p>\n</blockquote></li>\n<li><p>cell state<span class=\"math inline\">\\(C^t\\)</span>t1<span class=\"math inline\">\\(C^{t - 1}\\)</span><span class=\"math inline\">\\(C^t\\)</span><span class=\"math inline\">\\(-10^5\\)</span> 2d softmax</p>\n<blockquote>\n<p>The cell includes a 2D state (<span class=\"math inline\">\\(C^t\\)</span>) that keeps memory of the visited locations until time t by placing 1 in the corresponding location in the cell state. We use this state to prevent the network from returning to previously seen locations.</p>\n</blockquote>\n<p><strong>statesequencestatestate011sequencelogicstatelogic</strong></p></li>\n<li><p><span class=\"math inline\">\\(C^t\\)</span>mixed feature<span class=\"math inline\">\\(C^{t - 1}\\)</span><span class=\"math inline\">\\(C^t\\)</span>1mixed featurechannel attentionchannel attentionSE-Netmask mask<span class=\"math inline\">\\(C^{t - 1}\\)</span></p></li>\n<li><blockquote>\n<p>At test time, the model extracts the logits at time <span class=\"math inline\">\\(t\\)</span> from the representation network at location <span class=\"math inline\">\\(argmax_{i,j}(\\hat{R}^t_{i,j})\\)</span>.The final prediction is obtained by averaging the extracted logits across all times.</p>\n</blockquote></li>\n</ol>\n<p><strong>Saccader Cellsequencesequencefeature</strong></p>\n<h4 id=\"\"><span class=\"post-title-index\">1.3.1.6. </span></h4>\n<blockquote>\n<p>The goal of our training is to learn a policy that predicts a sequence of visual attentnion locations that is useful to the downsteam task (here image classification) in absence of location labels.</p>\n<p>We performed a three step training procedure using only the training class lables as supervision.</p>\n</blockquote>\n<p><img src=\"Saccader_eq1.jpg\"></p>\n<ol type=\"1\">\n<li><p>representation network</p>\n<p><span class=\"math inline\">\\(y_{target}\\)</span><span class=\"math inline\">\\(y_{target}\\)</span>region of interestposition</p>\n<blockquote>\n<p>Key to Saccader is a pretraining step that require only class lables and provides initial attention locations for policy gradient optimization.</p>\n</blockquote>\n<blockquote>\n<p>Our pretraining procedure overcomes the sparse-reward problem that makes hard attention models difficult to optimize. It requires access to only class lables and prvides initial attention locations.These initial locations provide better rewards for the policy gradient learning.</p>\n</blockquote></li>\n</ol>\n<p><img src=\"Saccader_eq2.jpg\"></p>\n<ol start=\"2\" type=\"1\">\n<li><p>location network (attention network, <span class=\"math inline\">\\(1 \\times 1\\)</span> mixing convolution and Sacader cell)</p>\n<p>T = 12</p>\n<p><font color=\"deeppink\"> </font></p></li>\n</ol>\n<p><img src=\"Saccader_eq3.jpg\"></p>\n<ol start=\"3\" type=\"1\">\n<li><blockquote>\n<p>we trained the whole model to maximize the expected reward, where the reward (<span class=\"math inline\">\\(r \\in \\{0, 1\\}\\)</span>) represents whether the model final prediction after 6 glimpses (T = 6) is correct.</p>\n</blockquote></li>\n</ol>\n<p><font color=\"deeppink\"></font></p>\n<span class=\"math inline\">\\(l^t_s\\)</span>saccader cellrT<span class=\"math inline\">\\(y_{target}\\)</span>\n<div id=\"\">\n\n</div>\n<p><strong>13loss<span class=\"math inline\">\\(y_{target}\\)</span></strong></p>\n<p>saccader cellsaccader cellpatch</p>\n<p><strong><a href=\"#ap\"></a></strong></p>\n<h4 id=\"ordered-logits-policysaccader\"><span class=\"post-title-index\">1.3.1.7. </span>ordered logits policySaccader</h4>\n<blockquote>\n<p>An ordered logits policy uses the BagNet model to pick the top K locations based on the largest class logits.</p>\n</blockquote>\n<p>Ordered logits policy</p>\n<blockquote>\n<p>The ordered logits policy strats off with accuracy much higher than a random policy, suggesting that the patches it initially picks are meaningful to classification.</p>\n<p>However, accuracy is still lower than the learned Saccader model, and <strong>performacne improves only slowly with additional glimpese. The ordered logits policy is able to capture some of the features relevant to classification, but it is a greedy policy that produces glimpses that cluster around a few top features (i.e., with low image coverage)</strong></p>\n</blockquote>\n<p>ordered logits policyglimpeseperformanceSaccader</p>\n<h4 id=\"\"><span class=\"post-title-index\">1.3.1.8. </span></h4>\n<p><a href=\"#\"></a>Sccaderglimplese</p>\n<blockquote>\n<p>In fact, increasing the number of glimpses beyond the number used for DRAM policy training leads to drop in performane, ulike the Saccader model that generalizes to greater umbers of glimpses.</p>\n</blockquote>\n<p><img src=\"Saccader_gl.jpg\" width=\"50%\" height=\"50%\"></p>\n<p>glimpsesSaccader<a href=\"#\"></a><font color=\"deeppink\"></font></p>\n<h3 id=\"hard-attention-for-scalable-image-classification\"><span class=\"post-title-index\">1.3.2. </span>Hard Attention for Scalable Image Classification</h3>\n<h4 id=\"\"><span class=\"post-title-index\">1.3.2.1. </span></h4>\n<p><img src=\"Tnet_over.jpg\" width=\"50%\" height=\"50%\"></p>\n<blockquote>\n<p>Muti-scale processing in the proposed TNet architecture. Starting from level 1, the image is processed in low resolution to get a coarse description of its content (red cube). Extracted features are used for (hard) selection of image regions worth processing in higher resolution. The process is repeated recursively (here to 2 additional levels). Features from all levels are combined (arrows on the right) to create the final image representation used for classificaiton (blue cube).</p>\n</blockquote>\n<p>Saccader</p>\n<h2 id=\"soft-attention\"><span class=\"post-title-index\">1.4. </span>Soft Attention</h2>\n<h3 id=\"nam-normalization-based-attention-module\"><span class=\"post-title-index\">1.4.1. </span>NAM, Normalization-based Attention Module</h3>\n<h4 id=\"\"><span class=\"post-title-index\">1.4.1.1. </span></h4>\n<blockquote>\n<p>Those methods successfully exploit the mutual information from different dimensions of feawture. However, they lack consideration on the contributing factors of weights, which is capable of furthr suppressing the insignificant channels or pixels.</p>\n</blockquote>\n<p><strong></strong></p>\n<p>bnbnattentionattentionbn<strong>bn</strong><br>\n<span class=\"math display\">\\[\n\\begin{align}\natt &amp;= norm(x) \\\\\natt &amp;= att \\times \\gamma + \\delta \\\\\natt &amp;= att \\times \\frac\\gamma{sum(\\gamma)} \\\\\nout &amp;= att.sigmoid() \\times x\n\\end{align}\n\\]</span></p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Channel_Att</span>(<span class=\"params\">nn.Module</span>):</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, channels, t=<span class=\"number\">16</span></span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Channel_Att, self).__init__()</span><br><span class=\"line\">        self.channels = channels   </span><br><span class=\"line\">        self.bn2 = nn.BatchNorm2d(self.channels, affine=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, x</span>):</span></span><br><span class=\"line\">        residual = x</span><br><span class=\"line\">        x = self.bn2(x)</span><br><span class=\"line\">        weight_bn = self.bn2.weight.data.<span class=\"built_in\">abs</span>() / torch.<span class=\"built_in\">sum</span>(self.bn2.weight.data.<span class=\"built_in\">abs</span>())</span><br><span class=\"line\">        x = x.permute(<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">1</span>).contiguous()</span><br><span class=\"line\">        x = torch.mul(weight_bn, x)</span><br><span class=\"line\">        x = x.permute(<span class=\"number\">0</span>, <span class=\"number\">3</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>).contiguous()</span><br><span class=\"line\">        x = torch.sigmoid(x) * residual  </span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br></pre></td></tr></tbody></table></figure>\n<h4 id=\"\"><span class=\"post-title-index\">1.4.1.2. </span></h4>\n<blockquote>\n<p>It applies a weightsparsity penalty to the attention modules, thus, making them more computational efficient while retaining similar performance.</p>\n<p>To suppress the less salient weights, we add a regularization term into the loss function.</p>\n</blockquote>\n<p><span class=\"math display\">\\[\nLoss = \\sum_{(x,y)}l(f(x, W), y) + p\\sum g(\\gamma) + p \\sum g(\\lambda)\n\\]</span></p>\n<p><span class=\"math inline\">\\(g\\)</span><span class=\"math inline\">\\(\\gamma\\)</span>bn<span class=\"math inline\">\\(\\lambda\\)</span>pix normalization</p>\n<p><font color=\"deeppink\">loss attentioncomputational efficient? computational efficientparameterFLOPS</font></p>\n<h4 id=\"\"><span class=\"post-title-index\">1.4.1.3. </span></h4>\n<p><font color=\"deeppink\"></font></p>\n<p><font color=\"deeppink\"></font></p>\n<h1 id=\"regularization\"><span class=\"post-title-index\">2. </span>Regularization</h1>\n<h2 id=\"adcm-attentnion-dropout-convolutional-module\"><span class=\"post-title-index\">2.1. </span>ADCM: Attentnion Dropout Convolutional Module</h2>\n<p><img src=\"ADCM.jpg\" alt=\"ADCM\"></p>\nCBAMCBAMattention weightsDropfeature mapdropattentionhard attention<br>\n\n<div id=\"footnotes\">\n<hr>\n<div id=\"footnotelist\">\n<ol>\n<li id=\"fn:1\">\n2018,Learn to pay attention.<a href=\"#fnref:1\" rev=\"footnote\"> </a>\n</li>\n<li id=\"fn:2\">\n2019, Approximating CNNs with bag-of-local-features models works surprisingly well on ImageNet.<a href=\"#fnref:2\" rev=\"footnote\"> </a>\n</li>\n</ol>\n</div>\n</div>\n</body></html>","site":{"data":{}},"excerpt":"<html><head></head><body></body></html>","more":"<h1 id=\"attention\">Attention</h1>\n<h2 id=\"hard-and-soft-attention\">Hard and Soft Attention</h2>\n<h3 id=\"attention-mechanisms-in-cv-a-survey\">Attention Mechanisms in CV: A Survey</h3>\n<h2 id=\"gumbel-softmax-hard-attention\">Gumbel-Softmax Hard Attention</h2>\n<h3 id=\"categorical-reparameterization-with-gumbel-softmax\">Categorical Reparameterization with Gumbel-Softmax</h3>\n<h4 id=\"stochastic-neural-networks-with-discrete-random-variables\">Stochastic Neural Networks with discrete random variables</h4>\n<p></p>\n<h4 id=\"stochastic-gradient-estimation\">Stochastic Gradient Estimation</h4>\n<p>dumbel-softmax, score function estimator, biased path derivative estimator</p>\n<blockquote>\n<p>However, no existing gradient estimator has been formulated specifically for categorical variables.</p>\n</blockquote>\n<h2 id=\"reinforce-learnning-hard-attention\">Reinforce Learnning Hard Attention</h2>\n<h3 id=\"scacader-improving-accuracy-of-hard-attention-models-for-vision\">2019 Scacader: Improving Accuracy of Hard Attention Models for Vision</h3>\n<h4 id=\"hard-attention\">Hard Attention</h4>\nhard attentionSoft Attention\n<div id=\"ap\">\n\n</div>\n<blockquote>\n<p>Our best models narrow the gap to common ImageNet baselines, achieving 75% top-1 and 91% top-5 while attending to less than one-third of the image.We further demonstrate that occluding the image patches prposed by the Saccader model highly impairs classification, thus confirming these patches strong relevance to the classification task.</p>\n</blockquote>\n<blockquote>\n<p>Typical soft attention mechanisms rescale features at one or more stages of the network. The soft mask used for rescaling often to provide some insight into the model's decision-making process, but the model's final decision may nonetheless rely on information provided by features with small weights <sup id=\"fnref:1\"><a href=\"#fn:1\" rel=\"footnote\">1</a></sup></p>\n</blockquote>\n<h4 id=\"glimpsepatchglimpse\">glimpsepatchglimpse</h4>\n<blockquote>\n<p>Models that employ hard attention make decisions based on only a subset of pixel in the input image, typically in the form of a series of glimpses.</p>\n</blockquote>\n<h4 id=\"hard-attentionhard-attention\">Hard AttentionHard Attention</h4>\n<blockquote>\n<p>Altough our aim in this work is to perform classification with only image-level class labels, out approach bears some resembalance to two-stage object detection models.</p>\n</blockquote>\n<blockquote>\n<p>These models operate by generating many region proposals and then applying a classification model to each proposal.</p>\n</blockquote>\n<blockquote>\n<p>Unlike our work, these approaches use ground-truth bounding boxes to train the classification model, and modern architectures also use bounding boxes to supervise the proposal generator.</p>\n</blockquote>\n<p><strong>Hard AttentionHard Attention, i.e., </strong></p>\n<h4 id=\"\"></h4>\n<p><img src=Saccader_Over.jpg width=80% height = 80%></p>\n<p>rep. netlogits per locationrepresentation networkBagNet<sup id=\"fnref:2\"><a href=\"#fn:2\" rel=\"footnote\">2</a></sup></p>\n<p>atten. netattentionSacadder cellwhatwhereconcatmixed</p>\n<p>Saccader cell</p>\n<p><strong>coordinate at time tslicetprediction</strong></p>\n<h4 id=\"saccader-cell\">Saccader cell</h4>\n<p><img src=Saccader_Cell.jpg width=80% height = 80%></p>\n<p></p>\n<ol type=\"1\">\n<li><p>Cell statestatestate2d softmaxlogitlogitstate</p>\n<blockquote>\n<p>This cell produces a sequence of locations. Elements in the sequence correctpond to target locations.</p>\n</blockquote></li>\n<li><p>cell state<span class=\"math inline\">\\(C^t\\)</span>t1<span class=\"math inline\">\\(C^{t - 1}\\)</span><span class=\"math inline\">\\(C^t\\)</span><span class=\"math inline\">\\(-10^5\\)</span> 2d softmax</p>\n<blockquote>\n<p>The cell includes a 2D state (<span class=\"math inline\">\\(C^t\\)</span>) that keeps memory of the visited locations until time t by placing 1 in the corresponding location in the cell state. We use this state to prevent the network from returning to previously seen locations.</p>\n</blockquote>\n<p><strong>statesequencestatestate011sequencelogicstatelogic</strong></p></li>\n<li><p><span class=\"math inline\">\\(C^t\\)</span>mixed feature<span class=\"math inline\">\\(C^{t - 1}\\)</span><span class=\"math inline\">\\(C^t\\)</span>1mixed featurechannel attentionchannel attentionSE-Netmask mask<span class=\"math inline\">\\(C^{t - 1}\\)</span></p></li>\n<li><blockquote>\n<p>At test time, the model extracts the logits at time <span class=\"math inline\">\\(t\\)</span> from the representation network at location <span class=\"math inline\">\\(argmax_{i,j}(\\hat{R}^t_{i,j})\\)</span>.The final prediction is obtained by averaging the extracted logits across all times.</p>\n</blockquote></li>\n</ol>\n<p><strong>Saccader Cellsequencesequencefeature</strong></p>\n<h4 id=\"\"></h4>\n<blockquote>\n<p>The goal of our training is to learn a policy that predicts a sequence of visual attentnion locations that is useful to the downsteam task (here image classification) in absence of location labels.</p>\n<p>We performed a three step training procedure using only the training class lables as supervision.</p>\n</blockquote>\n<p><img src=\"Saccader_eq1.jpg\" /></p>\n<ol type=\"1\">\n<li><p>representation network</p>\n<p><span class=\"math inline\">\\(y_{target}\\)</span><span class=\"math inline\">\\(y_{target}\\)</span>region of interestposition</p>\n<blockquote>\n<p>Key to Saccader is a pretraining step that require only class lables and provides initial attention locations for policy gradient optimization.</p>\n</blockquote>\n<blockquote>\n<p>Our pretraining procedure overcomes the sparse-reward problem that makes hard attention models difficult to optimize. It requires access to only class lables and prvides initial attention locations.These initial locations provide better rewards for the policy gradient learning.</p>\n</blockquote></li>\n</ol>\n<p><img src=\"Saccader_eq2.jpg\" /></p>\n<ol start=\"2\" type=\"1\">\n<li><p>location network (attention network, <span class=\"math inline\">\\(1 \\times 1\\)</span> mixing convolution and Sacader cell)</p>\n<p>T = 12</p>\n<p><font color =deeppink> </font></p></li>\n</ol>\n<p><img src=\"Saccader_eq3.jpg\" /></p>\n<ol start=\"3\" type=\"1\">\n<li><blockquote>\n<p>we trained the whole model to maximize the expected reward, where the reward (<span class=\"math inline\">\\(r \\in \\{0, 1\\}\\)</span>) represents whether the model final prediction after 6 glimpses (T = 6) is correct.</p>\n</blockquote></li>\n</ol>\n<p><font color=deeppink></font></p>\n<span class=\"math inline\">\\(l^t_s\\)</span>saccader cellrT<span class=\"math inline\">\\(y_{target}\\)</span>\n<div id=\"\">\n\n</div>\n<p><strong>13loss<span class=\"math inline\">\\(y_{target}\\)</span></strong></p>\n<p>saccader cellsaccader cellpatch</p>\n<p><strong><a href=\"#ap\"></a></strong></p>\n<h4 id=\"ordered-logits-policysaccader\">ordered logits policySaccader</h4>\n<blockquote>\n<p>An ordered logits policy uses the BagNet model to pick the top K locations based on the largest class logits.</p>\n</blockquote>\n<p>Ordered logits policy</p>\n<blockquote>\n<p>The ordered logits policy strats off with accuracy much higher than a random policy, suggesting that the patches it initially picks are meaningful to classification.</p>\n<p>However, accuracy is still lower than the learned Saccader model, and <strong>performacne improves only slowly with additional glimpese. The ordered logits policy is able to capture some of the features relevant to classification, but it is a greedy policy that produces glimpses that cluster around a few top features (i.e., with low image coverage)</strong></p>\n</blockquote>\n<p>ordered logits policyglimpeseperformanceSaccader</p>\n<h4 id=\"\"></h4>\n<p><a href=\"#\"></a>Sccaderglimplese</p>\n<blockquote>\n<p>In fact, increasing the number of glimpses beyond the number used for DRAM policy training leads to drop in performane, ulike the Saccader model that generalizes to greater umbers of glimpses.</p>\n</blockquote>\n<p><img src=Saccader_gl.jpg width=50% height = 50%></p>\n<p>glimpsesSaccader<a href=\"#\"></a><font color=deeppink></font></p>\n<h3 id=\"hard-attention-for-scalable-image-classification\">Hard Attention for Scalable Image Classification</h3>\n<h4 id=\"\"></h4>\n<p><img src=Tnet_over.jpg width=50% height = 50%></p>\n<blockquote>\n<p>Muti-scale processing in the proposed TNet architecture. Starting from level 1, the image is processed in low resolution to get a coarse description of its content (red cube). Extracted features are used for (hard) selection of image regions worth processing in higher resolution. The process is repeated recursively (here to 2 additional levels). Features from all levels are combined (arrows on the right) to create the final image representation used for classificaiton (blue cube).</p>\n</blockquote>\n<p>Saccader</p>\n<h2 id=\"soft-attention\">Soft Attention</h2>\n<h3 id=\"nam-normalization-based-attention-module\">NAM, Normalization-based Attention Module</h3>\n<h4 id=\"\"></h4>\n<blockquote>\n<p>Those methods successfully exploit the mutual information from different dimensions of feawture. However, they lack consideration on the contributing factors of weights, which is capable of furthr suppressing the insignificant channels or pixels.</p>\n</blockquote>\n<p><strong></strong></p>\n<p>bnbnattentionattentionbn<strong>bn</strong><br />\n<span class=\"math display\">\\[\n\\begin{align}\natt &amp;= norm(x) \\\\\natt &amp;= att \\times \\gamma + \\delta \\\\\natt &amp;= att \\times \\frac\\gamma{sum(\\gamma)} \\\\\nout &amp;= att.sigmoid() \\times x\n\\end{align}\n\\]</span></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Channel_Att</span>(<span class=\"params\">nn.Module</span>):</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, channels, t=<span class=\"number\">16</span></span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Channel_Att, self).__init__()</span><br><span class=\"line\">        self.channels = channels   </span><br><span class=\"line\">        self.bn2 = nn.BatchNorm2d(self.channels, affine=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, x</span>):</span></span><br><span class=\"line\">        residual = x</span><br><span class=\"line\">        x = self.bn2(x)</span><br><span class=\"line\">        weight_bn = self.bn2.weight.data.<span class=\"built_in\">abs</span>() / torch.<span class=\"built_in\">sum</span>(self.bn2.weight.data.<span class=\"built_in\">abs</span>())</span><br><span class=\"line\">        x = x.permute(<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">1</span>).contiguous()</span><br><span class=\"line\">        x = torch.mul(weight_bn, x)</span><br><span class=\"line\">        x = x.permute(<span class=\"number\">0</span>, <span class=\"number\">3</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>).contiguous()</span><br><span class=\"line\">        x = torch.sigmoid(x) * residual  </span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure>\n<h4 id=\"\"></h4>\n<blockquote>\n<p>It applies a weightsparsity penalty to the attention modules, thus, making them more computational efficient while retaining similar performance.</p>\n<p>To suppress the less salient weights, we add a regularization term into the loss function.</p>\n</blockquote>\n<p><span class=\"math display\">\\[\nLoss = \\sum_{(x,y)}l(f(x, W), y) + p\\sum g(\\gamma) + p \\sum g(\\lambda)\n\\]</span></p>\n<p><span class=\"math inline\">\\(g\\)</span><span class=\"math inline\">\\(\\gamma\\)</span>bn<span class=\"math inline\">\\(\\lambda\\)</span>pix normalization</p>\n<p><font color=deeppink>loss attentioncomputational efficient? computational efficientparameterFLOPS</font></p>\n<h4 id=\"\"></h4>\n<p><font color=deeppink></font></p>\n<p><font color=deeppink></font></p>\n<h1 id=\"regularization\">Regularization</h1>\n<h2 id=\"adcm-attentnion-dropout-convolutional-module\">ADCM: Attentnion Dropout Convolutional Module</h2>\n<p><img src=\"ADCM.jpg\" alt=\"ADCM\" /></p>\nCBAMCBAMattention weightsDropfeature mapdropattentionhard attention<br />\n\n<div id=\"footnotes\">\n<hr>\n<div id=\"footnotelist\">\n<ol>\n<li id=\"fn:1\">\n2018,Learn to pay attention.<a href=\"#fnref:1\" rev=\"footnote\"> </a>\n</li>\n<li id=\"fn:2\">\n2019, Approximating CNNs with bag-of-local-features models works surprisingly well on ImageNet.<a href=\"#fnref:2\" rev=\"footnote\"> </a>\n</li>\n</ol>\n</div>\n</div>\n"},{"title":"Transformer and BERT","top":false,"cover":false,"toc":true,"mathjax":true,"date":"2021-12-18T02:00:56.000Z","password":null,"summary":null,"description":"NLPTransformerBERT.","_content":"\nI will introduce transformer and BERT according to their original papers.\n[^1] [^2]\n\n[^2]:BERT: Pre-training of Deep Bidirectional Transformer for Language Understanding\n[^1]:Attention is all you need\n\nBased on the two paper, we will also take the first step to NLP.\n\n# Transformer #\n\n\n\n# BERT #\n\n","source":"_posts/Transformer-and-BERT.md","raw":"---\ntitle: Transformer and BERT\ntop: false\ncover: false\ntoc: true\nmathjax: true\ndate: 2021-12-18 10:00:56\npassword:\nsummary:\ndescription: NLPTransformerBERT.\ncategories:\n- Natural Language Processing\ntags:\n- Natural Language Processing\n- Transformer\n- BERT\n---\n\nI will introduce transformer and BERT according to their original papers.\n[^1] [^2]\n\n[^2]:BERT: Pre-training of Deep Bidirectional Transformer for Language Understanding\n[^1]:Attention is all you need\n\nBased on the two paper, we will also take the first step to NLP.\n\n# Transformer #\n\n\n\n# BERT #\n\n","slug":"Transformer-and-BERT","published":1,"updated":"2022-02-08T08:22:28.628Z","_id":"ckz3w5sui000p0cvke9djgqok","comments":1,"layout":"post","photos":[],"link":"","content":"<html><head></head><body><p>I will introduce transformer and BERT according to their original papers.<br>\n<sup id=\"fnref:1\"><a href=\"#fn:1\" rel=\"footnote\">1</a></sup> <sup id=\"fnref:2\"><a href=\"#fn:2\" rel=\"footnote\">2</a></sup></p>\n<p>Based on the two paper, we will also take the first step to NLP.</p>\n<h1 id=\"transformer\"><span class=\"post-title-index\">1. </span>Transformer</h1>\n<h1 id=\"bert\"><span class=\"post-title-index\">2. </span>BERT</h1>\n<div id=\"footnotes\">\n<hr>\n<div id=\"footnotelist\">\n<ol>\n<li id=\"fn:1\">\nAttention is all you need<a href=\"#fnref:1\" rev=\"footnote\"> </a>\n</li>\n<li id=\"fn:2\">\nBERT: Pre-training of Deep Bidirectional Transformer for Language Understanding<a href=\"#fnref:2\" rev=\"footnote\"> </a>\n</li>\n</ol>\n</div>\n</div>\n</body></html>","site":{"data":{}},"excerpt":"<html><head></head><body></body></html>","more":"<p>I will introduce transformer and BERT according to their original papers.<br />\n<sup id=\"fnref:1\"><a href=\"#fn:1\" rel=\"footnote\">1</a></sup> <sup id=\"fnref:2\"><a href=\"#fn:2\" rel=\"footnote\">2</a></sup></p>\n<p>Based on the two paper, we will also take the first step to NLP.</p>\n<h1 id=\"transformer\">Transformer</h1>\n<h1 id=\"bert\">BERT</h1>\n<div id=\"footnotes\">\n<hr>\n<div id=\"footnotelist\">\n<ol>\n<li id=\"fn:1\">\nAttention is all you need<a href=\"#fnref:1\" rev=\"footnote\"> </a>\n</li>\n<li id=\"fn:2\">\nBERT: Pre-training of Deep Bidirectional Transformer for Language Understanding<a href=\"#fnref:2\" rev=\"footnote\"> </a>\n</li>\n</ol>\n</div>\n</div>\n"}],"PostAsset":[{"_id":"source/_posts/An-Introduction-to-Git/git.jpg","slug":"git.jpg","post":"ckz3w5stk00030cvkf6sogjin","modified":0,"renderable":0},{"_id":"source/_posts/First-Step-to-RL/policy_network.py","slug":"policy_network.py","post":"ckz3w5su3000d0cvk9sww0q4g","modified":0,"renderable":0},{"_id":"source/_posts/First-Step-to-RL/q_learning.py","slug":"q_learning.py","post":"ckz3w5su3000d0cvk9sww0q4g","modified":0,"renderable":0},{"_id":"source/_posts/Tips-in-Papers/ADCM.jpg","slug":"ADCM.jpg","post":"ckz3w5sug000n0cvkckglevj9","modified":0,"renderable":0},{"_id":"source/_posts/Tips-in-Papers/Saccader_Cell.jpg","slug":"Saccader_Cell.jpg","post":"ckz3w5sug000n0cvkckglevj9","modified":0,"renderable":0},{"_id":"source/_posts/Tips-in-Papers/Saccader_Over.jpg","slug":"Saccader_Over.jpg","post":"ckz3w5sug000n0cvkckglevj9","modified":0,"renderable":0},{"_id":"source/_posts/Tips-in-Papers/Saccader_eq1.jpg","slug":"Saccader_eq1.jpg","post":"ckz3w5sug000n0cvkckglevj9","modified":0,"renderable":0},{"_id":"source/_posts/Tips-in-Papers/Saccader_eq2.jpg","slug":"Saccader_eq2.jpg","post":"ckz3w5sug000n0cvkckglevj9","modified":0,"renderable":0},{"_id":"source/_posts/Tips-in-Papers/Saccader_eq3.jpg","slug":"Saccader_eq3.jpg","post":"ckz3w5sug000n0cvkckglevj9","modified":0,"renderable":0},{"_id":"source/_posts/Tips-in-Papers/Saccader_gl.jpg","slug":"Saccader_gl.jpg","post":"ckz3w5sug000n0cvkckglevj9","modified":0,"renderable":0},{"_id":"source/_posts/Tips-in-Papers/Tnet_over.jpg","slug":"Tnet_over.jpg","post":"ckz3w5sug000n0cvkckglevj9","modified":0,"renderable":0}],"PostCategory":[{"post_id":"ckz3w5stc00010cvk1e6nfxqx","category_id":"ckz3w5stn00040cvkfetd3p1f","_id":"ckz3w5su4000e0cvk1lr94eyx"},{"post_id":"ckz3w5su9000h0cvk1ae98s85","category_id":"ckz3w5stz000a0cvk53j7dzkl","_id":"ckz3w5suj000q0cvk0i5g612s"},{"post_id":"ckz3w5sts00080cvk5fyn3gcq","category_id":"ckz3w5stz000a0cvk53j7dzkl","_id":"ckz3w5sul000u0cvk096pbjwo"},{"post_id":"ckz3w5stu00090cvk1v727gfc","category_id":"ckz3w5suj000r0cvk238sfky5","_id":"ckz3w5sum000x0cvkbtviao2a"},{"post_id":"ckz3w5su1000c0cvkctsy38ac","category_id":"ckz3w5sul000v0cvkglkp66ud","_id":"ckz3w5suo00100cvk6dx6fgie"},{"post_id":"ckz3w5stk00030cvkf6sogjin","category_id":"ckz3w5stz000a0cvk53j7dzkl","_id":"ckz3w5suo00140cvkgacwbrot"},{"post_id":"ckz3w5stk00030cvkf6sogjin","category_id":"ckz3w5sum000y0cvk7aqy1hqy","_id":"ckz3w5suq00180cvk6uie7amo"},{"post_id":"ckz3w5su3000d0cvk9sww0q4g","category_id":"ckz3w5suo00110cvk8gsogxrj","_id":"ckz3w5suq001a0cvk258c1sfv"},{"post_id":"ckz3w5str00070cvkatt21uiq","category_id":"ckz3w5stz000a0cvk53j7dzkl","_id":"ckz3w5sus001d0cvk09p1b8tg"},{"post_id":"ckz3w5str00070cvkatt21uiq","category_id":"ckz3w5suo00150cvkbt2w3tp1","_id":"ckz3w5sux001g0cvkcuud8f0f"},{"post_id":"ckz3w5sua000j0cvkdf3n4g2t","category_id":"ckz3w5suq001b0cvkczqu1wkz","_id":"ckz3w5sux001j0cvk8hw7gqno"},{"post_id":"ckz3w5sug000n0cvkckglevj9","category_id":"ckz3w5suq001b0cvkczqu1wkz","_id":"ckz3w5suy001n0cvkfhzr19mw"},{"post_id":"ckz3w5sui000p0cvke9djgqok","category_id":"ckz3w5sux001i0cvkflbk875e","_id":"ckz3w5suz001r0cvkh9l56dqu"}],"PostTag":[{"post_id":"ckz3w5stc00010cvk1e6nfxqx","tag_id":"ckz3w5stp00050cvkfoq5h4bj","_id":"ckz3w5su9000i0cvk6zxm8mi1"},{"post_id":"ckz3w5stc00010cvk1e6nfxqx","tag_id":"ckz3w5su0000b0cvk4hs46kh6","_id":"ckz3w5sud000k0cvk25to8a7l"},{"post_id":"ckz3w5stk00030cvkf6sogjin","tag_id":"ckz3w5su5000g0cvk21we610j","_id":"ckz3w5sui000o0cvk313y2qxe"},{"post_id":"ckz3w5str00070cvkatt21uiq","tag_id":"ckz3w5suf000m0cvk3smg723x","_id":"ckz3w5sul000t0cvka5hc69i6"},{"post_id":"ckz3w5sts00080cvk5fyn3gcq","tag_id":"ckz3w5suk000s0cvk4s8yeob1","_id":"ckz3w5suo00130cvk9j6l19tf"},{"post_id":"ckz3w5sts00080cvk5fyn3gcq","tag_id":"ckz3w5sum000w0cvk65wbag5g","_id":"ckz3w5sup00160cvkg56ucu2o"},{"post_id":"ckz3w5sts00080cvk5fyn3gcq","tag_id":"ckz3w5sun000z0cvk6hca4vgs","_id":"ckz3w5suq00190cvk0r5aedgd"},{"post_id":"ckz3w5stu00090cvk1v727gfc","tag_id":"ckz3w5suo00120cvk88videpq","_id":"ckz3w5sux001h0cvkg3hu7nm8"},{"post_id":"ckz3w5stu00090cvk1v727gfc","tag_id":"ckz3w5sup00170cvkhml9acvo","_id":"ckz3w5suy001k0cvkbm52hf64"},{"post_id":"ckz3w5stu00090cvk1v727gfc","tag_id":"ckz3w5sur001c0cvkfwwe4k62","_id":"ckz3w5suy001m0cvk2uss7gzx"},{"post_id":"ckz3w5su1000c0cvkctsy38ac","tag_id":"ckz3w5suw001f0cvk4xt024iz","_id":"ckz3w5suz001o0cvk855ogfuy"},{"post_id":"ckz3w5su3000d0cvk9sww0q4g","tag_id":"ckz3w5suy001l0cvkduw345e3","_id":"ckz3w5suz001q0cvkerc75kuc"},{"post_id":"ckz3w5su9000h0cvk1ae98s85","tag_id":"ckz3w5suz001p0cvk939kbtf8","_id":"ckz3w5sv0001t0cvkafbz4kb5"},{"post_id":"ckz3w5sua000j0cvkdf3n4g2t","tag_id":"ckz3w5suo00120cvk88videpq","_id":"ckz3w5sv1001x0cvkfhlabtx0"},{"post_id":"ckz3w5sua000j0cvkdf3n4g2t","tag_id":"ckz3w5sv0001u0cvkeeol6yn7","_id":"ckz3w5sv1001y0cvkda4pcj8y"},{"post_id":"ckz3w5sua000j0cvkdf3n4g2t","tag_id":"ckz3w5sur001c0cvkfwwe4k62","_id":"ckz3w5sv200200cvkdgf8ddgm"},{"post_id":"ckz3w5sug000n0cvkckglevj9","tag_id":"ckz3w5sv0001u0cvkeeol6yn7","_id":"ckz3w5sv200220cvkbev3dj4v"},{"post_id":"ckz3w5sug000n0cvkckglevj9","tag_id":"ckz3w5suo00120cvk88videpq","_id":"ckz3w5sv200230cvke00jb1qg"},{"post_id":"ckz3w5sui000p0cvke9djgqok","tag_id":"ckz3w5sv200210cvkft3ycja1","_id":"ckz3w5sv300260cvk13ts54ly"},{"post_id":"ckz3w5sui000p0cvke9djgqok","tag_id":"ckz3w5sv200240cvk3ztd019n","_id":"ckz3w5sv300270cvk2d8zho0b"},{"post_id":"ckz3w5sui000p0cvke9djgqok","tag_id":"ckz3w5sv300250cvken4ggg6a","_id":"ckz3w5sv300280cvk64qwalui"}],"Tag":[{"name":"Algorithm","_id":"ckz3w5stp00050cvkfoq5h4bj"},{"name":"Programming","_id":"ckz3w5su0000b0cvk4hs46kh6"},{"name":"Git","_id":"ckz3w5su5000g0cvk21we610j"},{"name":"Hexo","_id":"ckz3w5suf000m0cvk3smg723x"},{"name":"vim","_id":"ckz3w5suk000s0cvk4s8yeob1"},{"name":"markdown","_id":"ckz3w5sum000w0cvk65wbag5g"},{"name":"vimtex","_id":"ckz3w5sun000z0cvk6hca4vgs"},{"name":"Personal Thought","_id":"ckz3w5suo00120cvk88videpq"},{"name":"Experiments","_id":"ckz3w5sup00170cvkhml9acvo"},{"name":"private","_id":"ckz3w5sur001c0cvkfwwe4k62"},{"name":"Topological Data Analysis","_id":"ckz3w5suw001f0cvk4xt024iz"},{"name":"Reinforcement Learning","_id":"ckz3w5suy001l0cvkduw345e3"},{"name":"Multi Task Training","_id":"ckz3w5suz001p0cvk939kbtf8"},{"name":"Papers","_id":"ckz3w5sv0001u0cvkeeol6yn7"},{"name":"Natural Language Processing","_id":"ckz3w5sv200210cvkft3ycja1"},{"name":"Transformer","_id":"ckz3w5sv200240cvk3ztd019n"},{"name":"BERT","_id":"ckz3w5sv300250cvken4ggg6a"}]}}