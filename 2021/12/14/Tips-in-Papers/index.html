<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)">
<meta name="generator" content="Hexo 5.4.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/ye_32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/ye_16.ico">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"yexiang.ml","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.8.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>
<meta name="description" content="记录阅读论文过程中，新获取的信息。">
<meta property="og:type" content="article">
<meta property="og:title" content="Tips in Papers">
<meta property="og:url" content="http://yexiang.ml/2021/12/14/Tips-in-Papers/index.html">
<meta property="og:site_name" content="Ye, Xiang&#39;s Blog">
<meta property="og:description" content="记录阅读论文过程中，新获取的信息。">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://yexiang.ml/2021/12/14/Tips-in-Papers/Saccader_Over.jpg">
<meta property="og:image" content="http://yexiang.ml/2021/12/14/Tips-in-Papers/Saccader_Cell.jpg">
<meta property="og:image" content="http://yexiang.ml/2021/12/14/Tips-in-Papers/Saccader_eq1.jpg">
<meta property="og:image" content="http://yexiang.ml/2021/12/14/Tips-in-Papers/Saccader_eq2.jpg">
<meta property="og:image" content="http://yexiang.ml/2021/12/14/Tips-in-Papers/Saccader_eq3.jpg">
<meta property="og:image" content="http://yexiang.ml/2021/12/14/Tips-in-Papers/Saccader_gl.jpg">
<meta property="og:image" content="http://yexiang.ml/2021/12/14/Tips-in-Papers/Tnet_over.jpg">
<meta property="og:image" content="http://yexiang.ml/2021/12/14/Tips-in-Papers/ADCM.jpg">
<meta property="article:published_time" content="2021-12-14T08:33:03.000Z">
<meta property="article:modified_time" content="2022-05-12T11:20:52.770Z">
<meta property="article:author" content="Xiang Ye">
<meta property="article:tag" content="Personal Thought">
<meta property="article:tag" content="Papers">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yexiang.ml/2021/12/14/Tips-in-Papers/Saccader_Over.jpg">


<link rel="canonical" href="http://yexiang.ml/2021/12/14/Tips-in-Papers/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://yexiang.ml/2021/12/14/Tips-in-Papers/","path":"2021/12/14/Tips-in-Papers/","title":"Tips in Papers"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Tips in Papers | Ye, Xiang's Blog</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Ye, Xiang's Blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#attention"><span class="nav-text">1.  Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#hard-and-soft-attention"><span class="nav-text">1.1.  Hard and Soft Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#attention-mechanisms-in-cv-a-survey"><span class="nav-text">1.1.1.  Attention Mechanisms in CV: A Survey</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#gumbel-softmax-hard-attention"><span class="nav-text">1.2.  Gumbel-Softmax Hard Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#categorical-reparameterization-with-gumbel-softmax"><span class="nav-text">1.2.1.  Categorical Reparameterization with Gumbel-Softmax</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%96%87%E7%AB%A0%E5%BC%95%E5%87%BA%E4%BA%86%E4%B8%80%E7%B1%BB%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Cstochastic-neural-networks-with-discrete-random-variables"><span class="nav-text">1.2.1.1.  文章引出了一类神经网络，Stochastic Neural Networks with discrete random variables。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%96%87%E7%AB%A0%E5%BC%95%E5%87%BA%E4%BA%86%E4%B8%80%E4%B8%AA%E6%96%B9%E5%90%91stochastic-gradient-estimation"><span class="nav-text">1.2.1.2.  文章引出了一个方向，Stochastic Gradient Estimation</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#reinforce-learnning-hard-attention"><span class="nav-text">1.3.  Reinforce Learnning Hard Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-scacader-improving-accuracy-of-hard-attention-models-for-vision"><span class="nav-text">1.3.1.  2019 Scacader: Improving Accuracy of Hard Attention Models for Vision</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#hard-attention-%E9%80%89%E6%8B%A9%E7%9B%B8%E5%85%B3%E7%9A%84%E7%89%B9%E5%BE%81%E4%BD%9C%E4%B8%BA%E8%BE%93%E5%85%A5%E6%98%AF%E7%9C%9F%E6%AD%A3%E5%85%B7%E6%9C%89%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7%E7%9A%84"><span class="nav-text">1.3.1.1.  Hard Attention 选择相关的特征作为输入，是真正具有可解释性的。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%96%87%E4%B8%AD-glimpse-%E7%9A%84%E5%90%AB%E4%B9%89%E5%8F%AF%E4%BB%A5%E7%90%86%E8%A7%A3%E6%88%90%E6%AF%8F%E4%B8%80%E4%B8%AA%E4%BD%8D%E7%BD%AE%E5%8D%B3%E6%AF%8F%E4%B8%80%E4%B8%AA-patch%E5%B0%B1%E6%98%AF%E4%B8%80%E4%B8%AA-glimpse"><span class="nav-text">1.3.1.2.  文中 glimpse 的含义，可以理解成每一个位置，即每一个 patch，就是一个 glimpse。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%96%87%E7%AB%A0%E6%8F%90%E5%87%BA%E4%BA%86-hard-attention-%E4%B8%8E%E5%8F%8C%E9%98%B6%E6%AE%B5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E7%9A%84%E7%9B%B8%E4%BC%BC%E6%80%A7-%E4%BB%96%E4%BB%AC%E9%83%BD%E6%98%AF%E6%88%AA%E5%8F%96%E5%9B%BE%E5%83%8F%E5%9D%97%E4%BD%9C%E4%B8%BA%E8%BE%93%E5%85%A5%E7%84%B6%E5%90%8E%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB-%E4%B8%8D%E5%90%8C%E7%9A%84%E6%98%AF%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E5%AF%B9%E4%BA%8E%E7%9B%AE%E6%A0%87%E7%9A%84%E4%BD%8D%E7%BD%AE%E6%98%AF%E6%9C%89%E6%A0%87%E7%AD%BE%E7%9A%84%E8%80%8C-hard-attention-%E6%98%AF%E6%97%A0%E6%A0%87%E7%AD%BE%E7%9A%84"><span class="nav-text">1.3.1.3.  文章提出了 Hard Attention 与双阶段目标检测算法的相似性。他们都是截取图像块作为输入，然后进行分类。不同的是，目标检测算法对于目标的位置是有标签的，而 Hard Attention 是无标签的。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%BB%E7%BB%93%E6%9E%84"><span class="nav-text">1.3.1.4.  总结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#saccader-cell"><span class="nav-text">1.3.1.5.  Saccader cell</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E7%AD%96%E7%95%A5"><span class="nav-text">1.3.1.6.  训练策略</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%BA%E6%96%87%E5%AF%B9%E6%AF%94%E4%BA%86-ordered-logits-policy-%E5%92%8C-saccader"><span class="nav-text">1.3.1.7.  论文对比了 ordered logits policy 和 Saccader</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A9%BA%E9%97%B4%E4%B8%8A%E5%85%B6%E4%BB%96%E4%BD%8D%E7%BD%AE"><span class="nav-text">1.3.1.8.  空间上其他位置</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hard-attention-for-scalable-image-classification"><span class="nav-text">1.3.2.  Hard Attention for Scalable Image Classification</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%8B%E7%BB%8D"><span class="nav-text">1.3.2.1.  介绍</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#soft-attention"><span class="nav-text">1.4.  Soft Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#nam-normalization-based-attention-module"><span class="nav-text">1.4.1.  NAM, Normalization-based Attention Module</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%A9%E7%94%A8%E7%BD%91%E7%BB%9C%E5%8F%82%E6%95%B0%E7%9A%84%E4%BF%A1%E6%81%AF%E8%BF%9B%E4%B8%80%E6%AD%A5%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE%E5%86%8D%E5%88%A9%E7%94%A8"><span class="nav-text">1.4.1.1.  利用网络参数的信息，进一步处理数据（再利用）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B3%E4%BA%8E%E6%9D%83%E9%87%8D%E7%A8%80%E7%96%8F%E6%80%A7"><span class="nav-text">1.4.1.2.  关于权重稀疏性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%90%AF%E5%8F%91"><span class="nav-text">1.4.1.3.  启发</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#regularization"><span class="nav-text">2.  Regularization</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#adcm-attentnion-dropout-convolutional-module"><span class="nav-text">2.1.  ADCM: Attentnion Dropout Convolutional Module</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Xiang Ye"
      src="/images/avatar_100.jpg">
  <p class="site-author-name" itemprop="name">Xiang Ye</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">19</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">23</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>


  <div class="links-of-blogroll site-overview-item animated">
    <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://github.com/xyegithub" title="https:&#x2F;&#x2F;github.com&#x2F;xyegithub" rel="noopener" target="_blank">My Github</a>
        </li>
    </ul>
  </div>

        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yexiang.ml/2021/12/14/Tips-in-Papers/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar_100.jpg">
      <meta itemprop="name" content="Xiang Ye">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ye, Xiang's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Tips in Papers
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-12-14 16:33:03" itemprop="dateCreated datePublished" datetime="2021-12-14T16:33:03+08:00">2021-12-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-05-12 19:20:52" itemprop="dateModified" datetime="2022-05-12T19:20:52+08:00">2022-05-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/About-Papers/" itemprop="url" rel="index"><span itemprop="name">About Papers</span></a>
        </span>
    </span>

  
</div>

            <div class="post-description">记录阅读论文过程中，新获取的信息。</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <html><head></head><body><h1 id="attention"><span class="post-title-index">1. </span><a class="markdownIt-Anchor" href="#attention"></a> Attention</h1>
<h2 id="hard-and-soft-attention"><span class="post-title-index">1.1. </span><a class="markdownIt-Anchor" href="#hard-and-soft-attention"></a> Hard and Soft Attention</h2>
<h3 id="attention-mechanisms-in-cv-a-survey"><span class="post-title-index">1.1.1. </span><a class="markdownIt-Anchor" href="#attention-mechanisms-in-cv-a-survey"></a> Attention Mechanisms in CV: A Survey</h3>
<h2 id="gumbel-softmax-hard-attention"><span class="post-title-index">1.2. </span><a class="markdownIt-Anchor" href="#gumbel-softmax-hard-attention"></a> Gumbel-Softmax Hard Attention</h2>
<h3 id="categorical-reparameterization-with-gumbel-softmax"><span class="post-title-index">1.2.1. </span><a class="markdownIt-Anchor" href="#categorical-reparameterization-with-gumbel-softmax"></a> Categorical Reparameterization with Gumbel-Softmax</h3>
<h4 id="文章引出了一类神经网络stochastic-neural-networks-with-discrete-random-variables"><span class="post-title-index">1.2.1.1. </span><a class="markdownIt-Anchor" href="#文章引出了一类神经网络stochastic-neural-networks-with-discrete-random-variables"></a> 文章引出了一类神经网络，Stochastic Neural Networks with discrete random variables。</h4>
<p>具有离散随机变量的随机神经网络</p>
<h4 id="文章引出了一个方向stochastic-gradient-estimation"><span class="post-title-index">1.2.1.2. </span><a class="markdownIt-Anchor" href="#文章引出了一个方向stochastic-gradient-estimation"></a> 文章引出了一个方向，Stochastic Gradient Estimation</h4>
<p>随机梯度估计，主要的方法就有 dumbel-softmax, score function estimator, biased
path derivative estimator。</p>
<blockquote>
<p>However, no existing gradient estimator has been formulated specifically for
categorical variables.</p>
</blockquote>
<h2 id="reinforce-learnning-hard-attention"><span class="post-title-index">1.3. </span><a class="markdownIt-Anchor" href="#reinforce-learnning-hard-attention"></a> Reinforce Learnning Hard Attention</h2>
<h3 id="2019-scacader-improving-accuracy-of-hard-attention-models-for-vision"><span class="post-title-index">1.3.1. </span><a class="markdownIt-Anchor" href="#2019-scacader-improving-accuracy-of-hard-attention-models-for-vision"></a> 2019 Scacader: Improving Accuracy of Hard Attention Models for Vision</h3>
<h4 id="hard-attention-选择相关的特征作为输入是真正具有可解释性的"><span class="post-title-index">1.3.1.1. </span><a class="markdownIt-Anchor" href="#hard-attention-选择相关的特征作为输入是真正具有可解释性的"></a> Hard Attention 选择相关的特征作为输入，是真正具有可解释性的。</h4>
<p>从下面第一段话，可以看出，hard attention 是真正和性能挂钩的。也一定和特征重要性
是吻合的。但是 Soft Attention 不具有可解释性，小权重的特征并不一定不重
要</p><div id="ap"></div><p></p>
<blockquote>
<p>Our best models narrow the gap to common ImageNet baselines, achieving 75%
top-1 and 91% top-5 while attending to less than one-third of the image.We
further demonstrate that occluding the image patches prposed by the Saccader
model highly impairs classification, thus confirming these patches strong
relevance to the classification task.</p>
</blockquote>
<blockquote>
<p>Typical soft attention mechanisms rescale features at one or more stages of
the network. The soft mask used for rescaling often to provide some insight
into the model’s decision-making process, but the model’s final decision may
nonetheless rely on information provided by features with small weights &lt;sup id=“fnref:1”&gt;<a href="#fn:1" rel="footnote">1</a></p>
</blockquote>
<p>\</p>
<h4 id="文中-glimpse-的含义可以理解成每一个位置即每一个-patch就是一个-glimpse"><span class="post-title-index">1.3.1.2. </span><a class="markdownIt-Anchor" href="#文中-glimpse-的含义可以理解成每一个位置即每一个-patch就是一个-glimpse"></a> 文中 glimpse 的含义，可以理解成每一个位置，即每一个 patch，就是一个 glimpse。</h4>
<blockquote>
<p>Models that employ hard attention make decisions based on only a subset of
pixel in the input image, typically in the form of a series of glimpses.</p>
</blockquote>
<h4 id="文章提出了-hard-attention-与双阶段目标检测算法的相似性-他们都是截取图像块作为输入然后进行分类-不同的是目标检测算法对于目标的位置是有标签的而-hard-attention-是无标签的"><span class="post-title-index">1.3.1.3. </span><a class="markdownIt-Anchor" href="#文章提出了-hard-attention-与双阶段目标检测算法的相似性-他们都是截取图像块作为输入然后进行分类-不同的是目标检测算法对于目标的位置是有标签的而-hard-attention-是无标签的"></a> 文章提出了 Hard Attention 与双阶段目标检测算法的相似性。他们都是截取图像块作为输入，然后进行分类。不同的是，目标检测算法对于目标的位置是有标签的，而 Hard Attention 是无标签的。</h4>
<blockquote>
<p>Altough our aim in this work is to perform classification with only
image-level class labels, out approach bears some resembalance to two-stage
object detection models.</p>
</blockquote>
<blockquote>
<p>These models operate by generating many region proposals and then applying a
classification model to each proposal.</p>
</blockquote>
<blockquote>
<p>Unlike our work, these approaches use ground-truth bounding boxes to train the
classification model, and modern architectures also use bounding boxes to
supervise the proposal generator.</p>
</blockquote>
<p><strong>目标检测和 Hard Attention 的相似之处在于，他们都同时关注目标的位置和类别。相比
于目标检测，Hard Attention 可以做的更精细, i.e., 它可以像目标检测一样在图像域上
挑选特征，它还可以在任意一个特征域里挑选特征；它不仅可以像目标检测一样，挑选空域
的特征，还可以挑选通道域的特征。</strong></p>
<h4 id="总结构"><span class="post-title-index">1.3.1.4. </span><a class="markdownIt-Anchor" href="#总结构"></a> 总结构</h4>
<img src="Saccader_Over.jpg" width="80%" height="80%">
<p>最上面的 rep. net 以及 logits per location 之前都属于 representation network。这
部分挺常规，但是使用了‘BagNet’&lt;sup id=“fnref:2”&gt;<a href="#fn:2" rel="footnote">2</a>的方法，该方法保持了输出特征图中每个像素的感
受野大小。</p>
<p>下面的 atten. net 就是几个卷积层的堆叠。没有 attention 机制。到 Sacadder cell 之
前都是常规操作，除了一个 what 和 where 的 concat 得到 mixed。</p>
<p>Saccader cell 是技术关键点。</p>
<p><strong>coordinate at time t 的 slice 操作，对于坐标的选择而言，是一个不可导的操作。这
里是强化学习介入的地方。而且注意，这里 t 是一个序列，最后的 prediction 是求平均
。</strong></p>
<p>\</p>
<h4 id="saccader-cell"><span class="post-title-index">1.3.1.5. </span><a class="markdownIt-Anchor" href="#saccader-cell"></a> Saccader cell</h4>
<img src="Saccader_Cell.jpg" width="80%" height="80%">
<p>值得注意的几点</p>
<ol>
<li>Cell state 是一个 state 的序列，每个 state 都是一个经过 2d softmax 的
logit。这个 logit 表示该 state 预测的位置。</li>
</ol>
<blockquote>
<p>This cell produces a sequence of locations. Elements in the sequence
correctpond to target locations.</p>
</blockquote>
<ol start="2">
<li>需要保证 cell state 中预测之间都是不同的。<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>C</mi><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">C^t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7935559999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7935559999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span></span></span></span></span></span></span></span>记录了 t 时刻位置探索过的所有
位置。那些位置的值是 1。所以<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>C</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">C^{t - 1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span>两次介入<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>C</mi><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">C^t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7935559999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7935559999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span></span></span></span></span></span></span></span>的计算都乘以一个非常小的
数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>−</mo><mn>1</mn><msup><mn>0</mn><mn>5</mn></msup></mrow><annotation encoding="application/x-tex">-10^5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.897438em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">5</span></span></span></span></span></span></span></span></span></span></span> ，这样就保证了在 2d softmax 的时候，探索过的位置无法胜出。</li>
</ol>
<blockquote>
<p>The cell includes a 2D state (<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>C</mi><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">C^t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7935559999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7935559999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span></span></span></span></span></span></span></span>) that keeps memory of the visited
locations until time t by placing 1 in the corresponding location in the cell
state. We use this state to prevent the network from returning to previously
seen locations.</p>
</blockquote>
<p><strong>从这里也看出 state 和 sequence 是不同的。state 是记录探索过的位置。从途中可以
看出 state 是非 01 的，而由 1 可知，sequence 应该是图中右边产生 logic。state 的
目的，其实也是为了产生 logic。</strong></p>
<ol start="3">
<li>
<p>在制作<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>C</mi><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">C^t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7935559999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7935559999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span></span></span></span></span></span></span></span>的过程中，信息来源有两个 mixed feature 和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>C</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">C^{t - 1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span>。最后得到
的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>C</mi><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">C^t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7935559999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7935559999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span></span></span></span></span></span></span></span>通道是 1，所以 mixed feature 空间维度的压缩是必然的。在压缩的时候，选
用了 channel attention 机制。channel attention 机制又需要先空间压缩，这里不
像 SE-Net 一样直接压缩空间，而是又做了一个空间的 mask 压缩空间，这个 mask 用
了<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>C</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">C^{t - 1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span>的信息，去除掉了已经探索的位置信息。</p>
</li>
<li>
<blockquote>
<p>At test time, the model extracts the logits at time <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.61508em;vertical-align:0em;"></span><span class="mord mathdefault">t</span></span></span></span> from the
representation network at location <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>a</mi><mi>x</mi><mi mathvariant="normal">_</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><mo stretchy="false">(</mo><msup><mover accent="true"><mi>R</mi><mo>^</mo></mover><mi>t</mi></msup><mi mathvariant="normal">_</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">argmax\_{i,j}(\hat{R}^t\_{i,j})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.25677em;vertical-align:-0.31em;"></span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span></span><span class="mopen">(</span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9467699999999999em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span></span></span><span style="top:-3.25233em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.16666em;">^</span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7935559999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span></span></span></span></span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span></span><span class="mclose">)</span></span></span></span>.The
final prediction is obtained by averaging the extracted logits across all
times.</p>
</blockquote>
</li>
</ol>
<p><strong>Saccader Cell 的关键就在于产生一系列的 sequence。这些 sequence 可以用强化学习
算法优化，使其可以预测物体的位置，从而就进行了 feature 的选择。</strong></p>
<h4 id="训练策略"><span class="post-title-index">1.3.1.6. </span><a class="markdownIt-Anchor" href="#训练策略"></a> 训练策略</h4>
<blockquote>
<p>The goal of our training is to learn a policy that predicts a sequence of
visual attentnion locations that is useful to the downsteam task (here image
classification) in absence of location labels.</p>
<p>We performed a three step training procedure using only the training class
lables as supervision.</p>
</blockquote>
<p><img src="Saccader_eq1.jpg" alt=""></p>
<ol>
<li>预训练了 representation network</li>
</ol>
<p>这个公式增大了目标<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi><mi mathvariant="normal">_</mi><mrow><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi></mrow></mrow><annotation encoding="application/x-tex">y\_{target}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9250799999999999em;vertical-align:-0.31em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault">e</span><span class="mord mathdefault">t</span></span></span></span></span>的概率。增大的某个位置上<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi><mi mathvariant="normal">_</mi><mrow><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi></mrow></mrow><annotation encoding="application/x-tex">y\_{target}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9250799999999999em;vertical-align:-0.31em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault">e</span><span class="mord mathdefault">t</span></span></span></span></span>的概率，而且
所有位置连乘之后的概率。这里假设 region of interest 的概率将会被增大的最多。通过
这种方法就自动学习到了一些好的 position，在强化学习优化的时候，提供了一个好的探
索的起点。</p>
<blockquote>
<p>Key to Saccader is a pretraining step that require only class lables and
provides initial attention locations for policy gradient optimization.</p>
</blockquote>
<blockquote>
<p>Our pretraining procedure overcomes the sparse-reward problem that makes hard
attention models difficult to optimize. It requires access to only class
lables and prvides initial attention locations.These initial locations provide
better rewards for the policy gradient learning.</p>
</blockquote>
<p><img src="Saccader_eq2.jpg" alt=""></p>
<ol start="2">
<li>训练了 location network (attention network, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1 \times 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span> mixing convolution
and Sacader cell)</li>
</ol>
<p>这是一个自监督的预训练。提高了前 T = 12 次预测的点的概率。</p>
<p>文中说排序了，我感觉排序的次序和预测的次序是一致的。第一次预测是概率最大的，最后
一次预测是概率最小的。后面实验对比了该预训练对性能的影响。然而
，<font color="deeppink">现在还并不能完全了解这个预训练的作用机制。 </font></p>
<p><img src="Saccader_eq3.jpg" alt=""></p>
<ol start="3">
<li>
<blockquote>
<p>we trained the whole model to maximize the expected reward, where the
reward (<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>r</mi><mo>∈</mo><mrow><mn>0</mn><mo separator="true">,</mo><mn>1</mn></mrow></mrow><annotation encoding="application/x-tex">r \in {0, 1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8388800000000001em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">1</span></span></span></span></span>) represents whether the model final prediction
after 6 glimpses (T = 6) is correct.</p>
</blockquote>
</li>
</ol>
<p><font color="deeppink">这个公式并没有完全读懂。</font></p>
<p>用的是策略网络的方法。<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>l</mi><mi>s</mi><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">l^t_s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.040556em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7935559999999999em;"><span style="top:-2.4530000000000003em;margin-left:-0.01968em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">s</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span>是按 saccader cell 输出的位置概率生成的位置。公式的
第一行就是增大这个概率的值。当模型最后做出了正确预测的时候，r 权重大，损失函数以
更大的权重增大这个预测的概率，这就是策略网络的方法。等式的第二行是修正分类网络，
当预测正确的时候，那么有监督的增大这 T 个预测位置在<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi><mi mathvariant="normal">_</mi><mrow><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi></mrow></mrow><annotation encoding="application/x-tex">y\_{target}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9250799999999999em;vertical-align:-0.31em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault">e</span><span class="mord mathdefault">t</span></span></span></span></span>上的准确率
。</p><div id="空间"></div><p></p>
<p><strong>由公式 1 和公式 3 看出，对于图像空间，loss 函数只是增大预测点<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi><mi mathvariant="normal">_</mi><mrow><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi></mrow></mrow><annotation encoding="application/x-tex">y\_{target}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9250799999999999em;vertical-align:-0.31em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord"><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault">e</span><span class="mord mathdefault">t</span></span></span></span></span>的概
率，然后不会理会，空间上其他点预测的是什么。</strong></p>
<p>如果 saccader cell 的性能非常好，那么模型是没问题的。如果 saccader cell 预测会有
误差，比如抗干扰能力不好，预测到目标以外的 patch 上，那么性能会被影响。</p>
<p><strong>论文中说
，<a href="#ap">当预测的位置不存在的时候，模型的性能受到很大的影响。那么其实抑制预测之外的位置的，激活值会使得可以解释性更强？</a></strong></p>
<h4 id="论文对比了-ordered-logits-policy-和-saccader"><span class="post-title-index">1.3.1.7. </span><a class="markdownIt-Anchor" href="#论文对比了-ordered-logits-policy-和-saccader"></a> 论文对比了 ordered logits policy 和 Saccader</h4>
<blockquote>
<p>An ordered logits policy uses the BagNet model to pick the top K locations
based on the largest class logits.</p>
</blockquote>
<p>Ordered logits policy 选择了在空间上最大激活的位置，作为物体的位置，并把该预测作
为物体类别的预测。</p>
<blockquote>
<p>The ordered logits policy strats off with accuracy much higher than a random
policy, suggesting that the patches it initially picks are meaningful to
classification.</p>
<p>However, accuracy is still lower than the learned Saccader model, and
<strong>performacne improves only slowly with additional glimpese. The ordered
logits policy is able to capture some of the features relevant to
classification, but it is a greedy policy that produces glimpses that cluster
around a few top features (i.e., with low image coverage)</strong></p>
</blockquote>
<p>ordered logits policy 的缺陷在于，只关注了最重要的少数的特征。它的预测都围绕在这
些最重要的特征周围，所以增加 glimpese 的时候，很少新的信息引入，所以 performance
增长的很慢，而 Saccader 没有这个问题，它关注了完整的信息。</p>
<h4 id="空间上其他位置"><span class="post-title-index">1.3.1.8. </span><a class="markdownIt-Anchor" href="#空间上其他位置"></a> 空间上其他位置</h4>
<p>如<a href="#%E7%A9%BA%E9%97%B4">上文所说</a>，Sccader 只优化选中的位置，不管没有被选中的位置。那么当增加
glimplese 的时候，它的性能应该会受到影响的。但是不是。</p>
<blockquote>
<p>In fact, increasing the number of glimpses beyond the number used for DRAM
policy training leads to drop in performane, ulike the Saccader model that
generalizes to greater umbers of glimpses.</p>
</blockquote>
<img src="Saccader_gl.jpg" width="50%" height="50%">
<p>如图可以看出，增大 glimpses 的时候，准确率反而在增加。说明在训练的时候，没有被训
练到的地方，Saccader 的预测也还可以，与<a href="#%E7%A9%BA%E9%97%B4">上文所说</a>的理解不同
。<font color="deeppink">原因是什么还有待探究。</font></p>
<h3 id="hard-attention-for-scalable-image-classification"><span class="post-title-index">1.3.2. </span><a class="markdownIt-Anchor" href="#hard-attention-for-scalable-image-classification"></a> Hard Attention for Scalable Image Classification</h3>
<h4 id="介绍"><span class="post-title-index">1.3.2.1. </span><a class="markdownIt-Anchor" href="#介绍"></a> 介绍</h4>
<img src="Tnet_over.jpg" width="50%" height="50%">
<blockquote>
<p>Muti-scale processing in the proposed TNet architecture. Starting from level
1, the image is processed in low resolution to get a coarse description of its
content (red cube). Extracted features are used for (hard) selection of image
regions worth processing in higher resolution. The process is repeated
recursively (here to 2 additional levels). Features from all levels are
combined (arrows on the right) to create the final image representation used
for classificaiton (blue cube).</p>
</blockquote>
<p>先粗后细的粗粒不同尺度的图像，这篇文章使用和实现了 Saccader</p>
<h2 id="soft-attention"><span class="post-title-index">1.4. </span><a class="markdownIt-Anchor" href="#soft-attention"></a> Soft Attention</h2>
<h3 id="nam-normalization-based-attention-module"><span class="post-title-index">1.4.1. </span><a class="markdownIt-Anchor" href="#nam-normalization-based-attention-module"></a> NAM, Normalization-based Attention Module</h3>
<h4 id="利用网络参数的信息进一步处理数据再利用"><span class="post-title-index">1.4.1.1. </span><a class="markdownIt-Anchor" href="#利用网络参数的信息进一步处理数据再利用"></a> 利用网络参数的信息，进一步处理数据（再利用）</h4>
<blockquote>
<p>Those methods successfully exploit the mutual information from different
dimensions of feawture. However, they lack consideration on the contributing
factors of weights, which is capable of furthr suppressing the insignificant
channels or pixels.</p>
</blockquote>
<p>网络参数本身是表达了一定的信息的，比如某个卷积核提取什么样的信息，本身也代表一种
信息。神经网络中，每个参数都在图像的处理过程中使用一次，然而，<strong>一次是否就已经用
完了该参数的信息。它是否还有其他价值？</strong></p>
<p>文章重复利用了 bn 层中的权重，文章任务 bn 层的权重可以表达特征图的重要程度。然而
attention 正好又需要特征图的重要程度。所以文章在 attention 中利用了 bn 层的权重
。<strong>使得 bn 的权重出现重复利用的现象。</strong></p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mtable rowspacing="0.24999999999999992em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>a</mi><mi>t</mi><mi>t</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mi>n</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>a</mi><mi>t</mi><mi>t</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mi>a</mi><mi>t</mi><mi>t</mi><mo>×</mo><mi>γ</mi><mo>+</mo><mi>δ</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>a</mi><mi>t</mi><mi>t</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mi>a</mi><mi>t</mi><mi>t</mi><mo>×</mo><mfrac><mi>γ</mi><mrow><mi>s</mi><mi>u</mi><mi>m</mi><mo stretchy="false">(</mo><mi>γ</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mi>a</mi><mi>t</mi><mi>t</mi><mi mathvariant="normal">.</mi><mi>s</mi><mi>i</mi><mi>g</mi><mi>m</mi><mi>o</mi><mi>i</mi><mi>d</mi><mo stretchy="false">(</mo><mo stretchy="false">)</mo><mo>×</mo><mi>x</mi></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
att &amp;= norm(x) \\
att &amp;= att \times \gamma + \delta \\
att &amp;= att \times \frac\gamma{sum(\gamma)} \\
out &amp;= att.sigmoid() \times x
\end{aligned}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:6.84356em;vertical-align:-3.1717800000000005em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.67178em;"><span style="top:-5.93934em;"><span class="pstrut" style="height:3.10756em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="mord mathdefault">t</span><span class="mord mathdefault">t</span></span></span><span style="top:-4.43934em;"><span class="pstrut" style="height:3.10756em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="mord mathdefault">t</span><span class="mord mathdefault">t</span></span></span><span style="top:-2.671779999999999em;"><span class="pstrut" style="height:3.10756em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="mord mathdefault">t</span><span class="mord mathdefault">t</span></span></span><span style="top:-0.5957799999999995em;"><span class="pstrut" style="height:3.10756em;"></span><span class="mord"><span class="mord mathdefault">o</span><span class="mord mathdefault">u</span><span class="mord mathdefault">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:3.1717800000000005em;"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.67178em;"><span style="top:-5.93934em;"><span class="pstrut" style="height:3.10756em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathdefault">n</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">m</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span><span style="top:-4.43934em;"><span class="pstrut" style="height:3.10756em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathdefault">a</span><span class="mord mathdefault">t</span><span class="mord mathdefault">t</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault" style="margin-right:0.05556em;">γ</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault" style="margin-right:0.03785em;">δ</span></span></span><span style="top:-2.671779999999999em;"><span class="pstrut" style="height:3.10756em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathdefault">a</span><span class="mord mathdefault">t</span><span class="mord mathdefault">t</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1075599999999999em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">s</span><span class="mord mathdefault">u</span><span class="mord mathdefault">m</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.05556em;">γ</span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathdefault" style="margin-right:0.05556em;">γ</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span><span style="top:-0.5957799999999995em;"><span class="pstrut" style="height:3.10756em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathdefault">a</span><span class="mord mathdefault">t</span><span class="mord mathdefault">t</span><span class="mord">.</span><span class="mord mathdefault">s</span><span class="mord mathdefault">i</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault">m</span><span class="mord mathdefault">o</span><span class="mord mathdefault">i</span><span class="mord mathdefault">d</span><span class="mopen">(</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:3.1717800000000005em;"><span></span></span></span></span></span></span></span></span></span></span></span></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Channel_Att</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, channels, t=<span class="number">16</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Channel_Att, self).__init__()</span><br><span class="line">        self.channels = channels</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(self.channels, affine=<span class="literal">True</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        residual = x</span><br><span class="line">        x = self.bn2(x)</span><br><span class="line">        weight_bn = self.bn2.weight.data.<span class="built_in">abs</span>() / torch.<span class="built_in">sum</span>(self.bn2.weight.data.<span class="built_in">abs</span>())</span><br><span class="line">        x = x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).contiguous()</span><br><span class="line">        x = torch.mul(weight_bn, x)</span><br><span class="line">        x = x.permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>).contiguous()</span><br><span class="line">        x = torch.sigmoid(x) * residual</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></tbody></table></figure>
<h4 id="关于权重稀疏性"><span class="post-title-index">1.4.1.2. </span><a class="markdownIt-Anchor" href="#关于权重稀疏性"></a> 关于权重稀疏性</h4>
<blockquote>
<p>It applies a weightsparsity penalty to the attention modules, thus, making
them more computational efficient while retaining similar performance.</p>
<p>To suppress the less salient weights, we add a regularization term into the
loss function.</p>
</blockquote>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo>=</mo><munder><mo>∑</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></munder><mi>l</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>W</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>+</mo><mi>p</mi><mo>∑</mo><mi>g</mi><mo stretchy="false">(</mo><mi>γ</mi><mo stretchy="false">)</mo><mo>+</mo><mi>p</mi><mo>∑</mo><mi>g</mi><mo stretchy="false">(</mo><mi>λ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Loss = \sum_{(x,y)}l(f(x, W), y) + p\sum g(\gamma) + p \sum g(\lambda)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault">L</span><span class="mord mathdefault">o</span><span class="mord mathdefault">s</span><span class="mord mathdefault">s</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.5660100000000003em;vertical-align:-1.516005em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.808995em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">x</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">y</span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.516005em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.6000100000000002em;vertical-align:-0.55001em;"></span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-symbol large-op" style="position:relative;top:-0.000004999999999977245em;">∑</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.05556em;">γ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.6000100000000002em;vertical-align:-0.55001em;"></span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-symbol large-op" style="position:relative;top:-0.000004999999999977245em;">∑</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathdefault">λ</span><span class="mclose">)</span></span></span></span></span></p>
<p>第一项是正常的损失函数，第二项和第三项的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>g</mi><mi mathvariant="normal">（</mi><mi mathvariant="normal">）</mi></mrow><annotation encoding="application/x-tex">g（）</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord cjk_fallback">（</span><span class="mord cjk_fallback">）</span></span></span></span>是一范数，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05556em;">γ</span></span></span></span>是通道注意力中
bn 的权重，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">λ</span></span></span></span>是空间注意力中 pix normalization 的权重。加上这两项使得通道
和空间注意力都有了稀疏性。</p>
<p><font color="deeppink">loss 函数增加了权重的稀疏性，attention 把权重的稀疏性转换为
了特征的稀疏性，然而这个稀疏性并不是丢弃了特征或者权重，为什么它能够带来
computational efficient? 文章确实有 computational efficient，但是根据实验部分的
列表，parameter 和 FLOPS 的优势都是这个注意力机制的结构带来的，相比于其他注意力
机制，它是轻量而且计算低的。但是稀疏性和计算复杂度是无关的。</font></p>
<h4 id="启发"><span class="post-title-index">1.4.1.3. </span><a class="markdownIt-Anchor" href="#启发"></a> 启发</h4>
<p><font color="deeppink">这篇文章给人一个启发是参数复用，神经网络中所有参数几乎不会
复用。然而，除了文中提出的这个例子，是否还存在其他需要复用的场景或者理由
。</font></p>
<p><font color="deeppink">参数复用可以看成另一种权值共享，卷积是层内的权值共享，参数
复用是层间的权值共享，它是一种正则化，它对不变性产生什么影响？</font></p>
<h1 id="regularization"><span class="post-title-index">2. </span><a class="markdownIt-Anchor" href="#regularization"></a> Regularization</h1>
<h2 id="adcm-attentnion-dropout-convolutional-module"><span class="post-title-index">2.1. </span><a class="markdownIt-Anchor" href="#adcm-attentnion-dropout-convolutional-module"></a> ADCM: Attentnion Dropout Convolutional Module</h2>
<p><img src="ADCM.jpg" alt="ADCM"></p>
<p>在 CBAM 的基础上加入了正则化，把 CBAM 产生的 attention weights 作为 Drop 的概率
引导，来对 feature map 进行 drop。是一种对 attention 机制的正则化方法，很容以把
它误解为 hard attention。</p>
<div id="footnotes"><hr><div id="footnotelist"><ol><li id="fn:1">2018,Learn to pay attention.<a href="#fnref:1" rev="footnote"> ↩</a></li><li id="fn:2">2019, Approximating CNNs with bag-of-local-features models works
surprisingly well on ImageNet.<a href="#fnref:2" rev="footnote"> ↩</a></li></ol></div></div></body></html>
    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Personal-Thought/" rel="tag"># Personal Thought</a>
              <a href="/tags/Papers/" rel="tag"># Papers</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/12/10/An-Introduction-to-Git/" rel="prev" title="An Introduction to Git">
                  <i class="fa fa-chevron-left"></i> An Introduction to Git
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/12/15/Personal-Thought/" rel="next" title="Personal Thought">
                  Personal Thought <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Xiang Ye</span>
</div>

<script language=javascript>
    function siteTime() {
        window.setTimeout("siteTime()", 1000);
        var seconds = 1000;
        var minutes = seconds * 60;
        var hours = minutes * 60;
        var days = hours * 24;
        var years = days * 365;
        var today = new Date();
        var todayYear = today.getFullYear();
        var todayMonth = today.getMonth() + 1;
        var todayDate = today.getDate();
        var todayHour = today.getHours();
        var todayMinute = today.getMinutes();
        var todaySecond = today.getSeconds();
        /* Date.UTC() -- 返回date对象距世界标准时间(UTC)1970年1月1日午夜之间的毫秒数(时间戳)
        year - 作为date对象的年份，为4位年份值
        month - 0-11之间的整数，做为date对象的月份
        day - 1-31之间的整数，做为date对象的天数
        hours - 0(午夜24点)-23之间的整数，做为date对象的小时数
        minutes - 0-59之间的整数，做为date对象的分钟数
        seconds - 0-59之间的整数，做为date对象的秒数
        microseconds - 0-999之间的整数，做为date对象的毫秒数 */
        var t1 = Date.UTC(2021, 11, 20, 00, 00, 00); //北京时间2018-2-13 00:00:00
        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
        var diff = t2 - t1;
        var diffYears = Math.floor(diff / years);
        var diffDays = Math.floor((diff / days) - diffYears * 365);
        var diffHours = Math.floor((diff - (diffYears * 365 + diffDays) * days) / hours);
        var diffMinutes = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours) / minutes);
        var diffSeconds = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours - diffMinutes * minutes) / seconds);
        document.getElementById("sitetime").innerHTML = "本站已运行 " +diffYears+" 年 "+diffDays + " 天 " + diffHours + " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
    }/*因为建站时间还没有一年，就将之注释掉了。需要的可以取消*/
    siteTime();
</script>

<span id="sitetime"></span>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css" integrity="sha256-9/mhQQwkpU5okPfM5l0v3LnP9xtc6JK8dKW0/WlGaUc=" crossorigin="anonymous">



</body>
</html>
